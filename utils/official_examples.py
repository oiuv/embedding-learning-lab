#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®˜æ–¹ç¤ºä¾‹æ•´åˆ - text-embedding-v4æœ€ä½³å®è·µ
=====================================

åŸºäºå®˜æ–¹æ–‡æ¡£æä¾›çš„ä¸‰ç§æ ¸å¿ƒä½¿ç”¨æ–¹å¼ï¼š
1. å•æ–‡æœ¬å‘é‡åŒ–
2. æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–  
3. æ–‡ä»¶æ–‡æœ¬å‘é‡åŒ–

ä½œè€…ï¼šå®˜æ–¹æ–‡æ¡£ + é¡¹ç›®æ•´åˆ
"""

import os
from typing import List, Dict, Union
from openai import OpenAI
import json

class OfficialEmbeddingExamples:
    """å®˜æ–¹ç¤ºä¾‹æ•´åˆç±»"""
    
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        self.client = OpenAI(
            api_key=api_key or os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.model = "text-embedding-v4"
        self.dimensions = 1024
    
    def example_single_text_embedding(self, text: str) -> Dict:
        """ç¤ºä¾‹1ï¼šå•æ–‡æœ¬å‘é‡åŒ–
        
        é€‚ç”¨äºï¼šå•ä¸ªå¥å­ã€æ®µè½ã€å•†å“è¯„ä»·ç­‰åœºæ™¯
        """
        try:
            completion = self.client.embeddings.create(
                model=self.model,
                input=text,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            
            result = {
                "text": text,
                "embedding": completion.data[0].embedding,
                "dimensions": len(completion.data[0].embedding),
                "model": completion.model,
                "usage": completion.usage.dict()
            }
            
            return result
            
        except Exception as e:
            return {"error": str(e)}
    
    def example_batch_text_embedding(self, texts: List[str]) -> Dict:
        """ç¤ºä¾‹2ï¼šæ‰¹é‡æ–‡æœ¬å‘é‡åŒ–
        
        é€‚ç”¨äºï¼šæ–‡æ¡£é›†åˆã€å•†å“åˆ—è¡¨ã€è¯„è®ºæ‰¹é‡å¤„ç†ç­‰åœºæ™¯
        æ³¨æ„ï¼šå•æ¬¡æœ€å¤š10ä¸ªæ–‡æœ¬
        """
        if len(texts) > 10:
            return {"error": "ä¸€æ¬¡æœ€å¤šå¤„ç†10ä¸ªæ–‡æœ¬ï¼Œè¯·åˆ†æ‰¹å¤„ç†"}
            
        try:
            completion = self.client.embeddings.create(
                model=self.model,
                input=texts,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            
            results = []
            for i, data in enumerate(completion.data):
                results.append({
                    "text": texts[i],
                    "embedding": data.embedding,
                    "index": i
                })
            
            return {
                "results": results,
                "total": len(results),
                "model": completion.model,
                "usage": completion.usage.dict()
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def example_file_text_embedding(self, file_path: str) -> Dict:
        """ç¤ºä¾‹3ï¼šæ–‡ä»¶æ–‡æœ¬å‘é‡åŒ–
        
        é€‚ç”¨äºï¼šé•¿ç¯‡æ–‡æ¡£ã€å¤šä¸ªæ®µè½ã€æ‰¹é‡æ–‡æœ¬æ–‡ä»¶ç­‰åœºæ™¯
        æ³¨æ„ï¼šæ–‡ä»¶æ€»è¡Œæ•°ä¸è¶…è¿‡10è¡Œï¼Œæ¯è¡Œä¸è¶…è¿‡8192 Token
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                # è¯»å–æ–‡ä»¶å¹¶æŒ‰è¡Œåˆ†å‰²
                lines = [line.strip() for line in f if line.strip()]
                
            if len(lines) > 10:
                return {"error": f"æ–‡ä»¶è¡Œæ•°({len(lines)})è¶…è¿‡10è¡Œé™åˆ¶"}
                
            completion = self.client.embeddings.create(
                model=self.model,
                input=lines,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            
            results = []
            for i, data in enumerate(completion.data):
                results.append({
                    "line": lines[i],
                    "embedding": data.embedding,
                    "line_number": i + 1
                })
            
            return {
                "file_path": file_path,
                "results": results,
                "total_lines": len(results),
                "model": completion.model,
                "usage": completion.usage.dict()
            }
            
        except FileNotFoundError:
            return {"error": f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨"}
        except Exception as e:
            return {"error": str(e)}
    
    def example_different_dimensions(self, text: str) -> Dict:
        """ç¤ºä¾‹4ï¼šä¸åŒç»´åº¦çš„æ•ˆæœå¯¹æ¯”
        
        å±•ç¤ºä¸åŒç»´åº¦å¯¹åŒä¸€æ–‡æœ¬çš„å‘é‡åŒ–æ•ˆæœ
        """
        dimensions = [64, 128, 256, 512, 768, 1024, 1536, 2048]
        results = {}
        
        for dim in dimensions:
            try:
                completion = self.client.embeddings.create(
                    model=self.model,
                    input=text,
                    dimensions=dim,
                    encoding_format="float"
                )
                
                embedding = completion.data[0].embedding
                results[dim] = {
                    "embedding": embedding,
                    "norm": sum(x**2 for x in embedding) ** 0.5,
                    "memory_usage": len(embedding) * 4  # float32 = 4 bytes
                }
            except Exception as e:
                results[dim] = {"error": str(e)}
        
        return {
            "text": text,
            "dimension_comparison": results
        }
    
    def example_chinese_texts(self) -> Dict:
        """ç¤ºä¾‹5ï¼šä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–ç¤ºä¾‹
        
        å±•ç¤ºä¸­æ–‡æ–‡æœ¬çš„å‘é‡åŒ–æ•ˆæœï¼ŒåŒ…æ‹¬è¯—æ„è¡¨è¾¾ã€å•†å“è¯„ä»·ç­‰
        """
        chinese_texts = [
            "è¡£æœçš„è´¨é‡æ æ çš„ï¼Œå¾ˆæ¼‚äº®ï¼Œä¸æ‰æˆ‘ç­‰äº†è¿™ä¹ˆä¹…å•Šï¼Œå–œæ¬¢ï¼Œä»¥åè¿˜æ¥è¿™é‡Œä¹°",
            "é£æ€¥å¤©é«˜çŒ¿å•¸å“€ï¼Œæ¸šæ¸…æ²™ç™½é¸Ÿé£å›",
            "æ— è¾¹è½æœ¨è§è§ä¸‹ï¼Œä¸å°½é•¿æ±Ÿæ»šæ»šæ¥",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯",
            "Pythonæ˜¯æœ€æµè¡Œçš„æ•°æ®ç§‘å­¦è¯­è¨€",
            "è¿™ä¸ªå•†å“æ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—æ¨è",
            "ä»£ç å†™å¾—å¾ˆå¥½ï¼Œé€»è¾‘æ¸…æ™°ï¼Œæ³¨é‡Šå®Œæ•´"
        ]
        
        return self.example_batch_text_embedding(chinese_texts)
    
    def demo_all_examples(self):
        """æ¼”ç¤ºæ‰€æœ‰å®˜æ–¹ç¤ºä¾‹"""
        print("ğŸš€ å®˜æ–¹ç¤ºä¾‹æ¼”ç¤º - text-embedding-v4")
        print("=" * 50)
        
        # ç¤ºä¾‹1ï¼šå•æ–‡æœ¬
        print("\nğŸ“Œ ç¤ºä¾‹1ï¼šå•æ–‡æœ¬å‘é‡åŒ–")
        single_result = self.example_single_text_embedding(
            "è¡£æœçš„è´¨é‡æ æ çš„ï¼Œå¾ˆæ¼‚äº®ï¼Œä¸æ‰æˆ‘ç­‰äº†è¿™ä¹ˆä¹…å•Šï¼Œå–œæ¬¢"
        )
        if "error" not in single_result:
            print(f"æ–‡æœ¬ï¼š{single_result['text']}")
            print(f"å‘é‡ç»´åº¦ï¼š{single_result['dimensions']}")
            print(f"å‘é‡èŒƒæ•°ï¼š{sum(x**2 for x in single_result['embedding']) ** 0.5:.4f}")
        else:
            print(f"é”™è¯¯ï¼š{single_result['error']}")
        
        # ç¤ºä¾‹2ï¼šæ‰¹é‡æ–‡æœ¬
        print("\nğŸ“Œ ç¤ºä¾‹2ï¼šæ‰¹é‡æ–‡æœ¬å‘é‡åŒ–")
        batch_result = self.example_batch_text_embedding([
            "é£æ€¥å¤©é«˜çŒ¿å•¸å“€",
            "æ¸šæ¸…æ²™ç™½é¸Ÿé£å›", 
            "æ— è¾¹è½æœ¨è§è§ä¸‹",
            "ä¸å°½é•¿æ±Ÿæ»šæ»šæ¥"
        ])
        if "error" not in batch_result:
            print(f"å¤„ç†äº†{batch_result['total']}ä¸ªæ–‡æœ¬")
            for item in batch_result['results']:
                print(f"  {item['text'][:10]}... -> å‘é‡èŒƒæ•°ï¼š{sum(x**2 for x in item['embedding']) ** 0.5:.4f}")
        else:
            print(f"é”™è¯¯ï¼š{batch_result['error']}")
        
        # ç¤ºä¾‹3ï¼šä¸åŒç»´åº¦å¯¹æ¯”
        print("\nğŸ“Œ ç¤ºä¾‹3ï¼šä¸åŒç»´åº¦æ•ˆæœå¯¹æ¯”")
        dim_result = self.example_different_dimensions("æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯")
        if "dimension_comparison" in dim_result:
            print("ç»´åº¦ vs å†…å­˜å ç”¨ï¼š")
            for dim, data in dim_result["dimension_comparison"].items():
                if "error" not in data:
                    print(f"  {dim}ç»´ï¼š{data['memory_usage']}å­—èŠ‚")
        
        # ç¤ºä¾‹4ï¼šä¸­æ–‡æ–‡æœ¬
        print("\nğŸ“Œ ç¤ºä¾‹4ï¼šä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–")
        chinese_result = self.example_chinese_texts()
        if "error" not in chinese_result:
            print(f"å¤„ç†äº†{chinese_result['total']}ä¸ªä¸­æ–‡æ–‡æœ¬")

def create_sample_file():
    """åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶"""
    sample_texts = [
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œé€šè¿‡æ•°æ®å­¦ä¹ æ¨¡å¼",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚é—®é¢˜ï¼Œéœ€è¦å¤§é‡è®¡ç®—èµ„æº",
        "Pythonæ˜¯æœ€æµè¡Œçš„æ•°æ®ç§‘å­¦è¯­è¨€ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„åº“ç”Ÿæ€ç³»ç»Ÿ",
        "è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£å’Œå¤„ç†äººç±»è¯­è¨€"
    ]
    
    with open('data/sample_texts.txt', 'w', encoding='utf-8') as f:
        for text in sample_texts:
            f.write(text + '\n')
    
    print("âœ… ç¤ºä¾‹æ–‡ä»¶å·²åˆ›å»ºï¼šdata/sample_texts.txt")

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('data', exist_ok=True)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    create_sample_file()
    
    # è¿è¡Œæ¼”ç¤º
    examples = OfficialEmbeddingExamples()
    examples.demo_all_examples()
    
    print("\nğŸ‰ å®˜æ–¹ç¤ºä¾‹æ¼”ç¤ºå®Œæˆï¼")
    print("\nä½¿ç”¨æ–¹æ³•ï¼š")
    print("1. å•æ–‡æœ¬ï¼šexamples.example_single_text_embedding('ä½ çš„æ–‡æœ¬')")
    print("2. æ‰¹é‡ï¼šexamples.example_batch_text_embedding(['æ–‡æœ¬1', 'æ–‡æœ¬2'])")
    print("3. æ–‡ä»¶ï¼šexamples.example_file_text_embedding('data/sample_texts.txt')")
    print("4. ç»´åº¦å¯¹æ¯”ï¼šexamples.example_different_dimensions('æµ‹è¯•æ–‡æœ¬')")

if __name__ == "__main__":
    main()