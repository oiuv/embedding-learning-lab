#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬æ’åºå·¥å…·ç±»
=============

æä¾›æ–‡æœ¬æ’åºæ¨¡å‹çš„ç»Ÿä¸€æ¥å£å’Œå·¥å…·å‡½æ•°ï¼Œæ”¯æŒgte-rerankæ¨¡å‹çš„é›†æˆã€‚
"""

import os
import json
import hashlib
import time
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import logging

import dashscope
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RerankDocument:
    """æ’åºæ–‡æ¡£æ•°æ®ç»“æ„"""
    text: str
    doc_id: str = None
    score: float = 0.0
    metadata: Dict = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

@dataclass
class RerankResult:
    """æ’åºç»“æœæ•°æ®ç»“æ„"""
    document: RerankDocument
    relevance_score: float
    original_rank: int
    new_rank: int
    rank_change: int

class TextReranker:
    """æ–‡æœ¬æ’åºå™¨ç±»"""
    
    def __init__(self, 
                 model: str = "gte-rerank-v2",
                 api_key: str = None,
                 max_documents: int = 100,
                 cache_enabled: bool = True,
                 cache_ttl: int = 3600,
                 timeout: int = 30):
        """
        åˆå§‹åŒ–æ–‡æœ¬æ’åºå™¨
        
        Args:
            model: æ’åºæ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            max_documents: æœ€å¤§æ–‡æ¡£æ•°é‡é™åˆ¶
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
            timeout: APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.model = model
        self.max_documents = max_documents
        self.cache_enabled = cache_enabled
        self.cache_ttl = cache_ttl
        self.timeout = timeout
        
        # è®¾ç½®APIå¯†é’¥
        if api_key:
            dashscope.api_key = api_key
        elif os.getenv("DASHSCOPE_API_KEY"):
            dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")
        else:
            raise ValueError("è¯·æä¾›DASHSCOPE_API_KEY")
        
        # ç¼“å­˜ç®¡ç†
        self._cache = {}
        self._cache_stats = {"hits": 0, "misses": 0, "evictions": 0}
        
        logger.info(f"âœ… æ–‡æœ¬æ’åºå™¨åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {model}")
    
    def _generate_cache_key(self, query: str, documents: List[str], **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = json.dumps({
            "query": query,
            "documents": documents,
            "kwargs": kwargs
        }, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_cache(self, key: str) -> Optional[List[RerankResult]]:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if not self.cache_enabled:
            return None
            
        if key in self._cache:
            cached_data = self._cache[key]
            if datetime.now() < cached_data["expires"]:
                self._cache_stats["hits"] += 1
                return cached_data["results"]
            else:
                del self._cache[key]
                self._cache_stats["evictions"] += 1
        
        self._cache_stats["misses"] += 1
        return None
    
    def _set_cache(self, key: str, results: List[RerankResult]):
        """è®¾ç½®ç¼“å­˜"""
        if not self.cache_enabled:
            return
            
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        current_time = datetime.now()
        expired_keys = [
            k for k, v in self._cache.items() 
            if current_time >= v["expires"]
        ]
        for k in expired_keys:
            del self._cache[k]
            self._cache_stats["evictions"] += 1
        
        # æ·»åŠ æ–°ç¼“å­˜
        self._cache[key] = {
            "results": results,
            "expires": current_time + timedelta(seconds=self.cache_ttl)
        }
    
    def rerank(self, 
               query: str, 
               documents: List[RerankDocument],
               top_n: Optional[int] = None,
               return_documents: bool = True) -> List[RerankResult]:
        """
        é‡æ–°æ’åºæ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: å¾…æ’åºçš„æ–‡æ¡£åˆ—è¡¨
            top_n: è¿”å›å‰nä¸ªç»“æœï¼ŒNoneè¡¨ç¤ºè¿”å›æ‰€æœ‰
            return_documents: æ˜¯å¦è¿”å›æ–‡æ¡£å†…å®¹
        
        Returns:
            æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        if not documents:
            return []
        
        # é™åˆ¶æ–‡æ¡£æ•°é‡
        if len(documents) > self.max_documents:
            logger.warning(f"æ–‡æ¡£æ•°é‡è¶…è¿‡é™åˆ¶({len(documents)} > {self.max_documents})ï¼Œå°†è¿›è¡Œæˆªæ–­")
            documents = documents[:self.max_documents]
        
        # æ£€æŸ¥ç¼“å­˜
        doc_texts = [doc.text for doc in documents]
        cache_key = self._generate_cache_key(query, doc_texts, top_n=top_n)
        cached_results = self._get_cache(cache_key)
        
        if cached_results:
            logger.debug("ä½¿ç”¨ç¼“å­˜ç»“æœ")
            return cached_results
        
        try:
            # è°ƒç”¨æ’åºAPI
            response = dashscope.TextReRank.call(
                model=self.model,
                query=query,
                documents=doc_texts,
                top_n=top_n or len(documents),
                return_documents=return_documents
            )
            
            if response.status_code == 200:
                results = self._process_response(documents, response.output.results)
                self._set_cache(cache_key, results)
                return results
            else:
                logger.error(f"æ’åºAPIè°ƒç”¨å¤±è´¥: {response}")
                return []
                
        except Exception as e:
            logger.error(f"æ–‡æœ¬æ’åºå¤±è´¥: {e}")
            return []
    
    def _process_response(self, 
                         original_documents: List[RerankDocument],
                         api_results: List[Dict]) -> List[RerankResult]:
        """å¤„ç†APIå“åº”"""
        results = []
        
        # åˆ›å»ºåŸå§‹æ–‡æ¡£æ˜ å°„
        doc_map = {i: doc for i, doc in enumerate(original_documents)}
        
        for api_result in api_results:
            index = api_result.index
            relevance_score = api_result.relevance_score
            
            if index in doc_map:
                document = doc_map[index]
                result = RerankResult(
                    document=document,
                    relevance_score=relevance_score,
                    original_rank=index,
                    new_rank=len(results),
                    rank_change=index - len(results)
                )
                results.append(result)
        
        return results
    
    def batch_rerank(self, 
                    queries: List[str], 
                    documents_list: List[List[RerankDocument]],
                    top_n: Optional[int] = None) -> List[List[RerankResult]]:
        """
        æ‰¹é‡é‡æ–°æ’åº
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            documents_list: æ–‡æ¡£åˆ—è¡¨çš„åˆ—è¡¨
            top_n: è¿”å›å‰nä¸ªç»“æœ
        
        Returns:
            æ’åºç»“æœåˆ—è¡¨çš„åˆ—è¡¨
        """
        if len(queries) != len(documents_list):
            raise ValueError("æŸ¥è¯¢å’Œæ–‡æ¡£åˆ—è¡¨æ•°é‡ä¸åŒ¹é…")
        
        results = []
        for query, documents in zip(queries, documents_list):
            reranked = self.rerank(query, documents, top_n=top_n)
            results.append(reranked)
        
        return results
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self._cache_stats["hits"] + self._cache_stats["misses"]
        hit_rate = self._cache_stats["hits"] / total_requests if total_requests > 0 else 0
        
        return {
            "hits": self._cache_stats["hits"],
            "misses": self._cache_stats["misses"],
            "evictions": self._cache_stats["evictions"],
            "hit_rate": hit_rate,
            "cache_size": len(self._cache),
            "max_cache_size": len(self._cache) + self._cache_stats["evictions"]
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._cache_stats = {"hits": 0, "misses": 0, "evictions": 0}
        logger.info("âœ… ç¼“å­˜å·²æ¸…ç©º")
    
    def estimate_cost(self, num_documents: int) -> Dict[str, float]:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬"""
        # gte-rerank-v2: 0.0008å…ƒ/åƒè¾“å…¥Token
        cost_per_1k_tokens = 0.0008
        
        # ä¼°ç®—å¹³å‡æ¯ä¸ªæ–‡æ¡£çš„Tokenæ•°ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
        avg_tokens_per_doc = 200
        total_tokens = num_documents * avg_tokens_per_doc
        
        cost = (total_tokens / 1000) * cost_per_1k_tokens
        
        return {
            "total_tokens": total_tokens,
            "estimated_cost_cny": cost,
            "cost_per_1k_tokens": cost_per_1k_tokens
        }

class AdvancedReranker(TextReranker):
    """é«˜çº§æ–‡æœ¬æ’åºå™¨"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.weight_config = {
            "semantic_weight": 0.7,
            "keyword_weight": 0.2,
            "popularity_weight": 0.1
        }
    
    def hybrid_rank(self, 
                   query: str,
                   documents: List[RerankDocument],
                   query_embedding: List[float] = None,
                   doc_embeddings: List[List[float]] = None) -> List[RerankResult]:
        """
        æ··åˆæ’åºï¼ˆç»“åˆå¤šç§ä¿¡å·ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            query_embedding: æŸ¥è¯¢åµŒå…¥å‘é‡ï¼ˆå¯é€‰ï¼‰
            doc_embeddings: æ–‡æ¡£åµŒå…¥å‘é‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ’åºç»“æœ
        """
        # è·å–æ–‡æœ¬æ’åºç»“æœ
        rerank_results = self.rerank(query, documents)
        
        if not rerank_results or not query_embedding or not doc_embeddings:
            return rerank_results
        
        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        semantic_scores = []
        for i, (doc, embedding) in enumerate(zip(documents, doc_embeddings)):
            similarity = np.dot(query_embedding, embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(embedding)
            )
            semantic_scores.append((i, similarity))
        
        # åˆ›å»ºåˆ†æ•°æ˜ å°„
        rerank_scores = {r.original_rank: r.relevance_score for r in rerank_results}
        
        # è®¡ç®—æ··åˆåˆ†æ•°
        hybrid_results = []
        for result in rerank_results:
            original_idx = result.original_rank
            semantic_score = next(s for i, s in semantic_scores if i == original_idx)
            
            # æ ‡å‡†åŒ–åˆ†æ•°
            rerank_norm = result.relevance_score
            semantic_norm = (semantic_score + 1) / 2  # ä½™å¼¦ç›¸ä¼¼åº¦æ ‡å‡†åŒ–
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = (
                self.weight_config["semantic_weight"] * semantic_norm +
                self.weight_config["rerank_weight"] * rerank_norm
            )
            
            hybrid_result = RerankResult(
                document=result.document,
                relevance_score=final_score,
                original_rank=result.original_rank,
                new_rank=result.new_rank,
                rank_change=result.rank_change
            )
            hybrid_results.append(hybrid_result)
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        hybrid_results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        # æ›´æ–°æ’å
        for i, result in enumerate(hybrid_results):
            result.new_rank = i
        
        return hybrid_results

# å·¥å…·å‡½æ•°
def create_sample_documents(count: int = 10, category: str = "tech") -> List[RerankDocument]:
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
    sample_tech_docs = [
        "äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œç‰¹åˆ«æ˜¯åœ¨å½±åƒè¯†åˆ«å’Œç–¾ç—…é¢„æµ‹æ–¹é¢",
        "æ·±åº¦å­¦ä¹ ç®—æ³•åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œå‡†ç¡®ç‡å·²ç»è¶…è¿‡äººç±»æ°´å¹³",
        "åŒºå—é“¾æŠ€æœ¯ä¸ºé‡‘èè¡Œä¸šå¸¦æ¥äº†é©å‘½æ€§å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¯ä»˜å’Œæ¸…ç®—ç³»ç»Ÿæ–¹é¢",
        "é‡å­è®¡ç®—çš„å‘å±•å°†å½»åº•æ”¹å˜å¯†ç å­¦å’Œè®¡ç®—ç§‘å­¦çš„é¢è²Œ",
        "ç‰©è”ç½‘æŠ€æœ¯æ­£åœ¨è¿æ¥æ•°åäº¿è®¾å¤‡ï¼Œæ„å»ºæ™ºèƒ½åŒ–çš„ç”Ÿæ´»å’Œå·¥ä½œç¯å¢ƒ",
        "æœºå™¨å­¦ä¹ åœ¨é‡‘èé£é™©è¯„ä¼°ä¸­çš„åº”ç”¨æ˜¾è‘—æé«˜äº†é£é™©é¢„æµ‹çš„å‡†ç¡®æ€§",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ä½¿å¾—è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€",
        "è®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­çš„åº”ç”¨æ—¥ç›Šæˆç†Ÿ",
        "å¤§æ•°æ®åˆ†æå¸®åŠ©ä¼ä¸šä»æµ·é‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„å•†ä¸šæ´å¯Ÿ",
        "äº‘è®¡ç®—æŠ€æœ¯ä¸ºä¼ä¸šæä¾›äº†çµæ´»å¯æ‰©å±•çš„è®¡ç®—èµ„æº"
    ]
    
    return [
        RerankDocument(
            text=text,
            doc_id=f"doc_{i}",
            score=np.random.uniform(0.3, 0.9),
            metadata={"category": category, "index": i}
        )
        for i, text in enumerate(sample_tech_docs[:count])
    ]

def benchmark_rerank_performance(reranker: TextReranker, 
                               test_cases: List[Tuple[str, List[RerankDocument]]]) -> Dict:
    """åŸºå‡†æµ‹è¯•æ’åºæ€§èƒ½"""
    results = {
        "total_queries": len(test_cases),
        "total_documents": sum(len(docs) for _, docs in test_cases),
        "timing": {},
        "cache_stats": {}
    }
    
    start_time = time.time()
    
    for i, (query, documents) in enumerate(test_cases):
        case_start = time.time()
        reranked = reranker.rerank(query, documents)
        case_time = time.time() - case_start
        
        results["timing"][f"query_{i}"] = {
            "query": query,
            "document_count": len(documents),
            "time_seconds": case_time,
            "results_count": len(reranked)
        }
    
    total_time = time.time() - start_time
    results["total_time"] = total_time
    results["average_time_per_query"] = total_time / len(test_cases)
    results["cache_stats"] = reranker.get_cache_stats()
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–æ’åºå™¨
    reranker = TextReranker()
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    documents = create_sample_documents(5)
    query = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨"
    
    # æ‰§è¡Œæ’åº
    results = reranker.rerank(query, documents)
    
    print("ğŸ¯ æ–‡æœ¬æ’åºç»“æœ:")
    for i, result in enumerate(results):
        print(f"{i+1}. åˆ†æ•°: {result.relevance_score:.3f} - {result.document.text[:60]}...")
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    stats = reranker.get_cache_stats()
    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡: {stats}")
    
    # ä¼°ç®—æˆæœ¬
    cost = reranker.estimate_cost(len(documents))
    print(f"\nğŸ’° æˆæœ¬ä¼°ç®—: {cost}")