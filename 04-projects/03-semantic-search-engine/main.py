#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®æˆ˜é¡¹ç›®3ï¼šè¯­ä¹‰æœç´¢å¼•æ“
==================

åŸºäºæ–‡æœ¬åµŒå…¥çš„æ™ºèƒ½æœç´¢å¼•æ“ï¼Œå®ç°è¯­ä¹‰ç†è§£ã€æœç´¢ç»“æœèšç±»ã€ä¸ªæ€§åŒ–æ’åºã€‚

é¡¹ç›®åŠŸèƒ½ï¼š
1. è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç†è§£
2. è¯­ä¹‰æœç´¢åŒ¹é…
3. æœç´¢ç»“æœèšç±»
4. ä¸ªæ€§åŒ–æ’åº
5. æœç´¢å»ºè®®
6. æœç´¢åˆ†æ

æŠ€æœ¯æ ˆï¼š
- æ–‡æœ¬åµŒå…¥ï¼štext-embedding-v4
- å‘é‡ç´¢å¼•ï¼šFAISS
- èšç±»ç®—æ³•ï¼šK-means
- æ’åºç®—æ³•ï¼šBM25 + è¯­ä¹‰ç›¸ä¼¼åº¦
- Webæ¡†æ¶ï¼šFlask
"""

import os
import sys
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
import sqlite3
from datetime import datetime
import hashlib
from dataclasses import dataclass
import re
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
from utils.embedding_client import EmbeddingClient

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    doc_id: str
    title: str
    content: str
    score: float
    category: str
    highlights: List[str]
    metadata: Dict

class SemanticSearchEngine:
    """è¯­ä¹‰æœç´¢å¼•æ“"""
    
    def __init__(self, db_path: str = "search_engine.db"):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        self.client = EmbeddingClient()
        self.db_path = db_path
        self.init_database()
        self.documents = {}
        self.search_cache = {}
        
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ–‡æ¡£è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                category TEXT,
                tags TEXT,
                url TEXT,
                embedding TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                view_count INTEGER DEFAULT 0
            )
        ''')
        
        # æœç´¢å†å²è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query TEXT NOT NULL,
                user_id TEXT,
                results_count INTEGER,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # æœç´¢åˆ†æè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_analytics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query TEXT NOT NULL,
                clicked_doc_id TEXT,
                position INTEGER,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def add_document(self, doc_id: str, title: str, content: str, category: str = None, tags: List[str] = None, url: str = None):
        """æ·»åŠ æ–‡æ¡£"""
        # æ¸…ç†å†…å®¹
        clean_content = self.clean_text(content)
        
        # ç”ŸæˆåµŒå…¥
        embedding = self.client.get_embedding(f"{title} {clean_content}")
        embedding_str = json.dumps(embedding)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO documents (id, title, content, category, tags, url, embedding)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (doc_id, title, clean_content, category, json.dumps(tags or []), url, embedding_str))
        
        conn.commit()
        conn.close()
        
        # æ›´æ–°å†…å­˜ç¼“å­˜
        self.documents[doc_id] = {
            'title': title,
            'content': clean_content,
            'category': category,
            'tags': tags or [],
            'url': url,
            'embedding': embedding
        }
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s]', ' ', text)
        # åˆå¹¶ç©ºæ ¼
        text = ' '.join(text.split())
        return text
    
    def semantic_search(self, query: str, user_id: str = None, category: str = None, limit: int = 10) -> List[SearchResult]:
        """è¯­ä¹‰æœç´¢"""
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.client.get_embedding(query)
        
        # ç¼“å­˜æœç´¢
        cache_key = hashlib.md5(f"{query}_{category}_{limit}".encode()).hexdigest()
        if cache_key in self.search_cache:
            return self.search_cache[cache_key]
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ„å»ºæŸ¥è¯¢
        base_query = '''
            SELECT id, title, content, category, tags, url, view_count
            FROM documents
        '''
        params = []
        
        if category:
            base_query += ' WHERE category = ?'
            params.append(category)
        
        cursor.execute(base_query, params)
        docs = cursor.fetchall()
        conn.close()
        
        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        results = []
        query_vec = np.array(query_embedding)
        
        for doc in docs:
            doc_id, title, content, category, tags_str, url, view_count = doc
            
            # è·å–æ–‡æ¡£åµŒå…¥
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT embedding FROM documents WHERE id = ?', (doc_id,))
            embedding_str = cursor.fetchone()[0]
            conn.close()
            
            doc_vec = np.array(json.loads(embedding_str))
            
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            semantic_score = np.dot(query_vec, doc_vec) / (
                np.linalg.norm(query_vec) * np.linalg.norm(doc_vec)
            )
            
            # å…³é”®è¯åŒ¹é…åˆ†æ•°
            keyword_score = self.calculate_keyword_score(query, title, content)
            
            # ç»¼åˆåˆ†æ•°
            final_score = 0.7 * semantic_score + 0.3 * keyword_score
            
            # ç”Ÿæˆé«˜äº®
            highlights = self.generate_highlights(query, title, content)
            
            results.append(SearchResult(
                doc_id=doc_id,
                title=title,
                content=content[:200] + '...' if len(content) > 200 else content,
                score=final_score,
                category=category,
                highlights=highlights,
                metadata={
                    'url': url,
                    'view_count': view_count,
                    'tags': json.loads(tags_str)
                }
            ))
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        
        # è®°å½•æœç´¢å†å²
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO search_history (query, user_id, results_count)
            VALUES (?, ?, ?)
        ''', (query, user_id, len(results)))
        conn.commit()
        conn.close()
        
        # ç¼“å­˜ç»“æœ
        self.search_cache[cache_key] = results[:limit]
        
        return results[:limit]
    
    def calculate_keyword_score(self, query: str, title: str, content: str) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        query_words = set(query.lower().split())
        title_words = set(title.lower().split())
        content_words = set(content.lower().split())
        
        # æ ‡é¢˜åŒ¹é…
        title_match = len(query_words & title_words) / len(query_words) if query_words else 0
        
        # å†…å®¹åŒ¹é…
        content_match = len(query_words & content_words) / len(query_words) if query_words else 0
        
        return 0.6 * title_match + 0.4 * content_match
    
    def generate_highlights(self, query: str, title: str, content: str) -> List[str]:
        """ç”Ÿæˆæœç´¢é«˜äº®"""
        query_words = query.lower().split()
        
        highlights = []
        
        # æ ‡é¢˜é«˜äº®
        title_lower = title.lower()
        for word in query_words:
            if word in title_lower:
                start = title_lower.find(word)
                if start >= 0:
                    highlight = title[max(0, start-10):start+len(word)+10]
                    highlights.append(highlight)
        
        # å†…å®¹é«˜äº®
        content_lower = content.lower()
        for word in query_words:
            if word in content_lower:
                positions = [i for i in range(len(content_lower)) 
                           if content_lower.startswith(word, i)]
                for pos in positions[:2]:  # æœ€å¤š2ä¸ªé«˜äº®
                    highlight = content[max(0, pos-20):pos+len(word)+20]
                    highlights.append(highlight)
        
        return highlights[:3]  # æœ€å¤š3ä¸ªé«˜äº®
    
    def cluster_search_results(self, results: List[SearchResult], n_clusters: int = 3) -> Dict:
        """èšç±»æœç´¢ç»“æœ"""
        if len(results) < n_clusters:
            return {'clusters': [], 'noise': results}
        
        # æå–åµŒå…¥å‘é‡
        embeddings = []
        for result in results:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT embedding FROM documents WHERE id = ?', (result.doc_id,))
            embedding_str = cursor.fetchone()[0]
            conn.close()
            
            embeddings.append(json.loads(embedding_str))
        
        embeddings_array = np.array(embeddings)
        
        # ä½¿ç”¨K-meansèšç±»
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings_array)
        
        # æ„å»ºèšç±»ç»“æœ
        clusters = [[] for _ in range(n_clusters)]
        noise = []
        
        for i, (result, label) in enumerate(zip(results, labels)):
            clusters[label].append(result)
        
        # ä¸ºæ¯ä¸ªèšç±»ç”Ÿæˆä¸»é¢˜
        cluster_topics = []
        for i, cluster in enumerate(clusters):
            if cluster:
                # æå–å…³é”®è¯ä½œä¸ºä¸»é¢˜
                all_text = ' '.join([r.title + ' ' + r.content for r in cluster])
                words = re.findall(r'\w+', all_text.lower())
                word_counts = Counter(words)
                top_words = [w for w, c in word_counts.most_common(5) if len(w) > 2]
                
                cluster_topics.append({
                    'cluster_id': i,
                    'topic': ' '.join(top_words),
                    'documents': cluster,
                    'size': len(cluster)
                })
        
        return {'clusters': cluster_topics, 'noise': noise}
    
    def get_search_suggestions(self, query: str, limit: int = 5) -> List[str]:
        """è·å–æœç´¢å»ºè®®"""
        # åŸºäºå†å²æœç´¢å’Œæ–‡æ¡£å†…å®¹ç”Ÿæˆå»ºè®®
        suggestions = []
        
        # è·å–ç›¸ä¼¼çš„å†å²æŸ¥è¯¢
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT DISTINCT query FROM search_history
            WHERE query LIKE ? AND LENGTH(query) > 2
            ORDER BY timestamp DESC
            LIMIT 10
        ''', (f"{query}%",))
        
        history_queries = [row[0] for row in cursor.fetchall()]
        suggestions.extend(history_queries)
        
        # åŸºäºæ–‡æ¡£æ ‡é¢˜ç”Ÿæˆå»ºè®®
        cursor.execute('''
            SELECT DISTINCT title FROM documents
            WHERE title LIKE ?
            ORDER BY view_count DESC
            LIMIT 10
        ''', (f"%{query}%",))
        
        title_suggestions = [row[0] for row in cursor.fetchall()]
        suggestions.extend(title_suggestions)
        
        conn.close()
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_suggestions = list(set(suggestions))
        return unique_suggestions[:limit]
    
    def record_click(self, query: str, doc_id: str, position: int, user_id: str = None):
        """è®°å½•ç‚¹å‡»è¡Œä¸º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO search_analytics (query, clicked_doc_id, position)
            VALUES (?, ?, ?)
        ''', (query, doc_id, position))
        
        # æ›´æ–°æ–‡æ¡£æŸ¥çœ‹è®¡æ•°
        cursor.execute('''
            UPDATE documents SET view_count = view_count + 1 WHERE id = ?
        ''', (doc_id,))
        
        conn.commit()
        conn.close()
    
    def get_search_analytics(self) -> Dict:
        """è·å–æœç´¢åˆ†ææ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åŸºæœ¬ç»Ÿè®¡
        cursor.execute('SELECT COUNT(*) FROM search_history')
        total_searches = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM search_analytics')
        total_clicks = cursor.fetchone()[0]
        
        # çƒ­é—¨æŸ¥è¯¢
        cursor.execute('''
            SELECT query, COUNT(*) as count
            FROM search_history
            GROUP BY query
            ORDER BY count DESC
            LIMIT 10
        ''')
        top_queries = cursor.fetchall()
        
        # ç‚¹å‡»ç‡
        ctr = total_clicks / total_searches if total_searches > 0 else 0
        
        conn.close()
        
        return {
            'total_searches': total_searches,
            'total_clicks': total_clicks,
            'ctr': ctr,
            'top_queries': top_queries
        }
    
    def load_sample_documents(self):
        """åŠ è½½ç¤ºä¾‹æ–‡æ¡£"""
        sample_docs = [
            {
                'doc_id': 'doc_001',
                'title': 'æœºå™¨å­¦ä¹ å…¥é—¨æ•™ç¨‹',
                'content': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚ä¸»è¦ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚',
                'category': 'æŠ€æœ¯',
                'tags': ['æœºå™¨å­¦ä¹ ', 'AI', 'æ•™ç¨‹'],
                'url': '/docs/ml-intro'
            },
            {
                'doc_id': 'doc_002',
                'title': 'æ·±åº¦å­¦ä¹ å®æˆ˜æŒ‡å—',
                'content': 'æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œè§£å†³å¤æ‚é—®é¢˜ã€‚éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚',
                'category': 'æŠ€æœ¯',
                'tags': ['æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'å®æˆ˜'],
                'url': '/docs/deep-learning-guide'
            },
            {
                'doc_id': 'doc_003',
                'title': 'Pythonæ•°æ®åˆ†æå®Œå…¨æŒ‡å—',
                'content': 'Pythonæ˜¯æ•°æ®åˆ†æçš„é¦–é€‰è¯­è¨€ï¼Œä¸»è¦ä½¿ç”¨Pandasã€NumPyã€Matplotlibç­‰åº“ã€‚æœ¬æŒ‡å—æ¶µç›–ä»åŸºç¡€åˆ°é«˜çº§çš„æ•°æ®åˆ†ææŠ€å·§ã€‚',
                'category': 'ç¼–ç¨‹',
                'tags': ['Python', 'æ•°æ®åˆ†æ', 'Pandas'],
                'url': '/docs/python-data-analysis'
            },
            {
                'doc_id': 'doc_004',
                'title': 'åŒºå—é“¾æŠ€æœ¯åŸç†ä¸åº”ç”¨',
                'content': 'åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œé€šè¿‡å¯†ç å­¦ä¿è¯æ•°æ®å®‰å…¨ã€‚åº”ç”¨åŒ…æ‹¬åŠ å¯†è´§å¸ã€æ™ºèƒ½åˆçº¦ã€ä¾›åº”é“¾ç®¡ç†ç­‰ã€‚',
                'category': 'æŠ€æœ¯',
                'tags': ['åŒºå—é“¾', 'åˆ†å¸ƒå¼ç³»ç»Ÿ', 'åº”ç”¨'],
                'url': '/docs/blockchain-principles'
            }
        ]
        
        for doc in sample_docs:
            self.add_document(
                doc['doc_id'],
                doc['title'],
                doc['content'],
                doc['category'],
                doc['tags'],
                doc['url']
            )
    
    def demo_search_engine(self):
        """æ¼”ç¤ºæœç´¢å¼•æ“"""
        print("ğŸš€ å®æˆ˜é¡¹ç›®3ï¼šè¯­ä¹‰æœç´¢å¼•æ“")
        print("=" * 60)
        
        # åŠ è½½ç¤ºä¾‹æ–‡æ¡£
        print("ğŸ“š åŠ è½½ç¤ºä¾‹æ–‡æ¡£...")
        self.load_sample_documents()
        
        # æ¼”ç¤ºæœç´¢åŠŸèƒ½
        test_queries = [
            "æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ä¹ æ•™ç¨‹",
            "Pythonæ•°æ®åˆ†æ",
            "åŒºå—é“¾æŠ€æœ¯"
        ]
        
        print("\nğŸ¯ è¯­ä¹‰æœç´¢æ¼”ç¤º")
        print("=" * 50)
        
        for query in test_queries:
            print(f"\nâ“ æŸ¥è¯¢: {query}")
            results = self.semantic_search(query, limit=3)
            
            for i, result in enumerate(results, 1):
                print(f"\n   {i}. {result.title}")
                print(f"      åˆ†æ•°: {result.score:.3f}")
                print(f"      å†…å®¹: {result.content[:100]}...")
                print(f"      é«˜äº®: {result.highlights}")
        
        # æ¼”ç¤ºèšç±»åŠŸèƒ½
        print("\nğŸ¯ æœç´¢ç»“æœèšç±»æ¼”ç¤º")
        print("=" * 50)
        
        results = self.semantic_search("æŠ€æœ¯æ•™ç¨‹", limit=5)
        clusters = self.cluster_search_results(results, n_clusters=2)
        
        for cluster in clusters['clusters']:
            print(f"\nğŸ“Š èšç±»: {cluster['topic']}")
            print(f"   æ–‡æ¡£æ•°: {cluster['size']}")
            for doc in cluster['documents']:
                print(f"   - {doc.title}")
        
        # æ¼”ç¤ºæœç´¢å»ºè®®
        print("\nğŸ¯ æœç´¢å»ºè®®æ¼”ç¤º")
        print("=" * 50)
        
        suggestion_queries = ["æœºå™¨", "æ·±åº¦", "Python"]
        for query in suggestion_queries:
            suggestions = self.get_search_suggestions(query)
            print(f"\nâ“ æŸ¥è¯¢: {query}")
            print(f"   å»ºè®®: {suggestions}")
        
        # æ˜¾ç¤ºåˆ†ææ•°æ®
        print("\nğŸ“Š æœç´¢åˆ†ææ•°æ®")
        print("=" * 50)
        
        analytics = self.get_search_analytics()
        print(f"ğŸ“ˆ æ€»æœç´¢æ•°: {analytics['total_searches']}")
        print(f"ğŸ‘† æ€»ç‚¹å‡»æ•°: {analytics['total_clicks']}")
        print(f"ğŸ“Š ç‚¹å‡»ç‡: {analytics['ctr']:.2%}")
        
        if analytics['top_queries']:
            print("\nğŸ”¥ çƒ­é—¨æŸ¥è¯¢:")
            for query, count in analytics['top_queries']:
                print(f"   {query}: {count} æ¬¡")
        
        print("\nğŸ‰ è¯­ä¹‰æœç´¢å¼•æ“æ¼”ç¤ºå®Œæˆï¼")
        print("\nä¸‹ä¸€æ­¥ï¼š")
        print("   1. å¯åŠ¨æœç´¢æœåŠ¡: python search_service.py")
        print("   2. æ„å»ºç´¢å¼•: python build_index.py")
        print("   3. æµ‹è¯•æœç´¢API: python test_search.py")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è¯­ä¹‰æœç´¢å¼•æ“")
    print("=" * 60)
    
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    try:
        search_engine = SemanticSearchEngine()
        search_engine.demo_search_engine()
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()