#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®æˆ˜é¡¹ç›®4ï¼šæ–‡æ¡£åˆ†æå·¥å…·
==================

æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿï¼Œå®ç°æ–‡æ¡£åˆ†ç±»ã€å…³é”®ä¿¡æ¯æå–ã€ç›¸ä¼¼æ–‡æ¡£å‘ç°ã€åˆ†ææŠ¥å‘Šç”Ÿæˆã€‚

é¡¹ç›®åŠŸèƒ½ï¼š
1. æ–‡æ¡£è‡ªåŠ¨åˆ†ç±»
2. å…³é”®ä¿¡æ¯æå–
3. ç›¸ä¼¼æ–‡æ¡£å‘ç°
4. å†…å®¹æ‘˜è¦ç”Ÿæˆ
5. åˆ†ææŠ¥å‘Šå¯è§†åŒ–
6. æ‰¹é‡æ–‡æ¡£å¤„ç†

æŠ€æœ¯æ ˆï¼š
- æ–‡æ¡£å¤„ç†ï¼šPython-docx, PyPDF2
- æ–‡æœ¬åµŒå…¥ï¼štext-embedding-v4
- åˆ†ç±»ç®—æ³•ï¼šå¤šç§æœºå™¨å­¦ä¹ ç®—æ³•
- å¯è§†åŒ–ï¼šMatplotlib, Plotly
- æŠ¥å‘Šç”Ÿæˆï¼šJinja2æ¨¡æ¿
"""

import os
import sys
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
import sqlite3
from datetime import datetime
import hashlib
from dataclasses import dataclass
import re
from collections import Counter
import shutil
import mimetypes

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
from utils.embedding_client import EmbeddingClient

# å°è¯•å¯¼å…¥æ–‡æ¡£å¤„ç†åº“
try:
    import fitz  # PyMuPDF
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("âš ï¸ PyMuPDFæœªå®‰è£…ï¼ŒPDFå¤„ç†åŠŸèƒ½å—é™")

try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("âš ï¸ python-docxæœªå®‰è£…ï¼ŒWordå¤„ç†åŠŸèƒ½å—é™")

@dataclass
class DocumentInfo:
    """æ–‡æ¡£ä¿¡æ¯"""
    doc_id: str
    filename: str
    file_path: str
    file_type: str
    size: int
    created_at: datetime
    content: str
    category: str
    keywords: List[str]
    summary: str
    embedding: List[float]

class DocumentAnalyzer:
    """æ–‡æ¡£åˆ†æå·¥å…·"""
    
    def __init__(self, db_path: str = "document_analyzer.db"):
        """åˆå§‹åŒ–æ–‡æ¡£åˆ†æå™¨"""
        self.client = EmbeddingClient()
        self.db_path = db_path
        self.init_database()
        self.documents = {}
        self.supported_types = ['.txt', '.md', '.pdf', '.docx']
        
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ–‡æ¡£è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                filename TEXT NOT NULL,
                file_path TEXT NOT NULL,
                file_type TEXT,
                size INTEGER,
                content TEXT,
                category TEXT,
                keywords TEXT,
                summary TEXT,
                embedding TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # æ–‡æ¡£åˆ†ç±»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS document_categories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category_name TEXT NOT NULL,
                description TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ç›¸ä¼¼æ–‡æ¡£å…³ç³»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS document_similarity (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                doc1_id TEXT,
                doc2_id TEXT,
                similarity_score REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (doc1_id) REFERENCES documents(id),
                FOREIGN KEY (doc2_id) REFERENCES documents(id)
            )
        ''')
        
        # åˆ†æç»“æœè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analysis_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                doc_id TEXT,
                analysis_type TEXT,
                result_data TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (doc_id) REFERENCES documents(id)
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def extract_text_from_file(self, file_path: str) -> str:
        """ä»æ–‡ä»¶æå–æ–‡æœ¬"""
        if not os.path.exists(file_path):
            return ""
        
        file_extension = os.path.splitext(file_path)[1].lower()
        
        if file_extension == '.txt' or file_extension == '.md':
            return self.extract_text_from_txt(file_path)
        elif file_extension == '.pdf' and PDF_AVAILABLE:
            return self.extract_text_from_pdf(file_path)
        elif file_extension == '.docx' and DOCX_AVAILABLE:
            return self.extract_text_from_docx(file_path)
        else:
            return ""
    
    def extract_text_from_txt(self, file_path: str) -> str:
        """ä»æ–‡æœ¬æ–‡ä»¶æå–å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='gbk') as f:
                return f.read()
    
    def extract_text_from_pdf(self, file_path: str) -> str:
        """ä»PDFæå–æ–‡æœ¬"""
        if not PDF_AVAILABLE:
            return "PDFå¤„ç†åŠŸèƒ½ä¸å¯ç”¨"
        
        try:
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        except Exception as e:
            return f"PDFæå–å¤±è´¥: {str(e)}"
    
    def extract_text_from_docx(self, file_path: str) -> str:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        if not DOCX_AVAILABLE:
            return "Wordå¤„ç†åŠŸèƒ½ä¸å¯ç”¨"
        
        try:
            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            return f"Wordæå–å¤±è´¥: {str(e)}"
    
    def add_document(self, file_path: str, category: str = None) -> str:
        """æ·»åŠ æ–‡æ¡£"""
        if not os.path.exists(file_path):
            return None
        
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id = hashlib.md5(file_path.encode()).hexdigest()
        
        # æå–æ–‡æœ¬
        content = self.extract_text_from_file(file_path)
        if not content:
            return None
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        filename = os.path.basename(file_path)
        file_type = os.path.splitext(file_path)[1]
        file_size = os.path.getsize(file_path)
        
        # ç”Ÿæˆåˆ†ç±»
        if not category:
            category = self.auto_classify(content)
        
        # æå–å…³é”®è¯
        keywords = self.extract_keywords(content)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self.generate_summary(content)
        
        # ç”ŸæˆåµŒå…¥
        embedding = self.client.get_embedding(content)
        embedding_str = json.dumps(embedding)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO documents (id, filename, file_path, file_type, size, content, category, keywords, summary, embedding)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (doc_id, filename, file_path, file_type, file_size, content, category, json.dumps(keywords), summary, embedding_str))
        
        conn.commit()
        conn.close()
        
        # æ›´æ–°å†…å­˜ç¼“å­˜
        self.documents[doc_id] = DocumentInfo(
            doc_id=doc_id,
            filename=filename,
            file_path=file_path,
            file_type=file_type,
            size=file_size,
            created_at=datetime.now(),
            content=content,
            category=category,
            keywords=keywords,
            summary=summary,
            embedding=embedding
        )
        
        return doc_id
    
    def auto_classify(self, content: str) -> str:
        """è‡ªåŠ¨æ–‡æ¡£åˆ†ç±»"""
        # ç®€å•çš„å…³é”®è¯åˆ†ç±»
        tech_keywords = ['æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'äººå·¥æ™ºèƒ½', 'ç¼–ç¨‹', 'ç®—æ³•', 'æ•°æ®', 'ä»£ç ', 'å¼€å‘']
        business_keywords = ['å•†ä¸š', 'ç®¡ç†', 'è¥é”€', 'è´¢åŠ¡', 'æŠ•èµ„', 'ä¼ä¸š', 'å¸‚åœº', 'æˆ˜ç•¥']
        education_keywords = ['æ•™è‚²', 'å­¦ä¹ ', 'æ•™å­¦', 'è¯¾ç¨‹', 'åŸ¹è®­', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'çŸ¥è¯†']
        
        content_lower = content.lower()
        
        tech_score = sum(1 for kw in tech_keywords if kw in content_lower)
        business_score = sum(1 for kw in business_keywords if kw in content_lower)
        education_score = sum(1 for kw in education_keywords if kw in content_lower)
        
        scores = {'æŠ€æœ¯': tech_score, 'å•†ä¸š': business_score, 'æ•™è‚²': education_score}
        return max(scores, key=scores.get) or 'å…¶ä»–'
    
    def extract_keywords(self, content: str, n_keywords: int = 10) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        words = re.findall(r'\w+', content.lower())
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'ä¸º', 'å¯¹', 'ä¸­', 'ä¸Š', 'ä¸‹', 'è¿™', 'é‚£', 'æœ‰', 'å¯ä»¥', 'éœ€è¦', 'ä½¿ç”¨', 'è¿›è¡Œ', 'é€šè¿‡', 'èƒ½å¤Ÿ'}
        filtered_words = [w for w in words if w not in stop_words and len(w) > 1]
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(filtered_words)
        
        # è¿”å›é«˜é¢‘è¯
        return [w for w, c in word_counts.most_common(n_keywords)]
    
    def generate_summary(self, content: str, max_length: int = 200) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return content[:max_length] + '...' if len(content) > max_length else content
        
        # ç®€å•çš„æ‘˜è¦ï¼šé€‰æ‹©å‰å‡ ä¸ªå¥å­
        summary = ""
        for sentence in sentences[:3]:  # æœ€å¤š3ä¸ªå¥å­
            if len(summary) + len(sentence) <= max_length:
                summary += sentence + "ã€‚"
            else:
                break
        
        return summary or (content[:max_length] + '...' if len(content) > max_length else content)
    
    def find_similar_documents(self, doc_id: str, limit: int = 5) -> List[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£"""
        if doc_id not in self.documents:
            return []
        
        target_doc = self.documents[doc_id]
        target_embedding = np.array(target_doc.embedding)
        
        # è·å–æ‰€æœ‰æ–‡æ¡£
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT id, filename, content, category, keywords, embedding
            FROM documents WHERE id != ?
        ''', (doc_id,))
        
        docs = cursor.fetchall()
        conn.close()
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for doc_data in docs:
            doc_id2, filename, content, category, keywords_str, embedding_str = doc_data
            
            doc_embedding = np.array(json.loads(embedding_str))
            similarity = np.dot(target_embedding, doc_embedding) / (
                np.linalg.norm(target_embedding) * np.linalg.norm(doc_embedding)
            )
            
            keywords = json.loads(keywords_str)
            
            similarities.append({
                'doc_id': doc_id2,
                'filename': filename,
                'title': filename,  # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡é¢˜
                'content': content[:200] + '...' if len(content) > 200 else content,
                'category': category,
                'keywords': keywords,
                'similarity': similarity
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ä¿å­˜ç›¸ä¼¼å…³ç³»
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for sim in similarities[:limit]:
            cursor.execute('''
                INSERT OR REPLACE INTO document_similarity (doc1_id, doc2_id, similarity_score)
                VALUES (?, ?, ?)
            ''', (doc_id, sim['doc_id'], sim['similarity']))
        
        conn.commit()
        conn.close()
        
        return similarities[:limit]
    
    def batch_process_directory(self, directory: str, category: str = None) -> List[str]:
        """æ‰¹é‡å¤„ç†ç›®å½•"""
        if not os.path.exists(directory):
            return []
        
        processed_docs = []
        
        for filename in os.listdir(directory):
            file_path = os.path.join(directory, filename)
            
            if os.path.isfile(file_path):
                file_extension = os.path.splitext(filename)[1].lower()
                if file_extension in self.supported_types:
                    doc_id = self.add_document(file_path, category)
                    if doc_id:
                        processed_docs.append(doc_id)
        
        return processed_docs
    
    def generate_analysis_report(self, doc_id: str) -> Dict:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if doc_id not in self.documents:
            return {}
        
        doc = self.documents[doc_id]
        
        # è·å–ç›¸ä¼¼æ–‡æ¡£
        similar_docs = self.find_similar_documents(doc_id, limit=3)
        
        # ç»Ÿè®¡æ•°æ®
        content_length = len(doc.content)
        word_count = len(re.findall(r'\w+', doc.content))
        
        # åˆ†æç»“æœ
        report = {
            'document_info': {
                'doc_id': doc.doc_id,
                'filename': doc.filename,
                'file_type': doc.file_type,
                'size': doc.size,
                'category': doc.category,
                'keywords': doc.keywords,
                'summary': doc.summary
            },
            'statistics': {
                'content_length': content_length,
                'word_count': word_count,
                'keywords_count': len(doc.keywords)
            },
            'similar_documents': similar_docs,
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO analysis_results (doc_id, analysis_type, result_data)
            VALUES (?, ?, ?)
        ''', (doc_id, 'comprehensive_analysis', json.dumps(report)))
        
        conn.commit()
        conn.close()
        
        return report
    
    def load_sample_documents(self) -> List[str]:
        """åŠ è½½ç¤ºä¾‹æ–‡æ¡£"""
        sample_contents = [
            {
                'title': 'æœºå™¨å­¦ä¹ åŸºç¡€æ•™ç¨‹',
                'content': '''
                æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚
                ä¸»è¦ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚
                ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œæ— ç›‘ç£å­¦ä¹ å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ï¼Œ
                å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±å’Œæƒ©ç½šæœºåˆ¶ä¼˜åŒ–å†³ç­–ã€‚
                ''',
                'category': 'æŠ€æœ¯',
                'tags': ['æœºå™¨å­¦ä¹ ', 'AI', 'æ•™ç¨‹']
            },
            {
                'title': 'Pythonç¼–ç¨‹å…¥é—¨',
                'content': '''
                Pythonæ˜¯ä¸€ç§æ˜“å­¦æ˜“ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›åº”ç”¨äºæ•°æ®åˆ†æã€æœºå™¨å­¦ä¹ ã€Webå¼€å‘ç­‰é¢†åŸŸã€‚
                æœ¬æ•™ç¨‹æ¶µç›–PythonåŸºç¡€è¯­æ³•ã€æ•°æ®ç»“æ„ã€å‡½æ•°ã€ç±»ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚
                é€šè¿‡å®ä¾‹å­¦ä¹ å¦‚ä½•ç¼–å†™é«˜æ•ˆã€å¯ç»´æŠ¤çš„Pythonä»£ç ã€‚
                ''',
                'category': 'ç¼–ç¨‹',
                'tags': ['Python', 'ç¼–ç¨‹', 'å…¥é—¨']
            },
            {
                'title': 'æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹',
                'content': '''
                æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ•°æ®æ¸…æ´—ã€æ•°æ®æ¢ç´¢ã€æ•°æ®å»ºæ¨¡å’Œç»“æœè§£é‡Šã€‚
                ä½¿ç”¨Pythonå’Œç›¸å…³åº“å¦‚Pandasã€NumPyã€Scikit-learnç­‰å·¥å…·è¿›è¡Œæ•°æ®åˆ†æã€‚
                é€šè¿‡å¯è§†åŒ–æŠ€æœ¯å±•ç¤ºæ•°æ®æ´å¯Ÿï¼Œæ”¯æŒä¸šåŠ¡å†³ç­–ã€‚
                ''',
                'category': 'æ•°æ®åˆ†æ',
                'tags': ['æ•°æ®ç§‘å­¦', 'å·¥ä½œæµç¨‹', 'Python']
            }
        ]
        
        doc_ids = []
        for i, content in enumerate(sample_contents):
            doc_id = f"sample_{i+1}"
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = f"temp_doc_{i+1}.txt"
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(content['content'])
            
            # æ·»åŠ æ–‡æ¡£
            doc_id = self.add_document(temp_file, content['category'])
            if doc_id:
                doc_ids.append(doc_id)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_file)
        
        return doc_ids
    
    def demo_document_analyzer(self):
        """æ¼”ç¤ºæ–‡æ¡£åˆ†æå·¥å…·"""
        print("ğŸš€ å®æˆ˜é¡¹ç›®4ï¼šæ–‡æ¡£åˆ†æå·¥å…·")
        print("=" * 60)
        
        # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç›®å½•
        sample_dir = "sample_documents"
        if not os.path.exists(sample_dir):
            os.makedirs(sample_dir)
        
        # åŠ è½½ç¤ºä¾‹æ–‡æ¡£
        print("ğŸ“š åˆ›å»ºç¤ºä¾‹æ–‡æ¡£...")
        doc_ids = self.load_sample_documents()
        
        print(f"âœ… å·²åŠ è½½ {len(doc_ids)} ä¸ªç¤ºä¾‹æ–‡æ¡£")
        
        # æ¼”ç¤ºæ–‡æ¡£åˆ†æåŠŸèƒ½
        if doc_ids:
            print("\nğŸ¯ æ–‡æ¡£åˆ†ææ¼”ç¤º")
            print("=" * 50)
            
            for doc_id in doc_ids:
                doc = self.documents[doc_id]
                
                print(f"\nğŸ“„ æ–‡æ¡£: {doc.filename}")
                print(f"   ç±»åˆ«: {doc.category}")
                print(f"   å…³é”®è¯: {', '.join(doc.keywords[:5])}")
                print(f"   æ‘˜è¦: {doc.summary}")
                
                # æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£
                similar_docs = self.find_similar_documents(doc_id, limit=2)
                if similar_docs:
                    print("   ç›¸ä¼¼æ–‡æ¡£:")
                    for sim in similar_docs:
                        print(f"     - {sim['filename']} (ç›¸ä¼¼åº¦: {sim['similarity']:.3f})")
        
        # æ¼”ç¤ºæ‰¹é‡å¤„ç†
        print("\nğŸ¯ æ‰¹é‡å¤„ç†æ¼”ç¤º")
        print("=" * 50)
        
        batch_docs = self.batch_process_directory(sample_dir, category='æœªåˆ†ç±»')
        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: {len(batch_docs)} ä¸ªæ–‡æ¡£")
        
        # æ¼”ç¤ºåˆ†ææŠ¥å‘Š
        print("\nğŸ¯ åˆ†ææŠ¥å‘Šæ¼”ç¤º")
        print("=" * 50)
        
        if doc_ids:
            report = self.generate_analysis_report(doc_ids[0])
            
            print("ğŸ“Š åˆ†ææŠ¥å‘Š:")
            print(f"   æ–‡æ¡£: {report['document_info']['filename']}")
            print(f"   ç±»åˆ«: {report['document_info']['category']}")
            print(f"   è¯æ•°: {report['statistics']['word_count']}")
            print(f"   å…³é”®è¯: {len(report['document_info']['keywords'])}")
            
            if report['similar_documents']:
                print("   ç›¸ä¼¼æ–‡æ¡£åˆ†æ:")
                for sim in report['similar_documents']:
                    print(f"     - {sim['filename']}: {sim['similarity']:.3f}")
        
        # æ¸…ç†ç¤ºä¾‹ç›®å½•
        if os.path.exists(sample_dir):
            shutil.rmtree(sample_dir)
        
        print("\nğŸ‰ æ–‡æ¡£åˆ†æå·¥å…·æ¼”ç¤ºå®Œæˆï¼")
        print("\nä¸‹ä¸€æ­¥ï¼š")
        print("   1. è¿è¡Œåˆ†ææœåŠ¡: python analyzer_service.py")
        print("   2. å¯åŠ¨Webç•Œé¢: python web_app.py")
        print("   3. æµ‹è¯•åˆ†æAPI: python test_analyzer.py")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ–‡æ¡£åˆ†æå·¥å…·")
    print("=" * 60)
    
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    try:
        analyzer = DocumentAnalyzer()
        analyzer.demo_document_analyzer()
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()