#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ•™ç¨‹ç¬¬8è¯¾ï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•ä¸ä¼˜åŒ–
================================

æœ¬è¯¾ç¨‹å°†æ·±å…¥åˆ†ææ–‡æœ¬æ’åºæ¨¡å‹çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. å»ºç«‹å®Œå–„çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ä½“ç³»
2. åˆ†æä¸åŒè§„æ¨¡æ•°æ®ä¸‹çš„æ€§èƒ½è¡¨ç°
3. æŒæ¡ç¼“å­˜å’Œæ‰¹å¤„ç†ä¼˜åŒ–æŠ€å·§
4. ç›‘æ§APIè°ƒç”¨æˆæœ¬å’Œæ•ˆç‡
5. è®¾è®¡è‡ªé€‚åº”æ’åºç­–ç•¥

"""

import os
import sys
import json
import time
import numpy as np
from typing import List, Dict, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from concurrent.futures import ThreadPoolExecutor, as_completed
import matplotlib.font_manager as fm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.embedding_client import EmbeddingClient
from utils.text_reranker import TextReranker, RerankDocument, create_sample_documents

# è®¾ç½®ä¸­æ–‡å­—ä½“ - ä¼˜å…ˆæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
# æ•°å­¦ç¬¦å·è­¦å‘Šå¯ä»¥å¿½ç•¥ï¼Œä¸å½±å“ä¸­æ–‡æ˜¾ç¤º

# é…ç½®
import dashscope
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    document_count: int
    query_count: int
    total_time: float
    avg_time_per_query: float
    avg_time_per_document: float
    throughput: float  # docs/sec
    cache_hit_rate: float
    api_calls: int
    estimated_cost: float
    memory_usage_mb: float
    error_rate: float

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•å™¨"""
        self.embedding_client = EmbeddingClient()
        self.reranker = TextReranker()
        
        # æµ‹è¯•é…ç½®
        self.test_configs = [
            {"name": "å°è§„æ¨¡", "doc_count": 10, "query_count": 5},
            {"name": "ä¸­ç­‰è§„æ¨¡", "doc_count": 50, "query_count": 10},
            {"name": "å¤§è§„æ¨¡", "doc_count": 100, "query_count": 20},
            {"name": "è¶…å¤§è§„æ¨¡", "doc_count": 200, "query_count": 30}
        ]
        
        # æµ‹è¯•æŸ¥è¯¢
        self.test_queries = [
            "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨",
            "åŒºå—é“¾æŠ€æœ¯åœ¨é‡‘èæ”¯ä»˜ä¸­çš„åº”ç”¨",
            "æ·±åº¦å­¦ä¹ ç®—æ³•çš„æœ€æ–°è¿›å±•",
            "äº‘è®¡ç®—æŠ€æœ¯çš„æˆæœ¬ä¼˜åŒ–",
            "ç‰©è”ç½‘è®¾å¤‡çš„å®‰å…¨é˜²æŠ¤"
        ]
        
        print("âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•å¯åŠ¨")
        print("=" * 50)
    
    def generate_test_data(self, doc_count: int, query_count: int) -> Tuple[List[str], List[List[RerankDocument]]]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        queries = self.test_queries[:query_count]
        
        # ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆä¸åŒçš„æ–‡æ¡£é›†
        documents_list = []
        for i in range(query_count):
            # åˆ›å»ºç›¸å…³å’Œä¸ç›¸å…³çš„æ··åˆæ–‡æ¡£
            relevant_docs = create_sample_documents(min(doc_count // 2, 10))
            irrelevant_docs = create_sample_documents(max(doc_count - len(relevant_docs), 5))
            
            # ä¿®æ”¹ä¸ç›¸å…³æ–‡æ¡£çš„å†…å®¹
            for doc in irrelevant_docs:
                doc.text = f"è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£: {doc.text[:50]}..."
            
            all_docs = relevant_docs + irrelevant_docs
            documents_list.append(all_docs)
        
        return queries, documents_list
    
    def benchmark_single_scale(self, config: Dict) -> BenchmarkResult:
        """æµ‹è¯•å•ä¸ªè§„æ¨¡"""
        print(f"ğŸ§ª æµ‹è¯•è§„æ¨¡: {config['name']} ({config['doc_count']}æ–‡æ¡£, {config['query_count']}æŸ¥è¯¢)")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        queries, documents_list = self.generate_test_data(
            config["doc_count"], config["query_count"]
        )
        
        # é‡ç½®ç¼“å­˜
        self.reranker.clear_cache()
        
        # å¼€å§‹æµ‹è¯•
        start_time = time.time()
        api_calls = 0
        total_documents = 0
        
        for query, documents in zip(queries, documents_list):
            total_documents += len(documents)
            
            # æ‰§è¡Œæ’åº
            reranked = self.reranker.rerank(query, documents)
            api_calls += 1
        
        total_time = time.time() - start_time
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.reranker.get_cache_stats()
        
        # è®¡ç®—æˆæœ¬
        cost = self.reranker.estimate_cost(total_documents)["estimated_cost_cny"]
        
        return BenchmarkResult(
            test_name=config["name"],
            document_count=config["doc_count"],
            query_count=config["query_count"],
            total_time=total_time,
            avg_time_per_query=total_time / config["query_count"],
            avg_time_per_document=total_time / total_documents,
            throughput=total_documents / total_time,
            cache_hit_rate=cache_stats["hit_rate"],
            api_calls=api_calls,
            estimated_cost=cost,
            memory_usage_mb=0,  # ç®€åŒ–å¤„ç†
            error_rate=0  # ç®€åŒ–å¤„ç†
        )
    
    def benchmark_all_scales(self) -> List[BenchmarkResult]:
        """æµ‹è¯•æ‰€æœ‰è§„æ¨¡"""
        results = []
        
        for config in self.test_configs:
            try:
                result = self.benchmark_single_scale(config)
                results.append(result)
                print(f"   âœ… å®Œæˆ - æ€»æ—¶é—´: {result.total_time:.2f}s, ååé‡: {result.throughput:.1f} docs/sec")
            except Exception as e:
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
                # åˆ›å»ºé”™è¯¯ç»“æœ
                results.append(BenchmarkResult(
                    test_name=config["name"],
                    document_count=config["doc_count"],
                    query_count=config["query_count"],
                    total_time=0,
                    avg_time_per_query=0,
                    avg_time_per_document=0,
                    throughput=0,
                    cache_hit_rate=0,
                    api_calls=0,
                    estimated_cost=0,
                    memory_usage_mb=0,
                    error_rate=1.0
                ))
        
        return results
    
    def benchmark_cache_impact(self) -> Dict:
        """æµ‹è¯•ç¼“å­˜å½±å“"""
        print("\nğŸ’¾ ç¼“å­˜æ€§èƒ½æµ‹è¯•")
        
        # æµ‹è¯•æ•°æ®
        query = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨"
        documents = create_sample_documents(50)
        
        # é¦–æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰
        self.reranker.clear_cache()
        start_time = time.time()
        result1 = self.reranker.rerank(query, documents)
        first_time = time.time() - start_time
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆæœ‰ç¼“å­˜ï¼‰
        start_time = time.time()
        result2 = self.reranker.rerank(query, documents)
        cached_time = time.time() - start_time
        
        # ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.reranker.get_cache_stats()
        
        return {
            "first_call_time": first_time,
            "cached_call_time": cached_time,
            "speedup_factor": first_time / cached_time if cached_time > 0 else 0,
            "cache_hit_rate": cache_stats["hit_rate"],
            "cache_size": cache_stats["cache_size"]
        }
    
    def benchmark_concurrent_requests(self, 
                                    concurrent_requests: int = 5,
                                    doc_count: int = 20) -> Dict:
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        print(f"\nâš¡ å¹¶å‘æµ‹è¯• ({concurrent_requests}å¹¶å‘è¯·æ±‚)")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = []
        for i in range(concurrent_requests):
            query = f"æµ‹è¯•æŸ¥è¯¢ {i+1}"
            documents = create_sample_documents(doc_count)
            test_data.append((query, documents))
        
        # ä¸²è¡Œæ‰§è¡Œ
        start_time = time.time()
        for query, documents in test_data:
            self.reranker.rerank(query, documents)
        serial_time = time.time() - start_time
        
        # å¹¶å‘æ‰§è¡Œï¼ˆæ¨¡æ‹Ÿï¼‰
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
            futures = [
                executor.submit(self.reranker.rerank, query, documents)
                for query, documents in test_data
            ]
            results = [future.result() for future in as_completed(futures)]
        concurrent_time = time.time() - start_time
        
        return {
            "serial_time": serial_time,
            "concurrent_time": concurrent_time,
            "concurrency_overhead": concurrent_time - serial_time,
            "throughput_improvement": serial_time / concurrent_time if concurrent_time > 0 else 0
        }
    
    def benchmark_batch_processing(self, batch_sizes: List[int] = [10, 25, 50, 100]) -> Dict:
        """æµ‹è¯•æ‰¹å¤„ç†æ€§èƒ½"""
        print("\nğŸ“¦ æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•")
        
        base_query = "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¶‹åŠ¿"
        base_documents = create_sample_documents(100)
        
        results = {}
        
        for batch_size in batch_sizes:
            if batch_size > len(base_documents):
                continue
                
            documents = base_documents[:batch_size]
            
            start_time = time.time()
            result = self.reranker.rerank(base_query, documents)
            processing_time = time.time() - start_time
            
            cost = self.reranker.estimate_cost(batch_size)["estimated_cost_cny"]
            
            results[batch_size] = {
                "batch_size": batch_size,
                "processing_time": processing_time,
                "throughput": batch_size / processing_time,
                "cost_per_document": cost / batch_size,
                "cost_efficiency": batch_size / cost if cost > 0 else 0
            }
        
        return results
    
    def generate_optimization_report(self, results: List[BenchmarkResult]) -> Dict:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report = {
            "summary": {},
            "recommendations": [],
            "optimal_configs": {},
            "cost_analysis": {}
        }
        
        # æ€§èƒ½åˆ†æ
        valid_results = [r for r in results if r.error_rate == 0]
        if not valid_results:
            report["summary"]["status"] = "æ‰€æœ‰æµ‹è¯•å¤±è´¥"
            return report
        
        # æ‰¾å‡ºæœ€ä½³é…ç½®
        best_throughput = max(r.throughput for r in valid_results)
        best_config = next(r for r in valid_results if r.throughput == best_throughput)
        
        report["summary"].update({
            "best_throughput": best_throughput,
            "best_config": best_config.test_name,
            "total_tested_configs": len(results),
            "successful_configs": len(valid_results)
        })
        
        # æˆæœ¬åˆ†æ
        total_cost = sum(r.estimated_cost for r in valid_results)
        avg_cost_per_query = np.mean([r.estimated_cost for r in valid_results])
        
        report["cost_analysis"].update({
            "total_test_cost": total_cost,
            "avg_cost_per_query": avg_cost_per_query,
            "cost_efficiency_threshold": 0.001  # å…ƒ/æŸ¥è¯¢
        })
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if best_config.document_count <= 50:
            report["recommendations"].append(
                "å°è§„æ¨¡æ•°æ®(â‰¤50æ–‡æ¡£)å¯ç›´æ¥ä½¿ç”¨æ–‡æœ¬æ’åºæ¨¡å‹"
            )
        
        if best_config.cache_hit_rate > 0.5:
            report["recommendations"].append(
                "å¯ç”¨ç¼“å­˜å¯æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå»ºè®®ç¼“å­˜TTLè®¾ä¸º1å°æ—¶"
            )
        
        report["recommendations"].append(
            f"å½“å‰æœ€ä½³é…ç½®: {best_config.test_name} - ååé‡{best_throughput:.1f} docs/sec"
        )
        
        return report
    
    def visualize_benchmark_results(self, results: List[BenchmarkResult]):
        """å¯è§†åŒ–åŸºå‡†æµ‹è¯•ç»“æœ"""
        valid_results = [r for r in results if r.error_rate == 0]
        
        if not valid_results:
            print("âŒ æ— æœ‰æ•ˆæ•°æ®ç”¨äºå¯è§†åŒ–")
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle("æ–‡æœ¬æ’åºæ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ", fontsize=16)
        
        # æ•°æ®å‡†å¤‡
        scales = [r.test_name for r in valid_results]
        throughputs = [r.throughput for r in valid_results]
        times = [r.avg_time_per_query for r in valid_results]
        costs = [r.estimated_cost for r in valid_results]
        
        # ååé‡å¯¹æ¯”
        axes[0, 0].bar(scales, throughputs, color='skyblue')
        axes[0, 0].set_title("ååé‡å¯¹æ¯”")
        axes[0, 0].set_ylabel("ååé‡ (æ–‡æ¡£/ç§’)")
        
        # å¤„ç†æ—¶é—´å¯¹æ¯”
        axes[0, 1].bar(scales, times, color='lightgreen')
        axes[0, 1].set_title("å¹³å‡å¤„ç†æ—¶é—´")
        axes[0, 1].set_ylabel("æ—¶é—´ (ç§’)")
        
        # æˆæœ¬å¯¹æ¯”
        axes[1, 0].bar(scales, costs, color='salmon')
        axes[1, 0].set_title("é¢„ä¼°æˆæœ¬ (å…ƒ/æŸ¥è¯¢)")
        axes[1, 0].set_ylabel("æˆæœ¬(å…ƒ)")
        
        # ç¼“å­˜å‘½ä¸­ç‡
        cache_rates = [r.cache_hit_rate for r in valid_results]
        axes[1, 1].bar(scales, cache_rates, color='gold')
        axes[1, 1].set_title("ç¼“å­˜å‘½ä¸­ç‡")
        axes[1, 1].set_ylabel("å‘½ä¸­ç‡")
        axes[1, 1].set_ylim(0, 1)
        
        plt.tight_layout()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(f"benchmark_results_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.show()
    
    def run_complete_benchmark(self):
        """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        if not os.getenv("DASHSCOPE_API_KEY"):
            print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
            return
        
        try:
            # 1. åŸºç¡€è§„æ¨¡æµ‹è¯•
            print("\n1ï¸âƒ£ åŸºç¡€è§„æ¨¡æµ‹è¯•")
            scale_results = self.benchmark_all_scales()
            
            # 2. ç¼“å­˜æµ‹è¯•
            print("\n2ï¸âƒ£ ç¼“å­˜æ€§èƒ½æµ‹è¯•")
            cache_results = self.benchmark_cache_impact()
            
            # 3. å¹¶å‘æµ‹è¯•
            print("\n3ï¸âƒ£ å¹¶å‘æ€§èƒ½æµ‹è¯•")
            concurrent_results = self.benchmark_concurrent_requests()
            
            # 4. æ‰¹å¤„ç†æµ‹è¯•
            print("\n4ï¸âƒ£ æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•")
            batch_results = self.benchmark_batch_processing()
            
            # 5. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
            print("\n5ï¸âƒ£ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š")
            optimization_report = self.generate_optimization_report(scale_results)
            
            # 6. å¯è§†åŒ–ç»“æœ
            self.visualize_benchmark_results(scale_results)
            
            # 7. ä¿å­˜å®Œæ•´ç»“æœ
            self._save_benchmark_results({
                "scale_results": [asdict(r) for r in scale_results],
                "cache_results": cache_results,
                "concurrent_results": concurrent_results,
                "batch_results": batch_results,
                "optimization_report": optimization_report,
                "timestamp": datetime.now().isoformat()
            })
            
            print("\nğŸ“Š åŸºå‡†æµ‹è¯•å®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
    
    def _save_benchmark_results(self, results: Dict):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_benchmark_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    def quick_performance_check(self) -> Dict:
        """å¿«é€Ÿæ€§èƒ½æ£€æŸ¥"""
        print("âš¡ å¿«é€Ÿæ€§èƒ½æ£€æŸ¥...")
        
        # å°è§„æ¨¡æµ‹è¯•
        query = "äººå·¥æ™ºèƒ½åº”ç”¨"
        documents = create_sample_documents(10)
        
        start_time = time.time()
        result = self.reranker.rerank(query, documents)
        processing_time = time.time() - start_time
        
        cost = self.reranker.estimate_cost(len(documents))
        
        return {
            "status": "ok",
            "processing_time": processing_time,
            "documents_processed": len(documents),
            "estimated_cost": cost["estimated_cost_cny"],
            "throughput": len(documents) / processing_time,
            "timestamp": datetime.now().isoformat()
        }

def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    benchmark.run_complete_benchmark()

if __name__ == "__main__":
    main()