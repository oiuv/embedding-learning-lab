#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ•™ç¨‹ç¬¬7è¯¾ï¼šæ’åºæ¨¡å‹å¯¹æ¯”æ¼”ç¤º
==============================

æœ¬è¯¾ç¨‹å°†å¯¹æ¯”å±•ç¤ºæ–‡æœ¬æ’åºæ¨¡å‹ä¸ç°æœ‰åµŒå…¥æ¨¡å‹çš„å·®å¼‚å’Œä¼˜åŠ¿ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£ä¸åŒæ’åºæ–¹æ³•çš„ä¼˜ç¼ºç‚¹
2. æŒæ¡æ€§èƒ½å¯¹æ¯”åˆ†ææ–¹æ³•
3. å­¦ä¼šé€‰æ‹©åˆé€‚çš„æ’åºç­–ç•¥
4. åˆ†ææ’åºç»“æœçš„è´¨é‡å·®å¼‚
5. ä¼˜åŒ–æ’åºå‚æ•°é…ç½®

"""

import os
import sys
import json
import numpy as np
from typing import List, Dict, Tuple
from dataclasses import dataclass
from datetime import datetime
import time
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm

# è®¾ç½®ä¸­æ–‡å­—ä½“ - ä¼˜å…ˆæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
# æ•°å­¦ç¬¦å·è­¦å‘Šå¯ä»¥å¿½ç•¥ï¼Œä¸å½±å“ä¸­æ–‡æ˜¾ç¤º

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.embedding_client import EmbeddingClient
from utils.text_reranker import TextReranker, RerankDocument, RerankResult

# åˆå§‹åŒ–DashScope
import dashscope
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

@dataclass
class ComparisonMetrics:
    """å¯¹æ¯”æŒ‡æ ‡"""
    method: str
    query: str
    top_k: int
    precision: float
    recall: float
    f1_score: float
    avg_precision: float
    ndcg: float
    processing_time: float
    cost_estimate: float

class SortingComparison:
    """æ’åºå¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¯¹æ¯”åˆ†æå™¨"""
        self.embedding_client = EmbeddingClient()
        self.reranker = TextReranker()
        
        # è¯„ä¼°æ•°æ®
        self.evaluation_queries = [
            {
                "query": "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨",
                "relevant_docs": [0, 1, 2, 5, 8],
                "description": "åŒ»ç–—AIåº”ç”¨"
            },
            {
                "query": "åŒºå—é“¾æŠ€æœ¯åœ¨é‡‘èæ”¯ä»˜ä¸­çš„åº”ç”¨",
                "relevant_docs": [3, 6, 9, 12, 15],
                "description": "åŒºå—é“¾é‡‘èåº”ç”¨"
            },
            {
                "query": "æ·±åº¦å­¦ä¹ ç®—æ³•çš„æœ€æ–°è¿›å±•",
                "relevant_docs": [1, 4, 7, 10, 13],
                "description": "æ·±åº¦å­¦ä¹ è¿›å±•"
            }
        ]
        
        # ç”Ÿæˆæµ‹è¯•æ–‡æ¡£
        self.test_documents = self._generate_test_corpus()
        
        print("ğŸ” æ’åºæ¨¡å‹å¯¹æ¯”åˆ†æå¯åŠ¨")
        print("=" * 50)
    
    def _generate_test_corpus(self) -> List[RerankDocument]:
        """ç”Ÿæˆæµ‹è¯•æ–‡æ¡£è¯­æ–™"""
        documents = [
            # åŒ»ç–—AIç›¸å…³
            "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—å½±åƒè¯Šæ–­ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œç‰¹åˆ«æ˜¯åœ¨CTå’ŒMRIåˆ†ææ–¹é¢ï¼Œå‡†ç¡®ç‡å·²è¾¾åˆ°95%ä»¥ä¸Š",
            "æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—…ï¼Œé€šè¿‡åˆ†ææ‚£è€…çš„å†å²æ•°æ®æé«˜è¯Šæ–­æ•ˆç‡å’Œå‡†ç¡®æ€§",
            "æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨åŒ…æ‹¬ç–¾ç—…é¢„æµ‹ã€è¯ç‰©ç ”å‘å’Œä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆåˆ¶å®š",
            # åŒºå—é“¾ç›¸å…³
            "åŒºå—é“¾æŠ€æœ¯ä¸ºé‡‘èè¡Œä¸šå¸¦æ¥äº†é©å‘½æ€§å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨å¢ƒæ”¯ä»˜å’Œæ•°å­—è´§å¸äº¤æ˜“æ–¹é¢",
            "æ™ºèƒ½åˆçº¦æŠ€æœ¯ä½¿å¾—é‡‘èäº¤æ˜“æ›´åŠ é€æ˜å’Œé«˜æ•ˆï¼Œå‡å°‘äº†ä¸­é—´ç¯èŠ‚å’Œäº¤æ˜“æˆæœ¬",
            "å»ä¸­å¿ƒåŒ–é‡‘è(DeFi)åº”ç”¨åŸºäºåŒºå—é“¾æŠ€æœ¯ï¼Œä¸ºç”¨æˆ·æä¾›äº†æ— éœ€ä¼ ç»Ÿé“¶è¡Œçš„é‡‘èæœåŠ¡",
            # æ·±åº¦å­¦ä¹ ç›¸å…³
            "æ·±åº¦å­¦ä¹ ç®—æ³•åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œå‡†ç¡®ç‡å·²ç»è¶…è¿‡äººç±»æ°´å¹³",
            "ç¥ç»ç½‘ç»œæ¶æ„çš„ä¼˜åŒ–ä½¿å¾—æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡",
            "Transformeræ¶æ„çš„å‡ºç°å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯å‘å±•æ–¹å‘",
            # å…¶ä»–ç›¸å…³
            "äº‘è®¡ç®—æŠ€æœ¯ä¸ºä¼ä¸šæä¾›äº†çµæ´»å¯æ‰©å±•çš„è®¡ç®—èµ„æºï¼Œé™ä½äº†ITåŸºç¡€è®¾æ–½æˆæœ¬",
            "å¤§æ•°æ®åˆ†æå¸®åŠ©ä¼ä¸šä»æµ·é‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„å•†ä¸šæ´å¯Ÿå’Œå¸‚åœºè¶‹åŠ¿",
            "ç‰©è”ç½‘æŠ€æœ¯æ­£åœ¨è¿æ¥æ•°åäº¿è®¾å¤‡ï¼Œæ„å»ºæ™ºèƒ½åŒ–çš„ç”Ÿæ´»å’Œå·¥ä½œç¯å¢ƒ",
            "è®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­çš„åº”ç”¨æ—¥ç›Šæˆç†Ÿï¼Œå®‰å…¨æ€§ä¸æ–­æå‡",
            "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ä½¿å¾—è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œæ”¯æŒå¤šè¯­è¨€äº¤æµ",
            "æœºå™¨å­¦ä¹ åœ¨é‡‘èé£é™©è¯„ä¼°ä¸­çš„åº”ç”¨æ˜¾è‘—æé«˜äº†é£é™©é¢„æµ‹çš„å‡†ç¡®æ€§å’ŒåŠæ—¶æ€§",
            "äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ç³»ç»Ÿå¯ä»¥å¸®åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿæ£€æµ‹æ—©æœŸç™Œç—‡ç—…å˜ï¼Œæé«˜è¯Šæ–­ç²¾åº¦",
            "è¿œç¨‹åŒ»ç–—æŠ€æœ¯ç»“åˆAIå¯ä»¥ä¸ºåè¿œåœ°åŒºæä¾›æ›´å¥½çš„åŒ»ç–—æœåŠ¡ï¼Œç¼©å°åŒ»ç–—å·®è·"
        ]
        
        return [
            RerankDocument(
                text=text,
                doc_id=f"doc_{i}",
                score=np.random.uniform(0.3, 0.8),
                metadata={
                    "index": i,
                    "length": len(text),
                    "category": "tech" if i < 10 else "medical" if i < 13 else "finance"
                }
            )
            for i, text in enumerate(documents)
        ]
    
    def _embedding_based_ranking(self, 
                                query: str, 
                                documents: List[RerankDocument],
                                top_k: int = 10) -> List[Tuple[int, float]]:
        """åŸºäºåµŒå…¥çš„æ’åº"""
        # è·å–æŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_client.get_embedding(query)
        
        # è·å–æ–‡æ¡£åµŒå…¥å’Œè®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i, doc in enumerate(documents):
            doc_embedding = self.embedding_client.get_embedding(doc.text)
            similarity = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
            similarities.append((i, similarity))
        
        # æ’åºå¹¶è¿”å›å‰kä¸ª
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def _keyword_based_ranking(self, 
                             query: str, 
                             documents: List[RerankDocument],
                             top_k: int = 10) -> List[Tuple[int, float]]:
        """åŸºäºå…³é”®è¯çš„æ’åº"""
        query_words = set(query.lower().split())
        
        scores = []
        for i, doc in enumerate(documents):
            doc_words = set(doc.text.lower().split())
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            intersection = query_words.intersection(doc_words)
            score = len(intersection) / len(query_words) if query_words else 0
            
            scores.append((i, score))
        
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]
    
    def _rerank_based_ranking(self, 
                            query: str, 
                            documents: List[RerankDocument],
                            top_k: int = 10) -> List[Tuple[int, float]]:
        """åŸºäºæ–‡æœ¬æ’åºæ¨¡å‹çš„æ’åº"""
        try:
            results = self.reranker.rerank(query, documents, top_n=top_k)
            if results:
                return [(r.original_rank, r.relevance_score) for r in results]
            else:
                print("âš ï¸ æ–‡æœ¬æ’åºæ¨¡å‹æœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä½¿ç”¨åµŒå…¥æ¨¡å‹ä½œä¸ºå¤‡é€‰")
                return self._embedding_based_ranking(query, documents, top_k)
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬æ’åºæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}ï¼Œä½¿ç”¨åµŒå…¥æ¨¡å‹ä½œä¸ºå¤‡é€‰")
            return self._embedding_based_ranking(query, documents, top_k)
    
    def _calculate_metrics(self, 
                          ranked_indices: List[Tuple[int, float]],
                          relevant_indices: List[int],
                          k: int = 10) -> Dict[str, float]:
        """è®¡ç®—æ’åºæŒ‡æ ‡"""
        # è·å–å‰kä¸ªç»“æœçš„ç´¢å¼•
        top_k_indices = [idx for idx, _ in ranked_indices[:k]]
        
        # è®¡ç®—ç²¾ç¡®ç‡
        relevant_retrieved = len(set(top_k_indices).intersection(set(relevant_indices)))
        precision = relevant_retrieved / k if k > 0 else 0
        
        # è®¡ç®—å¬å›ç‡
        recall = relevant_retrieved / len(relevant_indices) if relevant_indices else 0
        
        # è®¡ç®—F1åˆ†æ•°
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        # è®¡ç®—å¹³å‡ç²¾ç¡®ç‡(AP)
        ap_sum = 0
        relevant_count = 0
        for i, idx in enumerate(top_k_indices):
            if idx in relevant_indices:
                relevant_count += 1
                ap_sum += relevant_count / (i + 1)
        
        avg_precision = ap_sum / len(relevant_indices) if relevant_indices else 0
        
        # è®¡ç®—NDCG
        dcg = 0
        idcg = 0
        for i, idx in enumerate(top_k_indices):
            relevance = 1 if idx in relevant_indices else 0
            dcg += relevance / np.log2(i + 2)
        
        # è®¡ç®—ç†æƒ³DCG
        ideal_relevances = [1] * min(len(relevant_indices), k) + [0] * max(0, k - len(relevant_indices))
        for i, rel in enumerate(ideal_relevances):
            idcg += rel / np.log2(i + 2)
        
        ndcg = dcg / idcg if idcg > 0 else 0
        
        return {
            "precision": precision,
            "recall": recall,
            "f1_score": f1_score,
            "avg_precision": avg_precision,
            "ndcg": ndcg
        }
    
    def compare_sorting_methods(self) -> List[ComparisonMetrics]:
        """å¯¹æ¯”ä¸åŒæ’åºæ–¹æ³•"""
        print("ğŸ“Š å¼€å§‹æ’åºæ–¹æ³•å¯¹æ¯”åˆ†æ...")
        
        results = []
        
        for test_case in self.evaluation_queries:
            query = test_case["query"]
            relevant_docs = test_case["relevant_docs"]
            
            print(f"\nğŸ” åˆ†ææŸ¥è¯¢: {query}")
            
            # æ–¹æ³•1: åµŒå…¥æ¨¡å‹æ’åº
            start_time = time.time()
            embedding_ranked = self._embedding_based_ranking(query, self.test_documents)
            embedding_time = time.time() - start_time
            
            embedding_metrics = self._calculate_metrics(embedding_ranked, relevant_docs)
            
            # æ–¹æ³•2: å…³é”®è¯æ’åº
            start_time = time.time()
            keyword_ranked = self._keyword_based_ranking(query, self.test_documents)
            keyword_time = time.time() - start_time
            
            keyword_metrics = self._calculate_metrics(keyword_ranked, relevant_docs)
            
            # æ–¹æ³•3: æ–‡æœ¬æ’åºæ¨¡å‹
            start_time = time.time()
            rerank_ranked = self._rerank_based_ranking(query, self.test_documents)
            rerank_time = time.time() - start_time
            
            rerank_metrics = self._calculate_metrics(rerank_ranked, relevant_docs)
            
            # æˆæœ¬ä¼°ç®—
            embedding_cost = self.reranker.estimate_cost(len(self.test_documents))["estimated_cost_cny"]
            rerank_cost = embedding_cost * 1.2  # æ–‡æœ¬æ’åºç¨è´µ
            
            # åˆ›å»ºç»“æœ
            methods = ["embedding", "keyword", "rerank"]
            times = [embedding_time, keyword_time, rerank_time]
            costs = [embedding_cost, 0, rerank_cost]  # å…³é”®è¯å‡ ä¹æ— æˆæœ¬
            metrics_list = [embedding_metrics, keyword_metrics, rerank_metrics]
            
            for method, time_cost, cost, metrics in zip(methods, times, costs, metrics_list):
                result = ComparisonMetrics(
                    method=method,
                    query=query,
                    top_k=10,
                    precision=metrics["precision"],
                    recall=metrics["recall"],
                    f1_score=metrics["f1_score"],
                    avg_precision=metrics["avg_precision"],
                    ndcg=metrics["ndcg"],
                    processing_time=time_cost,
                    cost_estimate=cost
                )
                results.append(result)
        
        return results
    
    def visualize_results(self, results: List[ComparisonMetrics]):
        """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
        print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # å‡†å¤‡æ•°æ®
        methods = ["åµŒå…¥æ¨¡å‹", "å…³é”®è¯", "æ–‡æœ¬æ’åº"]
        original_methods = ["embedding", "keyword", "rerank"]
        metrics = ["precision", "recall", "f1_score", "ndcg"]
        chinese_metrics = ["ç²¾ç¡®ç‡", "å¬å›ç‡", "F1åˆ†æ•°", "NDCG"]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle("æ’åºæ–¹æ³•æ€§èƒ½å¯¹æ¯”åˆ†æ", fontsize=16)
        
        for i, (metric, chinese_metric) in enumerate(zip(metrics, chinese_metrics)):
            row, col = i // 2, i % 2
            ax = axes[row, col]
            
            # è®¡ç®—æ¯ä¸ªæ–¹æ³•çš„å¹³å‡æŒ‡æ ‡
            method_scores = {}
            for method, original_method in zip(methods, original_methods):
                scores = [getattr(r, metric) for r in results if r.method == original_method]
                method_scores[method] = np.mean(scores)
            
            # ç»˜åˆ¶æŸ±çŠ¶å›¾
            bars = ax.bar(methods, [method_scores[m] for m in methods])
            ax.set_title(f"{chinese_metric} å¯¹æ¯”")
            ax.set_ylabel(chinese_metric)
            ax.set_ylim(0, 1)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, score in zip(bars, [method_scores[m] for m in methods]):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f"{score:.3f}", ha='center', va='bottom')
        
        plt.tight_layout()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(f"sorting_comparison_metrics_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.show()
        
        # æ€§èƒ½æ—¶é—´å¯¹æ¯”
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        method_times = {}
        for method, original_method in zip(methods, original_methods):
            times = [r.processing_time for r in results if r.method == original_method]
            method_times[method] = np.mean(times)
        
        bars = ax.bar(methods, [method_times[m] for m in methods], color=['skyblue', 'lightgreen', 'salmon'])
        ax.set_title("å¹³å‡å¤„ç†æ—¶é—´å¯¹æ¯”")
        ax.set_ylabel("æ—¶é—´ (ç§’)")
        ax.set_yscale('log')
        
        for bar, time_val in zip(bars, [method_times[m] for m in methods]):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,
                   f"{time_val:.3f}ç§’", ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(f"sorting_performance_time_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.show()
    
    def print_detailed_comparison(self, results: List[ComparisonMetrics]):
        """æ‰“å°è¯¦ç»†å¯¹æ¯”ç»“æœ"""
        print("\nğŸ“Š è¯¦ç»†å¯¹æ¯”ç»“æœ")
        print("=" * 80)
        
        # æŒ‰æŸ¥è¯¢åˆ†ç»„
        queries = list(set(r.query for r in results))
        
        for query in queries:
            print(f"\nğŸ” æŸ¥è¯¢: {query}")
            print("-" * 60)
            
            query_results = [r for r in results if r.query == query]
            
            # æ‰“å°è¡¨æ ¼
            print(f"{'æ–¹æ³•':<10} {'ç²¾ç¡®ç‡':>8} {'å¬å›ç‡':>8} {'F1åˆ†æ•°':>8} {'NDCG':>8} {'æ—¶é—´(s)':>8} {'æˆæœ¬(å…ƒ)':>10}")
            print("-" * 60)
            
            for result in query_results:
                print(f"{result.method:<10} {result.precision:>8.3f} {result.recall:>8.3f} "
                      f"{result.f1_score:>8.3f} {result.ndcg:>8.3f} "
                      f"{result.processing_time:>8.3f} {result.cost_estimate:>10.4f}")
        
        # è®¡ç®—å¹³å‡å€¼
        print(f"\nğŸ“ˆ å¹³å‡æ€§èƒ½æŒ‡æ ‡")
        print("-" * 60)
        
        methods = ["embedding", "keyword", "rerank"]
        print(f"{'æ–¹æ³•':<10} {'ç²¾ç¡®ç‡':>8} {'å¬å›ç‡':>8} {'F1åˆ†æ•°':>8} {'NDCG':>8} {'æ—¶é—´(s)':>8}")
        print("-" * 60)
        
        for method in methods:
            method_results = [r for r in results if r.method == method]
            avg_precision = np.mean([r.precision for r in method_results])
            avg_recall = np.mean([r.recall for r in method_results])
            avg_f1 = np.mean([r.f1_score for r in method_results])
            avg_ndcg = np.mean([r.ndcg for r in method_results])
            avg_time = np.mean([r.processing_time for r in method_results])
            
            print(f"{method:<10} {avg_precision:>8.3f} {avg_recall:>8.3f} "
                  f"{avg_f1:>8.3f} {avg_ndcg:>8.3f} {avg_time:>8.3f}")
    
    def performance_recommendations(self, results: List[ComparisonMetrics]) -> Dict[str, str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = {}
        
        # åˆ†æå„æ–¹æ³•è¡¨ç°
        method_stats = {}
        for method in ["embedding", "keyword", "rerank"]:
            method_results = [r for r in results if r.method == method]
            
            method_stats[method] = {
                "avg_precision": np.mean([r.precision for r in method_results]),
                "avg_recall": np.mean([r.recall for r in method_results]),
                "avg_time": np.mean([r.processing_time for r in method_results]),
                "avg_cost": np.mean([r.cost_estimate for r in method_results])
            }
        
        # ç”Ÿæˆå»ºè®®
        if method_stats["rerank"]["avg_precision"] > method_stats["embedding"]["avg_precision"]:
            recommendations["accuracy"] = "æ–‡æœ¬æ’åºæ¨¡å‹åœ¨ç²¾ç¡®ç‡æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œå»ºè®®åœ¨ç²¾åº¦è¦æ±‚é«˜çš„åœºæ™¯ä½¿ç”¨"
        else:
            recommendations["accuracy"] = "åµŒå…¥æ¨¡å‹å·²èƒ½æä¾›è‰¯å¥½çš„ç²¾ç¡®ç‡ï¼Œå¯è€ƒè™‘æˆæœ¬ä¼˜åŒ–"
        
        if method_stats["rerank"]["avg_time"] > method_stats["embedding"]["avg_time"] * 2:
            recommendations["performance"] = "æ–‡æœ¬æ’åºæ¨¡å‹è€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®å¯¹å®æ—¶æ€§è¦æ±‚ä¸é«˜çš„åœºæ™¯ä½¿ç”¨"
        else:
            recommendations["performance"] = "æ–‡æœ¬æ’åºæ¨¡å‹æ€§èƒ½å¯æ¥å—ï¼Œå¯ç”¨äºå®æ—¶åœºæ™¯"
        
        if method_stats["keyword"]["avg_precision"] < 0.5:
            recommendations["keyword"] = "å…³é”®è¯æ’åºæ•ˆæœè¾ƒå·®ï¼Œä¸å»ºè®®ä½œä¸ºä¸»è¦æ’åºæ–¹æ³•"
        else:
            recommendations["keyword"] = "å…³é”®è¯æ’åºå¯ä½œä¸ºå¿«é€Ÿç­›é€‰çš„å¤‡é€‰æ–¹æ¡ˆ"
        
        return recommendations
    
    def run_comparison_analysis(self):
        """è¿è¡Œå®Œæ•´å¯¹æ¯”åˆ†æ"""
        print("ğŸ” å¼€å§‹æ’åºæ–¹æ³•å¯¹æ¯”åˆ†æ")
        print("=" * 60)
        
        if not os.getenv("DASHSCOPE_API_KEY"):
            print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
            return
        
        try:
            # æ‰§è¡Œå¯¹æ¯”åˆ†æ
            results = self.compare_sorting_methods()
            
            # æ‰“å°è¯¦ç»†ç»“æœ
            self.print_detailed_comparison(results)
            
            # å¯è§†åŒ–ç»“æœ
            self.visualize_results(results)
            
            # ç”Ÿæˆå»ºè®®
            recommendations = self.performance_recommendations(results)
            
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for key, value in recommendations.items():
                print(f"   {key}: {value}")
            
            print(f"\nğŸ‰ å¯¹æ¯”åˆ†æå®Œæˆï¼")
            
            # ä¿å­˜ç»“æœ
            self._save_results(results)
            
        except Exception as e:
            print(f"âŒ å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
    
    def _save_results(self, results: List[ComparisonMetrics]):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"sorting_comparison_results_{timestamp}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        data = [
            {
                "method": r.method,
                "query": r.query,
                "top_k": r.top_k,
                "precision": r.precision,
                "recall": r.recall,
                "f1_score": r.f1_score,
                "avg_precision": r.avg_precision,
                "ndcg": r.ndcg,
                "processing_time": r.processing_time,
                "cost_estimate": r.cost_estimate
            }
            for r in results
        ]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = SortingComparison()
    analyzer.run_comparison_analysis()

if __name__ == "__main__":
    main()