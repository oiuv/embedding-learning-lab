#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ•™ç¨‹ç¬¬6è¯¾ï¼šæ–‡æœ¬æ’åºæ¨¡å‹ç³»ç»Ÿé›†æˆ
================================

æœ¬è¯¾ç¨‹å°†æ•™ä½ å¦‚ä½•å°†æ–‡æœ¬æ’åºæ¨¡å‹é›†æˆåˆ°ç°æœ‰çš„è¯­ä¹‰æœç´¢å’Œé—®ç­”ç³»ç»Ÿä¸­ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. è®¾è®¡æ’åºæ¨¡å‹é›†æˆæ¶æ„
2. å®ç°æ··åˆæ’åºç­–ç•¥
3. ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½
4. å¤„ç†å¤§è§„æ¨¡æ•°æ®æ’åº
5. æ·»åŠ æ’åºç»“æœç¼“å­˜æœºåˆ¶

"""

import os
import sys
import json
import pickle
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import hashlib
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
from utils.embedding_client import EmbeddingClient

# åˆå§‹åŒ–DashScope
import dashscope
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

@dataclass
class SearchDocument:
    """æœç´¢æ–‡æ¡£"""
    doc_id: str
    title: str
    content: str
    score: float
    metadata: Dict
    embedding: List[float] = None

@dataclass
class RankingResult:
    """æ’åºç»“æœ"""
    document: SearchDocument
    original_score: float
    rerank_score: float
    final_score: float
    rank_change: int

class TextRerankIntegrator:
    """æ–‡æœ¬æ’åºé›†æˆå™¨"""
    
    def __init__(self, cache_enabled: bool = True, cache_ttl: int = 3600):
        """åˆå§‹åŒ–é›†æˆå™¨"""
        self.client = EmbeddingClient()
        self.cache_enabled = cache_enabled
        self.cache_ttl = cache_ttl
        self.cache = {}
        self.cache_stats = {"hits": 0, "misses": 0}
        
        # é…ç½®å‚æ•°
        self.config = {
            "max_candidates": 100,      # æœ€å¤§å€™é€‰æ–‡æ¡£æ•°
            "min_score_threshold": 0.1,  # æœ€å°åˆ†æ•°é˜ˆå€¼
            "rerank_weight": 0.7,       # æ’åºæƒé‡
            "original_weight": 0.3,     # åŸå§‹åˆ†æ•°æƒé‡
            "batch_size": 25            # æ‰¹é‡å¤„ç†å¤§å°
        }
        
        print("âœ… æ–‡æœ¬æ’åºé›†æˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _generate_cache_key(self, query: str, documents: List[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{query}_{''.join(sorted(documents))}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_from_cache(self, key: str) -> Optional[List[RankingResult]]:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if not self.cache_enabled:
            return None
            
        if key in self.cache:
            cached_data = self.cache[key]
            if datetime.now() < cached_data["expires"]:
                self.cache_stats["hits"] += 1
                return cached_data["results"]
            else:
                del self.cache[key]
        
        self.cache_stats["misses"] += 1
        return None
    
    def _save_to_cache(self, key: str, results: List[RankingResult]):
        """ä¿å­˜ç»“æœåˆ°ç¼“å­˜"""
        if not self.cache_enabled:
            return
            
        self.cache[key] = {
            "results": results,
            "expires": datetime.now() + timedelta(seconds=self.cache_ttl)
        }
    
    def rerank_documents(self, 
                        query: str, 
                        documents: List[SearchDocument],
                        strategy: str = "hybrid") -> List[RankingResult]:
        """
        é‡æ–°æ’åºæ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            strategy: æ’åºç­–ç•¥ ('embedding', 'rerank', 'hybrid')
        
        Returns:
            æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        if not documents:
            return []
        
        # é™åˆ¶å€™é€‰æ–‡æ¡£æ•°é‡
        documents = documents[:self.config["max_candidates"]]
        
        # æ£€æŸ¥ç¼“å­˜
        doc_texts = [f"{doc.title} {doc.content}" for doc in documents]
        cache_key = self._generate_cache_key(query, doc_texts)
        cached_results = self._get_from_cache(cache_key)
        
        if cached_results:
            return cached_results
        
        # æ ¹æ®ç­–ç•¥é€‰æ‹©æ’åºæ–¹æ³•
        if strategy == "embedding":
            results = self._rank_by_embedding(query, documents)
        elif strategy == "rerank":
            results = self._rank_by_rerank(query, documents)
        elif strategy == "hybrid":
            results = self._rank_by_hybrid(query, documents)
        else:
            raise ValueError(f"Unknown strategy: {strategy}")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_to_cache(cache_key, results)
        
        return results
    
    def _rank_by_embedding(self, query: str, documents: List[SearchDocument]) -> List[RankingResult]:
        """ä½¿ç”¨åµŒå…¥æ¨¡å‹æ’åº"""
        query_embedding = self.client.get_embedding(query)
        
        results = []
        for doc in documents:
            if doc.embedding is None:
                doc.embedding = self.client.get_embedding(f"{doc.title} {doc.content}")
            
            similarity = np.dot(query_embedding, doc.embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc.embedding)
            )
            
            results.append(RankingResult(
                document=doc,
                original_score=doc.score,
                rerank_score=similarity,
                final_score=similarity,
                rank_change=0
            ))
        
        return sorted(results, key=lambda x: x.final_score, reverse=True)
    
    def _rank_by_rerank(self, query: str, documents: List[SearchDocument]) -> List[RankingResult]:
        """ä½¿ç”¨æ–‡æœ¬æ’åºæ¨¡å‹æ’åº"""
        doc_texts = [f"{doc.title} {doc.content}" for doc in documents]
        
        try:
            response = dashscope.TextReRank.call(
                model="gte-rerank-v2",
                query=query,
                documents=doc_texts,
                top_n=len(documents)
            )
            
            if response.status_code == 200:
                results = []
                rerank_results = response.output.results
                
                # åˆ›å»ºåŸå§‹æ’åæ˜ å°„
                original_ranks = {doc.doc_id: i for i, doc in enumerate(documents)}
                
                for rank, result in enumerate(rerank_results):
                    doc_idx = result.index
                    doc = documents[doc_idx]
                    
                    results.append(RankingResult(
                        document=doc,
                        original_score=doc.score,
                        rerank_score=result.relevance_score,
                        final_score=result.relevance_score,
                        rank_change=original_ranks[doc.doc_id] - rank
                    ))
                
                return results
            else:
                print(f"âŒ æ’åºå¤±è´¥: {response}")
                return []
                
        except Exception as e:
            print(f"âŒ è°ƒç”¨æ’åºæ¨¡å‹å¤±è´¥: {e}")
            return []
    
    def _rank_by_hybrid(self, query: str, documents: List[SearchDocument]) -> List[RankingResult]:
        """æ··åˆæ’åºç­–ç•¥"""
        # è·å–åµŒå…¥æ¨¡å‹åˆ†æ•°
        embedding_results = self._rank_by_embedding(query, documents)
        
        # è·å–æ’åºæ¨¡å‹åˆ†æ•°
        rerank_results = self._rank_by_rerank(query, documents)
        
        if not rerank_results:
            return embedding_results
        
        # åˆ›å»ºæ˜ å°„
        embedding_scores = {r.document.doc_id: r.rerank_score for r in embedding_results}
        rerank_scores = {r.document.doc_id: r.rerank_score for r in rerank_results}
        
        # è®¡ç®—æ··åˆåˆ†æ•°
        results = []
        for doc in documents:
            doc_id = doc.doc_id
            
            # æ ‡å‡†åŒ–åˆ†æ•°
            embedding_norm = (embedding_scores[doc_id] + 1) / 2  # ä½™å¼¦ç›¸ä¼¼åº¦æ ‡å‡†åŒ–
            rerank_norm = rerank_scores.get(doc_id, 0)
            
            # æ··åˆåˆ†æ•°
            final_score = (
                self.config["original_weight"] * embedding_norm +
                self.config["rerank_weight"] * rerank_norm
            )
            
            # è®¡ç®—æ’åå˜åŒ–
            original_rank = next(i for i, r in enumerate(embedding_results) 
                             if r.document.doc_id == doc_id)
            rerank_rank = next(i for i, r in enumerate(rerank_results) 
                           if r.document.doc_id == doc_id)
            
            results.append(RankingResult(
                document=doc,
                original_score=embedding_scores[doc_id],
                rerank_score=rerank_norm,
                final_score=final_score,
                rank_change=original_rank - rerank_rank
            ))
        
        return sorted(results, key=lambda x: x.final_score, reverse=True)
    
    def batch_rerank(self, 
                    queries: List[str], 
                    documents_list: List[List[SearchDocument]]) -> List[List[RankingResult]]:
        """æ‰¹é‡é‡æ–°æ’åº"""
        results = []
        
        for query, documents in zip(queries, documents_list):
            reranked = self.rerank_documents(query, documents)
            results.append(reranked)
        
        return results
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = self.cache_stats["hits"] / total if total > 0 else 0
        
        return {
            "hits": self.cache_stats["hits"],
            "misses": self.cache_stats["misses"],
            "hit_rate": hit_rate,
            "cache_size": len(self.cache)
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.cache_stats = {"hits": 0, "misses": 0}
        print("âœ… ç¼“å­˜å·²æ¸…ç©º")

class ComplexDataGenerator:
    """å¤æ‚æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨"""
        self.seed_data = {
            "tech": {
                "titles": [
                    "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨",
                    "æ·±åº¦å­¦ä¹ ç®—æ³•çš„æœ€æ–°è¿›å±•",
                    "åŒºå—é“¾æŠ€æœ¯åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨",
                    "é‡å­è®¡ç®—çš„å‘å±•å‰æ™¯",
                    "ç‰©è”ç½‘æŠ€æœ¯çš„æ ‡å‡†åŒ–è¿›ç¨‹"
                ],
                "content_templates": [
                    "{topic}æŠ€æœ¯è¿‘å¹´æ¥å‘å±•è¿…é€Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨{application}é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚",
                    "ç ”ç©¶è¡¨æ˜ï¼Œ{topic}æŠ€æœ¯åœ¨è§£å†³{problem}é—®é¢˜æ—¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
                    "{topic}æŠ€æœ¯çš„å•†ä¸šåŒ–åº”ç”¨æ­£åœ¨åŠ é€Ÿï¼Œé¢„è®¡å°†åœ¨{timeframe}å†…å®ç°é‡å¤§çªç ´ã€‚"
                ]
            },
            "medical": {
                "titles": [
                    "è‚ºç™Œæ—©æœŸç­›æŸ¥çš„æ–°æ–¹æ³•",
                    "ç³–å°¿ç—…ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆ",
                    "å¿ƒè¡€ç®¡ç–¾ç—…é¢„é˜²ç­–ç•¥",
                    "ç¥ç»ç³»ç»Ÿç–¾ç—…çš„è¯Šæ–­æŠ€æœ¯",
                    "ç™Œç—‡å…ç–«æ²»ç–—çš„æœ€æ–°è¿›å±•"
                ],
                "content_templates": [
                    "{disease}çš„æ—©æœŸè¯Šæ–­å¯¹äºæé«˜æ²»æ„ˆç‡è‡³å…³é‡è¦ï¼Œ{method}æŠ€æœ¯æ˜¾ç¤ºå‡ºè‰¯å¥½å‰æ™¯ã€‚",
                    "{treatment}æ–¹æ³•åœ¨{condition}æ²»ç–—ä¸­æ•ˆæœæ˜¾è‘—ï¼Œæ‚£è€…ç”Ÿå­˜ç‡æé«˜äº†{percentage}%ã€‚",
                    "{technology}æŠ€æœ¯çš„å¼•å…¥ä½¿å¾—{disease}çš„è¯Šæ–­å‡†ç¡®ç‡æå‡äº†{improvement}ã€‚"
                ]
            },
            "finance": {
                "titles": [
                    "æ•°å­—è´§å¸çš„ç›‘ç®¡æ”¿ç­–",
                    "é‡‘èç§‘æŠ€å¯¹ä¼ ç»Ÿé“¶è¡Œä¸šçš„å½±å“",
                    "é‡åŒ–æŠ•èµ„ç­–ç•¥çš„ä¼˜åŒ–",
                    "åŒºå—é“¾æŠ€æœ¯åœ¨æ”¯ä»˜ç³»ç»Ÿä¸­çš„åº”ç”¨",
                    "é‡‘èé£é™©ç®¡ç†çš„åˆ›æ–°æ–¹æ³•"
                ],
                "content_templates": [
                    "{topic}åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨æ­£åœ¨æ”¹å˜ä¼ ç»Ÿçš„{business}æ¨¡å¼ã€‚",
                    "{technology}æŠ€æœ¯çš„é‡‡ç”¨ä½¿å¾—{process}æ•ˆç‡æé«˜äº†{percentage}%ã€‚",
                    "{innovation}ä¸º{sector}è¡Œä¸šå¸¦æ¥äº†æ–°çš„å‘å±•æœºé‡å’ŒæŒ‘æˆ˜ã€‚"
                ]
            }
        }
    
    def generate_document_corpus(self, 
                                category: str, 
                                count: int,
                                include_noise: bool = True) -> List[SearchDocument]:
        """ç”Ÿæˆæ–‡æ¡£è¯­æ–™"""
        documents = []
        
        if category not in self.seed_data:
            raise ValueError(f"Unknown category: {category}")
        
        data = self.seed_data[category]
        
        for i in range(count):
            # ç”Ÿæˆç›¸å…³æ–‡æ¡£
            title = np.random.choice(data["titles"])
            template = np.random.choice(data["content_templates"])
            
            # å¡«å……æ¨¡æ¿
            content = template.format(
                topic=title.split("åœ¨")[0] if "åœ¨" in title else title,
                application=f"{category}é¢†åŸŸ",
                problem="å¤æ‚é—®é¢˜",
                timeframe="æœªæ¥5å¹´",
                disease="ç–¾ç—…",
                method="æ–°æŠ€æœ¯",
                treatment="æ–°ç–—æ³•",
                percentage="20-30%",
                technology="å…ˆè¿›",
                improvement="æ˜¾è‘—",
                business="ä¸šåŠ¡",
                process="æµç¨‹",
                innovation="åˆ›æ–°",
                sector=category
            )
            
            # æ·»åŠ æ›´å¤šå†…å®¹
            content += " " + self._generate_additional_content(category)
            
            score = np.random.uniform(0.3, 0.9)
            
            doc = SearchDocument(
                doc_id=f"{category}_{i:04d}",
                title=title,
                content=content,
                score=score,
                metadata={
                    "category": category,
                    "length": len(content),
                    "created": datetime.now().isoformat(),
                    "relevance": np.random.choice(["high", "medium", "low"])
                }
            )
            
            documents.append(doc)
        
        # æ·»åŠ å™ªå£°æ–‡æ¡£ï¼ˆä¸ç›¸å…³æ–‡æ¡£ï¼‰
        if include_noise:
            noise_count = max(1, count // 5)
            noise_docs = self._generate_noise_documents(noise_count)
            documents.extend(noise_docs)
        
        return documents
    
    def _generate_additional_content(self, category: str) -> str:
        """ç”Ÿæˆé™„åŠ å†…å®¹"""
        additional_sentences = {
            "tech": [
                "è¯¥æŠ€æœ¯çš„å…³é”®çªç ´åœ¨äºç®—æ³•çš„ä¼˜åŒ–å’Œè®¡ç®—æ•ˆç‡çš„æå‡ã€‚",
                "å®é™…åº”ç”¨ä¸­éœ€è¦è€ƒè™‘æ•°æ®éšç§å’Œå®‰å…¨é—®é¢˜ã€‚",
                "æœªæ¥çš„å‘å±•æ–¹å‘åŒ…æ‹¬è·¨é¢†åŸŸèåˆå’Œæ ‡å‡†åŒ–å»ºè®¾ã€‚"
            ],
            "medical": [
                "ä¸´åºŠè¯•éªŒæ•°æ®æ˜¾ç¤ºè¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚",
                "æ‚£è€…ä¾ä»æ€§å’Œé•¿æœŸéšè®¿æ˜¯ç ”ç©¶æˆåŠŸçš„å…³é”®å› ç´ ã€‚",
                "ä¸ªæ€§åŒ–åŒ»ç–—æ–¹æ¡ˆéœ€è¦æ ¹æ®æ‚£è€…å…·ä½“æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚"
            ],
            "finance": [
                "ç›‘ç®¡ç¯å¢ƒçš„å®Œå–„å¯¹äºè¡Œä¸šçš„å¥åº·å‘å±•è‡³å…³é‡è¦ã€‚",
                "é£é™©æ§åˆ¶æœºåˆ¶æ˜¯ç³»ç»Ÿç¨³å®šè¿è¡Œçš„é‡è¦ä¿éšœã€‚",
                "å¸‚åœºæ¥å—åº¦å’Œç”¨æˆ·æ•™è‚²éœ€è¦æŒç»­æ¨è¿›ã€‚"
            ]
        }
        
        return " ".join(np.random.choice(additional_sentences[category], 2))
    
    def _generate_noise_documents(self, count: int) -> List[SearchDocument]:
        """ç”Ÿæˆå™ªå£°æ–‡æ¡£"""
        noise_topics = [
            "å® ç‰©é¥²å…»æŠ€å·§",
            "æ—…è¡Œæ”»ç•¥åˆ†äº«",
            "ç¾é£Ÿåˆ¶ä½œæ•™ç¨‹",
            "è¿åŠ¨å¥èº«æŒ‡å—",
            "å›­è‰ºç§æ¤çŸ¥è¯†"
        ]
        
        documents = []
        for i in range(count):
            topic = np.random.choice(noise_topics)
            content = f"è¿™æ˜¯ä¸€ä¸ªå…³äº{topic}çš„æ–‡æ¡£ï¼Œå†…å®¹å®Œå…¨ä¸ç›¸å…³ã€‚"
            
            doc = SearchDocument(
                doc_id=f"noise_{i:04d}",
                title=f"å™ªå£°æ–‡æ¡£ï¼š{topic}",
                content=content,
                score=np.random.uniform(0.1, 0.3),
                metadata={
                    "category": "noise",
                    "length": len(content),
                    "created": datetime.now().isoformat(),
                    "relevance": "none"
                }
            )
            
            documents.append(doc)
        
        return documents

class IntegrationDemo:
    """é›†æˆæ¼”ç¤ºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤º"""
        self.reranker = TextRerankIntegrator()
        self.data_generator = ComplexDataGenerator()
        
        print("ğŸš€ æ–‡æœ¬æ’åºé›†æˆæ¼”ç¤ºå¯åŠ¨")
        print("=" * 50)
    
    def demo_search_integration(self):
        """æœç´¢ç³»ç»Ÿé›†æˆæ¼”ç¤º"""
        print("ğŸ” æœç´¢ç³»ç»Ÿé›†æˆæ¼”ç¤º")
        print("=" * 40)
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        query = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„æœ€æ–°åº”ç”¨"
        documents = self.data_generator.generate_document_corpus("medical", 15)
        
        print(f"ğŸ“Š ç”Ÿæˆ {len(documents)} ç¯‡åŒ»ç–—ç›¸å…³æ–‡æ¡£")
        
        # ä½¿ç”¨ä¸åŒç­–ç•¥æ’åº
        strategies = ["embedding", "rerank", "hybrid"]
        
        for strategy in strategies:
            print(f"\nğŸ¯ ä½¿ç”¨{strategy}ç­–ç•¥æ’åº:")
            results = self.reranker.rerank_documents(query, documents, strategy=strategy)
            
            for i, result in enumerate(results[:5]):
                print(f"   {i+1}. {result.document.title[:50]}...")
                print(f"       åŸå§‹åˆ†æ•°: {result.original_score:.3f}")
                print(f"       é‡æ’åˆ†æ•°: {result.rerank_score:.3f}")
                print(f"       æœ€ç»ˆåˆ†æ•°: {result.final_score:.3f}")
                print(f"       æ’åå˜åŒ–: {result.rank_change:+.0f}")
    
    def demo_batch_processing(self):
        """æ‰¹é‡å¤„ç†æ¼”ç¤º"""
        print("\nâš¡ æ‰¹é‡å¤„ç†æ¼”ç¤º")
        print("=" * 40)
        
        # ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢çš„æµ‹è¯•æ•°æ®
        queries = [
            "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨",
            "åŒºå—é“¾æŠ€æœ¯åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨",
            "æ·±åº¦å­¦ä¹ ç®—æ³•çš„æœ€æ–°è¿›å±•"
        ]
        
        documents_list = [
            self.data_generator.generate_document_corpus("medical", 10),
            self.data_generator.generate_document_corpus("finance", 10),
            self.data_generator.generate_document_corpus("tech", 10)
        ]
        
        start_time = time.time()
        results = self.reranker.batch_rerank(queries, documents_list)
        total_time = time.time() - start_time
        
        print(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"   æŸ¥è¯¢æ•°é‡: {len(queries)}")
        print(f"   æ€»æ–‡æ¡£æ•°: {sum(len(docs) for docs in documents_list)}")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"   å¹³å‡æ¯ä¸ªæŸ¥è¯¢: {total_time/len(queries):.2f}s")
        
        for i, (query, docs) in enumerate(zip(queries, results)):
            print(f"\n   æŸ¥è¯¢{i+1}: {query}")
            print(f"   å‰3ä¸ªç»“æœ:")
            for j, result in enumerate(docs[:3]):
                print(f"      {j+1}. {result.document.title[:40]}... ({result.final_score:.3f})")
    
    def demo_cache_performance(self):
        """ç¼“å­˜æ€§èƒ½æ¼”ç¤º"""
        print("\nğŸ’¾ ç¼“å­˜æ€§èƒ½æ¼”ç¤º")
        print("=" * 40)
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        query = "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¶‹åŠ¿"
        documents = self.data_generator.generate_document_corpus("tech", 20)
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        start_time = time.time()
        results1 = self.reranker.rerank_documents(query, documents)
        first_time = time.time() - start_time
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
        start_time = time.time()
        results2 = self.reranker.rerank_documents(query, documents)
        second_time = time.time() - start_time
        
        # æ£€æŸ¥ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.reranker.get_cache_stats()
        
        print(f"ğŸ“Š ç¼“å­˜æ€§èƒ½åˆ†æ:")
        print(f"   é¦–æ¬¡è°ƒç”¨æ—¶é—´: {first_time:.3f}s")
        print(f"   ç¼“å­˜è°ƒç”¨æ—¶é—´: {second_time:.3f}s")
        print(f"   æ€§èƒ½æå‡: {((first_time-second_time)/first_time*100):.1f}%")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}")
        print(f"   ç¼“å­˜å¤§å°: {cache_stats['cache_size']}")
    
    def run_integration_demo(self):
        """è¿è¡Œé›†æˆæ¼”ç¤º"""
        print("ğŸ“ æ–‡æœ¬æ’åºç³»ç»Ÿé›†æˆæ¼”ç¤º")
        print("=" * 60)
        
        if not os.getenv("DASHSCOPE_API_KEY"):
            print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
            return
        
        try:
            self.demo_search_integration()
            self.demo_batch_processing()
            self.demo_cache_performance()
            
            print("\nğŸ‰ é›†æˆæ¼”ç¤ºå®Œæˆï¼")
            print("\nğŸ“š ä½ å­¦ä¼šäº†ï¼š")
            print("âœ… æ–‡æœ¬æ’åºæ¨¡å‹çš„ç³»ç»Ÿé›†æˆ")
            print("âœ… æ··åˆæ’åºç­–ç•¥çš„å®ç°")
            print("âœ… ç¼“å­˜æœºåˆ¶çš„ä¼˜åŒ–")
            print("âœ… æ‰¹é‡å¤„ç†çš„æ•ˆç‡æå‡")
            
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    demo = IntegrationDemo()
    demo.run_integration_demo()

if __name__ == "__main__":
    main()