#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬1è¯¾ï¼šä»€ä¹ˆæ˜¯æ–‡æœ¬åµŒå…¥ (Text Embedding)
========================================

æœ¬è¯¾ç¨‹å°†å¸®åŠ©ä½ ç†è§£æ–‡æœ¬åµŒå…¥çš„åŸºæœ¬æ¦‚å¿µå’Œå·¥ä½œåŸç†ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£ä»€ä¹ˆæ˜¯æ–‡æœ¬åµŒå…¥
2. äº†è§£æ–‡æœ¬åµŒå…¥çš„ç”¨é€”
3. è®¤è¯†å‘é‡ç©ºé—´æ¨¡å‹
4. ç†è§£è¯­ä¹‰ç›¸ä¼¼æ€§

"""

import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict
import json

class TextEmbeddingConcept:
    """æ–‡æœ¬åµŒå…¥æ¦‚å¿µæ¼”ç¤ºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¦‚å¿µæ¼”ç¤º"""
        self.concepts = {
            "word_to_vector": "å°†å•è¯æ˜ å°„åˆ°æ•°å€¼å‘é‡çš„è¿‡ç¨‹",
            "semantic_space": "é«˜ç»´ç©ºé—´ä¸­çš„è¯­ä¹‰è¡¨ç¤º",
            "similarity": "é€šè¿‡å‘é‡è·ç¦»è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§",
            "dimension": "å‘é‡çš„é•¿åº¦ï¼Œé€šå¸¸128-1024ç»´"
        }
    
    def demonstrate_word_mapping(self):
        """æ¼”ç¤ºå•è¯åˆ°å‘é‡çš„æ˜ å°„"""
        print("ğŸ¯ ç¬¬1éƒ¨åˆ†ï¼šå•è¯åˆ°å‘é‡çš„æ˜ å°„")
        print("=" * 50)
        
        # ç®€å•çš„å•è¯åˆ°å‘é‡æ˜ å°„ç¤ºä¾‹ï¼ˆæ¨¡æ‹Ÿï¼‰
        word_vectors = {
            "çŒ«": [0.1, 0.3, 0.8, 0.2],
            "ç‹—": [0.2, 0.4, 0.7, 0.3],
            "æ±½è½¦": [0.8, 0.1, 0.0, 0.9],
            "é£æœº": [0.9, 0.2, 0.1, 0.8]
        }
        
        print("ğŸ“Š å•è¯ -> å‘é‡æ˜ å°„ç¤ºä¾‹:")
        for word, vector in word_vectors.items():
            print(f"'{word}' -> {vector}")
        
        return word_vectors
    
    def demonstrate_similarity_calculation(self, word_vectors: Dict[str, List[float]]):
        """æ¼”ç¤ºç›¸ä¼¼åº¦è®¡ç®—"""
        print("\nğŸ¯ ç¬¬2éƒ¨åˆ†ï¼šè®¡ç®—å•è¯ç›¸ä¼¼åº¦")
        print("=" * 50)
        
        def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
            """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            return dot_product / (norm1 * norm2)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        words = list(word_vectors.keys())
        for i, word1 in enumerate(words):
            for j, word2 in enumerate(words[i+1:], i+1):
                sim = cosine_similarity(word_vectors[word1], word_vectors[word2])
                print(f"'{word1}' å’Œ '{word2}' çš„ç›¸ä¼¼åº¦: {sim:.3f}")
    
    def visualize_2d_projection(self):
        """å¯è§†åŒ–2DæŠ•å½±"""
        print("\nğŸ¯ ç¬¬3éƒ¨åˆ†ï¼š2Då¯è§†åŒ–")
        print("=" * 50)
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        words = ["é«˜å…´", "æ‚²ä¼¤", "æ„¤æ€’", "æƒŠè®¶", "ææƒ§", "åŒæ¶"]
        # æ¨¡æ‹Ÿæƒ…æ„Ÿå‘é‡çš„2DæŠ•å½±
        coordinates = {
            "é«˜å…´": [0.8, 0.7],
            "æ‚²ä¼¤": [-0.7, -0.5],
            "æ„¤æ€’": [-0.5, 0.8],
            "æƒŠè®¶": [0.6, -0.3],
            "ææƒ§": [-0.8, -0.2],
            "åŒæ¶": [-0.4, -0.8]
        }
        
        # ç»˜åˆ¶2Då›¾
        # è®¾ç½®matplotlibä»¥æ”¯æŒä¸­æ–‡æ˜¾ç¤º
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        plt.figure(figsize=(10, 8))
        
        for word, (x, y) in coordinates.items():
            plt.scatter(x, y, s=100)
            plt.annotate(word, (x, y), fontsize=12, ha='center', va='bottom')
        
        plt.title("æƒ…æ„Ÿè¯æ±‡çš„2Då‘é‡ç©ºé—´è¡¨ç¤º", fontsize=14)
        plt.xlabel("ç»´åº¦1", fontsize=12)
        plt.ylabel("ç»´åº¦2", fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
        
        # ä¿å­˜å›¾ç‰‡
        plt.savefig('01-basics/embedding_concept_visualization.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… å¯è§†åŒ–å·²ä¿å­˜ä¸º 'embedding_concept_visualization.png'")
    
    def explain_key_concepts(self):
        """è§£é‡Šå…³é”®æ¦‚å¿µ"""
        print("\nğŸ¯ ç¬¬4éƒ¨åˆ†ï¼šå…³é”®æ¦‚å¿µè§£é‡Š")
        print("=" * 50)
        
        concepts = {
            "ç»´åº¦ (Dimension)": 
                "æ–‡æœ¬å‘é‡çš„é•¿åº¦ï¼Œ128ç»´è¡¨ç¤ºæ¯ä¸ªæ–‡æœ¬ç”¨128ä¸ªæ•°å­—è¡¨ç¤º",
            "è¯­ä¹‰ç©ºé—´ (Semantic Space)": 
                "é«˜ç»´ç©ºé—´ï¼Œè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘",
            "ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)": 
                "é€šè¿‡å‘é‡å¤¹è§’è®¡ç®—ç›¸ä¼¼åº¦ï¼ŒèŒƒå›´-1åˆ°1ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒ",
            "åµŒå…¥æ¨¡å‹ (Embedding Model)": 
                "å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡çš„AIæ¨¡å‹ï¼Œå¦‚Word2Vecã€BERTç­‰",
            "ä¸Šä¸‹æ–‡ (Context)": 
                "æ–‡æœ¬çš„ç¯å¢ƒä¿¡æ¯ï¼Œå½±å“è¯çš„å«ä¹‰"
        }
        
        for concept, explanation in concepts.items():
            print(f"ğŸ“– {concept}:")
            print(f"   {explanation}\n")
    
    def demonstrate_real_world_examples(self):
        """æ¼”ç¤ºå®é™…åº”ç”¨åœºæ™¯"""
        print("ğŸ¯ ç¬¬5éƒ¨åˆ†ï¼šå®é™…åº”ç”¨åœºæ™¯")
        print("=" * 50)
        
        scenarios = [
            {
                "åœºæ™¯": "æœç´¢å¼•æ“",
                "æè¿°": "ç†è§£ç”¨æˆ·æœç´¢æ„å›¾ï¼Œè¿”å›ç›¸å…³ç»“æœ",
                "ç¤ºä¾‹": "æœç´¢'è‹¹æœ'å¯åŒºåˆ†æ°´æœå’Œå…¬å¸"
            },
            {
                "åœºæ™¯": "æ¨èç³»ç»Ÿ",
                "æè¿°": "åŸºäºå†…å®¹ç›¸ä¼¼æ€§æ¨èå•†å“æˆ–æ–‡ç« ",
                "ç¤ºä¾‹": "é˜…è¯»æœºå™¨å­¦ä¹ æ–‡ç« åæ¨èAIç›¸å…³å†…å®¹"
            },
            {
                "åœºæ™¯": "æƒ…æ„Ÿåˆ†æ",
                "æè¿°": "åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘",
                "ç¤ºä¾‹": "åˆ¤æ–­ç”¨æˆ·è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢"
            },
            {
                "åœºæ™¯": "æ™ºèƒ½å®¢æœ",
                "æè¿°": "ç†è§£ç”¨æˆ·é—®é¢˜ï¼ŒåŒ¹é…æœ€ä½³ç­”æ¡ˆ",
                "ç¤ºä¾‹": "'å¦‚ä½•é€€è´§'åŒ¹é…åˆ°é€€è´§æ”¿ç­–"
            },
            {
                "åœºæ™¯": "å†…å®¹èšç±»",
                "æè¿°": "å°†ç›¸ä¼¼å†…å®¹è‡ªåŠ¨åˆ†ç»„",
                "ç¤ºä¾‹": "å°†æ–°é—»æ–‡ç« æŒ‰ä¸»é¢˜åˆ†ç±»"
            }
        ]
        
        for scenario in scenarios:
            print(f"ğŸ¯ {scenario['åœºæ™¯']}:")
            print(f"   æè¿°: {scenario['æè¿°']}")
            print(f"   ç¤ºä¾‹: {scenario['ç¤ºä¾‹']}\n")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç¬¬1è¯¾ï¼šä»€ä¹ˆæ˜¯æ–‡æœ¬åµŒå…¥")
    print("=" * 60)
    print("æ¬¢è¿æ¥åˆ°æ–‡æœ¬åµŒå…¥å­¦ä¹ ä¹‹æ—…ï¼")
    print("æœ¬è¯¾ç¨‹å°†å¸®åŠ©ä½ ç†è§£æ–‡æœ¬åµŒå…¥çš„æ ¸å¿ƒæ¦‚å¿µã€‚\n")
    
    try:
        input("ğŸ“š æŒ‰å›è½¦é”®å¼€å§‹å­¦ä¹ ...")
        
        # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
        demo = TextEmbeddingConcept()
        
        # è¿è¡Œæ¼”ç¤º
        print("\n" + "="*60)
        word_vectors = demo.demonstrate_word_mapping()
        input("\nğŸ“š æŒ‰å›è½¦é”®ç»§ç»­åˆ°ç›¸ä¼¼åº¦è®¡ç®—...")
        
        print("\n" + "="*60)
        demo.demonstrate_similarity_calculation(word_vectors)
        input("\nğŸ“Š æŒ‰å›è½¦é”®ç»§ç»­åˆ°å¯è§†åŒ–éƒ¨åˆ†...")
        
        print("\n" + "="*60)
        demo.visualize_2d_projection()
        input("\nğŸ“– æŒ‰å›è½¦é”®ç»§ç»­åˆ°å…³é”®æ¦‚å¿µè§£é‡Š...")
        
        print("\n" + "="*60)
        demo.explain_key_concepts()
        input("\nğŸ¯ æŒ‰å›è½¦é”®æŸ¥çœ‹å®é™…åº”ç”¨åœºæ™¯...")
        
        print("\n" + "="*60)
        demo.demonstrate_real_world_examples()
        
        print("\n" + "="*60)
        print("ğŸ‰ ç¬¬1è¯¾å®Œæˆï¼")
        print("ä½ å·²ç»äº†è§£äº†ï¼š")
        print("âœ… ä»€ä¹ˆæ˜¯æ–‡æœ¬åµŒå…¥")
        print("âœ… å¦‚ä½•è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦")
        print("âœ… æ–‡æœ¬åµŒå…¥çš„å®é™…åº”ç”¨")
        print("\nğŸ“‚ å¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜ä¸º 'embedding_concept_visualization.png'")
        print("\nğŸ¯ ä¸‹ä¸€è¯¾ï¼š02-first-embedding.py - è·å–ç¬¬ä¸€ä¸ªæ–‡æœ¬å‘é‡")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è¯¾ç¨‹å·²ä¸­æ–­ï¼Œæ¬¢è¿ä¸‹æ¬¡ç»§ç»­å­¦ä¹ ï¼")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print("ğŸ”„ è¯·æ£€æŸ¥ç¯å¢ƒé…ç½®åé‡è¯•")
    finally:
        input("\nğŸ“š æŒ‰å›è½¦é”®é€€å‡ºè¯¾ç¨‹...")

if __name__ == "__main__":
    main()