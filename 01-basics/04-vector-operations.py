#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬4è¯¾ï¼šå‘é‡æ“ä½œåŸºç¡€
============================

æœ¬è¯¾ç¨‹å°†æ•™ä½ å¦‚ä½•å¤„ç†å’Œæ“ä½œæ–‡æœ¬åµŒå…¥å‘é‡ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£å‘é‡çš„åŸºæœ¬æ“ä½œ
2. æŒæ¡å‘é‡åŠ å‡ä¹˜é™¤
3. å­¦ä¹ å‘é‡å½’ä¸€åŒ–
4. å®ç°å‘é‡é™ç»´
5. ç†è§£å‘é‡ç©ºé—´ä¸­çš„è¯­ä¹‰è¿ç®—

"""

import os
import sys
import numpy as np
from typing import List, Tuple, Dict
import matplotlib.pyplot as plt
from openai import OpenAI
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

class VectorOperations:
    """å‘é‡æ“ä½œç±»"""
    
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–"""
        try:
            self.client = OpenAI(
                api_key=api_key or os.getenv("DASHSCOPE_API_KEY"),
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            self.model = "text-embedding-v4"
            self.dimensions = 1024
            print("âœ… å‘é‡æ“ä½œå™¨åˆå§‹åŒ–æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            sys.exit(1)
    
    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬åµŒå…¥"""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=[text],
                dimensions=self.dimensions,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ è·å–åµŒå…¥å¤±è´¥: {e}")
            return []
    
    def vector_addition(self, vec1: List[float], vec2: List[float]) -> List[float]:
        """å‘é‡åŠ æ³•"""
        return (np.array(vec1) + np.array(vec2)).tolist()
    
    def vector_subtraction(self, vec1: List[float], vec2: List[float]) -> List[float]:
        """å‘é‡å‡æ³•"""
        return (np.array(vec1) - np.array(vec2)).tolist()
    
    def vector_scaling(self, vec: List[float], scale: float) -> List[float]:
        """å‘é‡ç¼©æ”¾"""
        return (np.array(vec) * scale).tolist()
    
    def vector_norm(self, vec: List[float]) -> float:
        """è®¡ç®—å‘é‡èŒƒæ•°"""
        return np.linalg.norm(vec)
    
    def vector_normalize(self, vec: List[float]) -> List[float]:
        """å‘é‡å½’ä¸€åŒ–"""
        norm = self.vector_norm(vec)
        if norm == 0:
            return vec
        return (np.array(vec) / norm).tolist()
    
    def vector_dot_product(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ç‚¹ç§¯"""
        return np.dot(vec1, vec2)
    
    def demonstrate_basic_operations(self):
        """æ¼”ç¤ºåŸºæœ¬å‘é‡æ“ä½œ"""
        print("ğŸ¯ ç¬¬1éƒ¨åˆ†ï¼šåŸºæœ¬å‘é‡æ“ä½œ")
        print("=" * 50)
        
        # è·å–ä¸¤ä¸ªæ–‡æœ¬çš„å‘é‡
        text1 = "å›½ç‹"
        text2 = "ç”·äºº"
        text3 = "å¥³äºº"
        
        vec1 = self.get_embedding(text1)
        vec2 = self.get_embedding(text2)
        vec3 = self.get_embedding(text3)
        
        if not all([vec1, vec2, vec3]):
            print("âŒ è·å–åµŒå…¥å¤±è´¥")
            return
        
        # æ¼”ç¤ºå„ç§æ“ä½œ
        print(f"ğŸ“Š åŸå§‹å‘é‡èŒƒæ•°:")
        print(f"   '{text1}': {self.vector_norm(vec1):.4f}")
        print(f"   '{text2}': {self.vector_norm(vec2):.4f}")
        print(f"   '{text3}': {self.vector_norm(vec3):.4f}")
        
        # å‘é‡åŠ æ³•
        addition = self.vector_addition(vec1, vec2)
        print(f"\nâ• å‘é‡åŠ æ³• '{text1}' + '{text2}':")
        print(f"   ç»“æœèŒƒæ•°: {self.vector_norm(addition):.4f}")
        
        # å‘é‡å‡æ³•
        subtraction = self.vector_subtraction(vec1, vec2)
        print(f"\nâ– å‘é‡å‡æ³• '{text1}' - '{text2}':")
        print(f"   ç»“æœèŒƒæ•°: {self.vector_norm(subtraction):.4f}")
        
        # å‘é‡ç¼©æ”¾
        scaled = self.vector_scaling(vec1, 2.0)
        print(f"\nğŸ“ å‘é‡ç¼©æ”¾ '{text1}' Ã— 2:")
        print(f"   åŸå§‹èŒƒæ•°: {self.vector_norm(vec1):.4f}")
        print(f"   ç¼©æ”¾åèŒƒæ•°: {self.vector_norm(scaled):.4f}")
        
        # å‘é‡å½’ä¸€åŒ–
        normalized = self.vector_normalize(vec1)
        print(f"\nğŸ¯ å‘é‡å½’ä¸€åŒ– '{text1}':")
        print(f"   å½’ä¸€åŒ–åèŒƒæ•°: {self.vector_norm(normalized):.4f}")
        
        # ç‚¹ç§¯
        dot_product = self.vector_dot_product(vec1, vec2)
        print(f"\nğŸ”¢ ç‚¹ç§¯ '{text1}' Â· '{text2}': {dot_product:.4f}")
    
    def demonstrate_semantic_analogy(self):
        """æ¼”ç¤ºè¯­ä¹‰ç±»æ¯”"""
        print("\nğŸ¯ ç¬¬2éƒ¨åˆ†ï¼šè¯­ä¹‰ç±»æ¯”è¿ç®—")
        print("=" * 50)
        
        # ç»å…¸çš„å›½ç‹-ç”·äºº+å¥³äºº=å¥³ç‹ç±»æ¯”
        texts = ["å›½ç‹", "ç”·äºº", "å¥³äºº", "å¥³ç‹"]
        embeddings = {}
        
        for text in texts:
            embeddings[text] = self.get_embedding(text)
        
        # æ‰§è¡Œç±»æ¯”è¿ç®—
        king = np.array(embeddings["å›½ç‹"])
        man = np.array(embeddings["ç”·äºº"])
        woman = np.array(embeddings["å¥³äºº"])
        queen = np.array(embeddings["å¥³ç‹"])
        
        # è®¡ç®—å›½ç‹ - ç”·äºº + å¥³äºº
        analogy_result = king - man + woman
        
        # è®¡ç®—ä¸å¥³ç‹çš„ç›¸ä¼¼åº¦
        similarity = np.dot(analogy_result, queen) / (np.linalg.norm(analogy_result) * np.linalg.norm(queen))
        
        print("ğŸ“ è¯­ä¹‰ç±»æ¯”è¿ç®—:")
        print(f"   å…¬å¼: å›½ç‹ - ç”·äºº + å¥³äºº â‰ˆ å¥³ç‹")
        print(f"   è®¡ç®—ç»“æœä¸'å¥³ç‹'çš„ç›¸ä¼¼åº¦: {similarity:.4f}")
        
        # éªŒè¯å…¶ä»–å¯èƒ½çš„ç­”æ¡ˆ
        candidates = ["å¥³ç‹", "å…¬ä¸»", "çš‡å", "ç‹å", "çš‡å¸"]
        candidate_embeddings = {cand: self.get_embedding(cand) for cand in candidates}
        
        print("\nğŸ” éªŒè¯å…¶ä»–å€™é€‰è¯:")
        for cand, emb in candidate_embeddings.items():
            cand_vec = np.array(emb)
            sim = np.dot(analogy_result, cand_vec) / (np.linalg.norm(analogy_result) * np.linalg.norm(cand_vec))
            print(f"   {cand}: {sim:.4f}")
    
    def demonstrate_vector_clustering(self):
        """æ¼”ç¤ºå‘é‡èšç±»"""
        print("\nğŸ¯ ç¬¬3éƒ¨åˆ†ï¼šå‘é‡èšç±»åˆ†æ")
        print("=" * 50)
        
        # å‡†å¤‡ä¸åŒç±»åˆ«çš„æ–‡æœ¬
        categories = {
            "æŠ€æœ¯": ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ"],
            "ç¾é£Ÿ": ["æŠ«è¨", "æ±‰å ¡", "å¯¿å¸", "ç«é”…"],
            "è¿åŠ¨": ["è¶³çƒ", "ç¯®çƒ", "æ¸¸æ³³", "è·‘æ­¥"]
        }
        
        # è·å–æ‰€æœ‰æ–‡æœ¬çš„åµŒå…¥
        all_texts = []
        all_embeddings = []
        labels = []
        
        for category, texts in categories.items():
            for text in texts:
                all_texts.append(text)
                labels.append(category)
                all_embeddings.append(self.get_embedding(text))
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        embeddings_array = np.array(all_embeddings)
        
        # è®¡ç®—ç±»åˆ«ä¸­å¿ƒ
        category_centers = {}
        for category, texts in categories.items():
            indices = [i for i, label in enumerate(labels) if label == category]
            category_embeddings = embeddings_array[indices]
            category_centers[category] = np.mean(category_embeddings, axis=0)
        
        print("ğŸ“Š ç±»åˆ«ä¸­å¿ƒè·ç¦»åˆ†æ:")
        for cat1, center1 in category_centers.items():
            for cat2, center2 in category_centers.items():
                if cat1 != cat2:
                    distance = np.linalg.norm(center1 - center2)
                    print(f"   {cat1} ä¸ {cat2} çš„è·ç¦»: {distance:.2f}")
        
        return all_texts, all_embeddings, labels
    
    def demonstrate_dimensionality_reduction(self):
        """æ¼”ç¤ºé™ç»´æŠ€æœ¯"""
        print("\nğŸ¯ ç¬¬4éƒ¨åˆ†ï¼šé™ç»´å¯è§†åŒ–")
        print("=" * 50)
        
        # è·å–ç¤ºä¾‹æ–‡æœ¬
        texts = [
            "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ",
            "æŠ«è¨", "æ±‰å ¡", "å¯¿å¸", "ç«é”…",
            "è¶³çƒ", "ç¯®çƒ", "æ¸¸æ³³", "è·‘æ­¥",
            "æ±½è½¦", "é£æœº", "ç«è½¦", "è‡ªè¡Œè½¦"
        ]
        
        # è·å–é«˜ç»´åµŒå…¥
        embeddings = [self.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        print(f"ğŸ“Š åŸå§‹ç»´åº¦: {embeddings_array.shape}")
        
        # PCAé™ç»´åˆ°2D
        pca = PCA(n_components=2)
        embeddings_2d_pca = pca.fit_transform(embeddings_array)
        
        # t-SNEé™ç»´åˆ°2D
        tsne = TSNE(n_components=2, random_state=42, perplexity=3)
        embeddings_2d_tsne = tsne.fit_transform(embeddings_array)
        
        # è®¾ç½®matplotlibä»¥æ”¯æŒä¸­æ–‡æ˜¾ç¤º
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # PCAå¯è§†åŒ–
        ax1.scatter(embeddings_2d_pca[:, 0], embeddings_2d_pca[:, 1], alpha=0.6)
        for i, text in enumerate(texts):
            ax1.annotate(text, (embeddings_2d_pca[i, 0], embeddings_2d_pca[i, 1]), 
                        fontsize=9, alpha=0.8)
        ax1.set_title("PCAé™ç»´åˆ°2D")
        ax1.grid(True, alpha=0.3)
        
        # t-SNEå¯è§†åŒ–
        ax2.scatter(embeddings_2d_tsne[:, 0], embeddings_2d_tsne[:, 1], alpha=0.6)
        for i, text in enumerate(texts):
            ax2.annotate(text, (embeddings_2d_tsne[i, 0], embeddings_2d_tsne[i, 1]), 
                        fontsize=9, alpha=0.8)
        ax2.set_title("t-SNEé™ç»´åˆ°2D")
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('01-basics/vector_operations_visualization.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… é™ç»´å¯è§†åŒ–å·²ä¿å­˜")
        
        # è®¡ç®—é™ç»´åçš„ä¿¡æ¯ä¿ç•™
        explained_variance = np.sum(pca.explained_variance_ratio_)
        print(f"ğŸ“ˆ PCAä¿ç•™çš„æ–¹å·®æ¯”ä¾‹: {explained_variance:.2%}")
    
    def demonstrate_vector_statistics(self):
        """æ¼”ç¤ºå‘é‡ç»Ÿè®¡åˆ†æ"""
        print("\nğŸ¯ ç¬¬5éƒ¨åˆ†ï¼šå‘é‡ç»Ÿè®¡åˆ†æ")
        print("=" * 50)
        
        # è·å–å¤šä¸ªæ–‡æœ¬çš„åµŒå…¥
        texts = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ"]
        embeddings = [self.get_embedding(text) for text in texts]
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        embeddings_array = np.array(embeddings)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        print("ğŸ“Š åµŒå…¥ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"   å‘é‡ç»´åº¦: {embeddings_array.shape[1]}")
        print(f"   å‘é‡å‡å€¼: {np.mean(embeddings_array):.4f}")
        print(f"   å‘é‡æ ‡å‡†å·®: {np.std(embeddings_array):.4f}")
        
        # è®¡ç®—æ¯ç»´åº¦çš„ç»Ÿè®¡
        dim_means = np.mean(embeddings_array, axis=0)
        dim_stds = np.std(embeddings_array, axis=0)
        
        print(f"\nğŸ“ˆ ç»´åº¦ç»Ÿè®¡ (å‰10ç»´):")
        for i in range(min(10, len(dim_means))):
            print(f"   ç»´åº¦ {i}: å‡å€¼={dim_means[i]:.4f}, æ ‡å‡†å·®={dim_stds[i]:.4f}")
        
        # è®¡ç®—æ–‡æœ¬é—´çš„è·ç¦»çŸ©é˜µ
        distance_matrix = np.zeros((len(texts), len(texts)))
        for i in range(len(texts)):
            for j in range(len(texts)):
                distance_matrix[i][j] = np.linalg.norm(embeddings_array[i] - embeddings_array[j])
        
        print(f"\nğŸ“ è·ç¦»çŸ©é˜µ:")
        print("æ–‡æœ¬:", texts)
        print(distance_matrix)
        
        return embeddings_array

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç¬¬4è¯¾ï¼šå‘é‡æ“ä½œåŸºç¡€")
    print("=" * 60)
    print("æœ¬è¯¾ç¨‹å°†æ•™ä½ å¦‚ä½•æ“ä½œå’Œåˆ†ææ–‡æœ¬åµŒå…¥å‘é‡ã€‚\n")
    
    try:
        # åˆ›å»ºæ“ä½œå™¨å®ä¾‹
        operator = VectorOperations()
        
        input("ğŸ“ æŒ‰å›è½¦é”®å¼€å§‹å­¦ä¹ åŸºæœ¬å‘é‡æ“ä½œ...")
        print("\n" + "="*60)
        operator.demonstrate_basic_operations()
        
        input("\nğŸ§  æŒ‰å›è½¦é”®ä½“éªŒè¯­ä¹‰ç±»æ¯”è¿ç®—...")
        print("\n" + "="*60)
        operator.demonstrate_semantic_analogy()
        
        input("\nğŸ“Š æŒ‰å›è½¦é”®è¿›è¡Œå‘é‡èšç±»åˆ†æ...")
        print("\n" + "="*60)
        texts, embeddings, labels = operator.demonstrate_vector_clustering()
        
        input("\nğŸ“‰ æŒ‰å›è½¦é”®å­¦ä¹ é™ç»´å¯è§†åŒ–...")
        print("\n" + "="*60)
        operator.demonstrate_dimensionality_reduction()
        
        input("\nğŸ“ˆ æŒ‰å›è½¦é”®è¿›è¡Œå‘é‡ç»Ÿè®¡åˆ†æ...")
        print("\n" + "="*60)
        embeddings_array = operator.demonstrate_vector_statistics()
        
        print("\n" + "="*60)
        print("ğŸ‰ ç¬¬4è¯¾å®Œæˆï¼")
        print("ğŸ“ åŸºç¡€è¯¾ç¨‹å·²å…¨éƒ¨å®Œæˆï¼")
        print("\nä½ å·²ç»æŒæ¡äº†ï¼š")
        print("âœ… åŸºæœ¬å‘é‡æ“ä½œï¼ˆåŠ å‡ä¹˜é™¤ï¼‰")
        print("âœ… è¯­ä¹‰ç±»æ¯”è¿ç®—")
        print("âœ… å‘é‡èšç±»åˆ†æ")
        print("âœ… é™ç»´å¯è§†åŒ–æŠ€æœ¯")
        print("âœ… å‘é‡ç»Ÿè®¡åˆ†æ")
        print("\nğŸ“‚ å¯è§†åŒ–ç»“æœå·²ä¿å­˜ä¸º 'vector_operations_visualization.png'")
        print("\nğŸš€ æ­å–œä½ å®Œæˆäº†åŸºç¡€è¯¾ç¨‹ï¼")
        print("\nğŸ¯ å‡†å¤‡è¿›å…¥ä¸­çº§åº”ç”¨é˜¶æ®µ...")
        print("\nä¸­çº§æ¨¡å—ï¼š02-intermediate/01-semantic-search.py - è¯­ä¹‰æœç´¢")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è¯¾ç¨‹å·²ä¸­æ–­ï¼Œæ¬¢è¿ä¸‹æ¬¡ç»§ç»­å­¦ä¹ ï¼")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print("ğŸ”„ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®åé‡è¯•")
    finally:
        input("\nğŸ“š æŒ‰å›è½¦é”®é€€å‡ºåŸºç¡€è¯¾ç¨‹...")

if __name__ == "__main__":
    main()