#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ–‡æœ¬åµŒå…¥åº”ç”¨ç¤ºä¾‹
====================

æœ¬æ–‡ä»¶å±•ç¤ºæ–‡æœ¬åµŒå…¥åœ¨å®é™…ä¸šåŠ¡ä¸­çš„é«˜çº§åº”ç”¨
"""

import os
import json
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Optional
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import pandas as pd
from datetime import datetime
import sqlite3

class AdvancedEmbeddingSystem:
    """é«˜çº§æ–‡æœ¬åµŒå…¥ç³»ç»Ÿ"""
    
    def __init__(self, api_key: str = None, db_path: str = "embeddings.db"):
        self.client = OpenAI(
            api_key=api_key or os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.model = "text-embedding-v4"
        self.dimensions = 1024
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                embedding TEXT NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
    
    def add_document(self, text: str, metadata: Dict = None) -> int:
        """æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“"""
        embedding = self.get_embedding(text)
        embedding_str = json.dumps(embedding)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO documents (text, embedding, metadata) VALUES (?, ?, ?)",
            (text, embedding_str, json.dumps(metadata or {}))
        )
        doc_id = cursor.lastrowid
        conn.commit()
        conn.close()
        return doc_id
    
    def search_similar(self, query: str, limit: int = 5, threshold: float = 0.7) -> List[Dict]:
        """åœ¨æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        query_embedding = self.get_embedding(query)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT id, text, embedding, metadata FROM documents")
        
        results = []
        for row in cursor.fetchall():
            doc_id, text, embedding_str, metadata = row
            doc_embedding = json.loads(embedding_str)
            similarity = self.cosine_similarity(query_embedding, doc_embedding)
            
            if similarity >= threshold:
                results.append({
                    'id': doc_id,
                    'text': text,
                    'similarity': similarity,
                    'metadata': json.loads(metadata)
                })
        
        conn.close()
        return sorted(results, key=lambda x: x['similarity'], reverse=True)[:limit]
    
    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬åµŒå…¥"""
        response = self.client.embeddings.create(
            model=self.model,
            input=[text],
            dimensions=self.dimensions,
            encoding_format="float"
        )
        return response.data[0].embedding
    
    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡è·å–æ–‡æœ¬åµŒå…¥"""
        # APIé™åˆ¶æ‰¹å¤„ç†å¤§å°ä¸º10
        max_batch_size = 10
        all_embeddings = []
        
        for i in range(0, len(texts), max_batch_size):
            batch = texts[i:i+max_batch_size]
            response = self.client.embeddings.create(
                model=self.model,
                input=batch,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            all_embeddings.extend([data.embedding for data in response.data])
        
        return all_embeddings
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    def visualize_embeddings(self, texts: List[str], labels: List[str] = None, 
                           method: str = "tsne", save_path: str = None):
        """å¯è§†åŒ–æ–‡æœ¬åµŒå…¥"""
        # é™åˆ¶æ‰¹å¤„ç†å¤§å°ä¸º10ä»¥å†…
        batch_size = 10
        if len(texts) > batch_size:
            print(f"âš ï¸ æ–‡æœ¬æ•°é‡è¿‡å¤š({len(texts)}ä¸ª)ï¼Œé™åˆ¶ä¸º{batch_size}ä¸ª")
            texts = texts[:batch_size]
            if labels:
                labels = labels[:batch_size]
        
        embeddings = self.get_embeddings_batch(texts)
        embeddings_array = np.array(embeddings)
        
        # é™ç»´
        if method == "pca":
            reducer = PCA(n_components=2, random_state=42)
        else:  # tsne
            reducer = TSNE(n_components=2, random_state=42, perplexity=min(5, len(texts)-1))
        
        reduced_embeddings = reducer.fit_transform(embeddings_array)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], 
                            alpha=0.6, s=100)
        
        # æ·»åŠ æ ‡ç­¾ - ä½¿ç”¨è‹±æ–‡æ ‡ç­¾é¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
        fallback_labels = [
            "ML", "DL", "NN", "Python", "Java", "Web", 
            "Food", "Pasta", "Sushi", "Sports"
        ]
        
        for i, text in enumerate(texts):
            if labels and i < len(labels):
                label = labels[i]
                # å¦‚æœæ ‡ç­¾æ˜¯ä¸­æ–‡ï¼Œä½¿ç”¨è‹±æ–‡fallback
                if any('\u4e00' <= c <= '\u9fff' for c in str(label)):
                    label = fallback_labels[i] if i < len(fallback_labels) else f"Item{i+1}"
            else:
                # ä¸­æ–‡æ–‡æœ¬ä½¿ç”¨è‹±æ–‡ç¼©å†™
                label = fallback_labels[i] if i < len(fallback_labels) else f"Item{i+1}"
            
            ax.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), 
                        fontsize=10, alpha=0.8, ha='center')
        
        ax.set_title(f"Text Embedding Visualization ({method.upper()})")
        ax.set_xlabel("Component 1")
        ax.set_ylabel("Component 2")
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ–‡æœ¬è¯´æ˜
        fig.text(0.02, 0.02, 
                f"Texts: {len(texts)} | Method: {method.upper()} | Dimension: {self.dimensions}", 
                fontsize=8, alpha=0.7)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜ä¸º {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def build_knowledge_base(self, documents: List[Dict]):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("ğŸ”¨ æ„å»ºçŸ¥è¯†åº“...")
        for doc in documents:
            doc_id = self.add_document(
                doc['text'], 
                doc.get('metadata', {})
            )
            print(f"âœ… å·²æ·»åŠ æ–‡æ¡£ {doc_id}: {doc['text'][:50]}...")
    
    def smart_qa_system(self, question: str, context_docs: int = 3) -> Dict:
        """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""
        similar_docs = self.search_similar(question, limit=context_docs)
        
        if not similar_docs:
            return {
                "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯",
                "confidence": 0.0,
                "sources": []
            }
        
        # ç®€å•çš„é—®é¢˜å›ç­”ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„LLMï¼‰
        context = "\n".join([doc['text'] for doc in similar_docs])
        
        return {
            "answer": f"åŸºäºæ‰¾åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼Œå¯èƒ½çš„ç­”æ¡ˆæ˜¯ï¼š{similar_docs[0]['text'][:100]}...",
            "confidence": similar_docs[0]['similarity'],
            "sources": similar_docs
        }

def demo_knowledge_base():
    """æ¼”ç¤ºçŸ¥è¯†åº“ç³»ç»Ÿ"""
    print("ğŸ“š çŸ¥è¯†åº“ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    system = AdvancedEmbeddingSystem()
    
    # æ„å»ºçŸ¥è¯†åº“
    documents = [
        {
            "text": "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼",
            "metadata": {"category": "æŠ€æœ¯", "tags": ["AI", "åŸºç¡€"]}
        },
        {
            "text": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚ä»»åŠ¡",
            "metadata": {"category": "æŠ€æœ¯", "tags": ["AI", "æ·±åº¦å­¦ä¹ "]}
        },
        {
            "text": "Pythonæ˜¯æœ€æµè¡Œçš„æ•°æ®ç§‘å­¦ç¼–ç¨‹è¯­è¨€ï¼Œæœ‰ä¸°å¯Œçš„åº“æ”¯æŒ",
            "metadata": {"category": "ç¼–ç¨‹", "tags": ["Python", "æ•°æ®ç§‘å­¦"]}
        },
        {
            "text": "å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ä¸»è¦ç”¨äºå›¾åƒè¯†åˆ«å’Œå¤„ç†ä»»åŠ¡",
            "metadata": {"category": "æŠ€æœ¯", "tags": ["æ·±åº¦å­¦ä¹ ", "CNN"]}
        },
        {
            "text": "å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)é€‚ç”¨äºå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—",
            "metadata": {"category": "æŠ€æœ¯", "tags": ["æ·±åº¦å­¦ä¹ ", "RNN"]}
        }
    ]
    
    system.build_knowledge_base(documents)
    
    # é—®ç­”æµ‹è¯•
    questions = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "CNNå’ŒRNNæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "Pythonåœ¨æ•°æ®ç§‘å­¦ä¸­çš„åº”ç”¨"
    ]
    
    for question in questions:
        print(f"\nâ“ é—®é¢˜: {question}")
        result = system.smart_qa_system(question)
        print(f"ğŸ¤– å›ç­”: {result['answer']}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {result['confidence']:.3f}")

def demo_anomaly_detection():
    """æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹"""
    print("\nğŸ” å¼‚å¸¸æ£€æµ‹æ¼”ç¤º")
    print("=" * 50)
    
    system = AdvancedEmbeddingSystem()
    
    # æ­£å¸¸è¯„è®º
    normal_reviews = [
        "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„",
        "ç‰©æµé€Ÿåº¦å¿«ï¼ŒåŒ…è£…å®Œå¥½",
        "å®¢æœæ€åº¦å¾ˆå¥½ï¼Œè§£å†³é—®é¢˜åŠæ—¶",
        "ä»·æ ¼åˆç†ï¼Œç‰©æœ‰æ‰€å€¼",
        "ä¼šå†æ¬¡è´­ä¹°ï¼Œæ¨èç»™å¤§å®¶"
    ]
    
    # å¼‚å¸¸è¯„è®ºï¼ˆå¯èƒ½åŒ…å«å¹¿å‘Šã€åƒåœ¾ä¿¡æ¯ç­‰ï¼‰
    suspicious_reviews = [
        "ğŸ”¥ğŸ”¥ğŸ”¥é™æ—¶æŠ¢è´­ï¼ç‚¹å‡»é“¾æ¥è·å–ä¼˜æƒ ğŸ”¥ğŸ”¥ğŸ”¥",
        "åŠ æˆ‘å¾®ä¿¡ï¼šXXXXXï¼Œè·å–æ›´å¤šä¼˜æƒ ä¿¡æ¯",
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è¯„è®ºï¼Œæ²¡æœ‰ä»»ä½•æ„ä¹‰çš„å†…å®¹",
        "åƒåœ¾åƒåœ¾åƒåœ¾åƒåœ¾åƒåœ¾åƒåœ¾åƒåœ¾åƒåœ¾",
        "è¯·è”ç³»QQï¼š123456789ï¼Œä¸“ä¸šåˆ·å•å›¢é˜Ÿ"
    ]
    
    # è®¡ç®—æ­£å¸¸è¯„è®ºçš„ä¸­å¿ƒå‘é‡
    normal_embeddings = system.get_embeddings_batch(normal_reviews)
    normal_center = np.mean(normal_embeddings, axis=0)
    
    # æ£€æµ‹å¼‚å¸¸
    all_reviews = normal_reviews + suspicious_reviews
    all_embeddings = system.get_embeddings_batch(all_reviews)
    
    threshold = 0.5  # å¼‚å¸¸é˜ˆå€¼
    
    for review, embedding in zip(all_reviews, all_embeddings):
        similarity = system.cosine_similarity(embedding, normal_center.tolist())
        
        if similarity < threshold:
            print(f"âš ï¸ å¼‚å¸¸: {review} (ç›¸ä¼¼åº¦: {similarity:.3f})")
        else:
            print(f"âœ… æ­£å¸¸: {review[:20]}... (ç›¸ä¼¼åº¦: {similarity:.3f})")

def demo_semantic_search_engine():
    """æ¼”ç¤ºè¯­ä¹‰æœç´¢å¼•æ“"""
    print("\nğŸ” è¯­ä¹‰æœç´¢å¼•æ“æ¼”ç¤º")
    print("=" * 50)
    
    system = AdvancedEmbeddingSystem()
    
    # ç”µå•†äº§å“æè¿°
    products = [
        "iPhone 15 Pro Max 256GB åŸè‰²é’›é‡‘å± 5Gæ‰‹æœº",
        "åä¸ºMate 60 Pro 12GB+512GB é›…é»‘ å«æ˜Ÿé€šä¿¡",
        "å°ç±³14 Ultra 16GB+1TB é’›é‡‘å±ç‰ˆ å¾•å¡å½±åƒ",
        "MacBook Pro M3 14è‹±å¯¸ 18GB+512GB æ·±ç©ºé»‘",
        "åä¸ºMateBook X Pro 13ä»£é…·ç¿ 32GB+1TB",
        "æˆ´æ£®V12æ— çº¿å¸å°˜å™¨ æ‰‹æŒé™¤è¨ æ¿€å…‰æ¢æµ‹",
        "iPad Pro M2 12.9è‹±å¯¸ 256GB WiFiç‰ˆ",
        "ç´¢å°¼WH-1000XM5 é™å™ªè€³æœº æ— çº¿è“ç‰™"
    ]
    
    # æ·»åŠ åˆ°çŸ¥è¯†åº“
    for product in products:
        system.add_document(product, {"type": "product", "category": "ç”µå­äº§å“"})
    
    # ç”¨æˆ·æœç´¢æŸ¥è¯¢
    search_queries = [
        "æœ€å¥½çš„æ‹ç…§æ‰‹æœº",
        "åŠå…¬ç”¨çš„ç¬”è®°æœ¬ç”µè„‘",
        "è‹¹æœçš„äº§å“",
        "æ— çº¿è€³æœº",
        "é«˜ç«¯æ‰‹æœº"
    ]
    
    for query in search_queries:
        print(f"\nğŸ” æœç´¢: '{query}'")
        results = system.search_similar(query, limit=3)
        
        for result in results:
            print(f"  ğŸ“± {result['text']} (ç›¸å…³åº¦: {result['similarity']:.3f})")

def demo_visualization():
    """æ¼”ç¤ºå¯è§†åŒ–"""
    print("\nğŸ¨ æ–‡æœ¬åµŒå…¥å¯è§†åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    system = AdvancedEmbeddingSystem()
    
    # å‡†å¤‡æ•°æ® - é™åˆ¶ä¸º10ä¸ªä»¥å†…
    texts = [
        "æœºå™¨å­¦ä¹ ç®—æ³•",
        "æ·±åº¦å­¦ä¹ æ¨¡å‹",
        "ç¥ç»ç½‘ç»œ",
        "Pythonç¼–ç¨‹",
        "Javaå¼€å‘",
        "Webå‰ç«¯",
        "ç‰›æ’çƒ¹é¥ª",
        "æ„å¤§åˆ©é¢",
        "å¯¿å¸åˆ¶ä½œ",
        "è¶³çƒæ¯”èµ›"
    ]
    
    labels = ["AI", "AI", "AI", "ç¼–ç¨‹", "ç¼–ç¨‹", "ç¼–ç¨‹", 
              "ç¾é£Ÿ", "ç¾é£Ÿ", "ç¾é£Ÿ", "è¿åŠ¨"]
    
    # åˆ›å»ºå¯è§†åŒ–
    system.visualize_embeddings(texts, labels, method="tsne", 
                               save_path="embeddings_visualization.png")
    print("âœ… å¯è§†åŒ–å·²ä¿å­˜ä¸º embeddings_visualization.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜çº§æ–‡æœ¬åµŒå…¥åº”ç”¨æ¼”ç¤º")
    print("=" * 60)
    
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
        return
    
    try:
        # è¿è¡Œé«˜çº§æ¼”ç¤º
        demo_knowledge_base()
        demo_anomaly_detection()
        demo_semantic_search_engine()
        demo_visualization()
        
        print("\nğŸ‰ é«˜çº§åº”ç”¨æ¼”ç¤ºå®Œæˆï¼")
        print("è¿™äº›æŠ€æœ¯å¯ä»¥åº”ç”¨äºï¼š")
        print("   â€¢ æ™ºèƒ½å®¢æœç³»ç»Ÿ")
        print("   â€¢ å†…å®¹æ¨èå¼•æ“")
        print("   â€¢ åƒåœ¾å†…å®¹è¿‡æ»¤")
        print("   â€¢ è¯­ä¹‰æœç´¢å¼•æ“")
        print("   â€¢ çŸ¥è¯†å›¾è°±æ„å»º")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()