#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬åµŒå…¥(Embedding)å…¨é¢å­¦ä¹ æ•™ç¨‹
================================

ä»€ä¹ˆæ˜¯æ–‡æœ¬åµŒå…¥ï¼Ÿ
æ–‡æœ¬åµŒå…¥æ˜¯å°†æ–‡æœ¬(è¯è¯­ã€å¥å­ã€æ–‡æ¡£)è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„æ•°å€¼å‘é‡çš„æŠ€æœ¯ã€‚
è¿™äº›å‘é‡èƒ½å¤Ÿæ•æ‰æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»ç›¸è¿‘ã€‚

ç”¨é€”ï¼š
1. è¯­ä¹‰æœç´¢ - ç†è§£æŸ¥è¯¢æ„å›¾ï¼Œè¿”å›ç›¸å…³ç»“æœ
2. æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®— - åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼ç¨‹åº¦
3. æ¨èç³»ç»Ÿ - åŸºäºå†…å®¹ç›¸ä¼¼æ€§æ¨è
4. èšç±»åˆ†æ - å°†ç›¸ä¼¼æ–‡æœ¬åˆ†ç»„
5. å¼‚å¸¸æ£€æµ‹ - è¯†åˆ«ä¸å¸¸è§„å†…å®¹ä¸ç¬¦çš„æ–‡æœ¬
6. æƒ…æ„Ÿåˆ†æ - å°†æ–‡æœ¬æ˜ å°„åˆ°æƒ…æ„Ÿç©ºé—´
7. é—®ç­”ç³»ç»Ÿ - æ‰¾åˆ°ä¸ç”¨æˆ·é—®é¢˜æœ€åŒ¹é…çš„ç­”æ¡ˆ
"""

import os
import json
import numpy as np
from typing import List, Dict, Tuple
from openai import OpenAI
from datetime import datetime
import pickle

class EmbeddingTutorial:
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–åµŒå…¥å®¢æˆ·ç«¯"""
        self.client = OpenAI(
            api_key=api_key or os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.model = "text-embedding-v4"
        self.dimensions = 1024
        
    def get_embedding(self, text: str) -> List[float]:
        """è·å–å•æ®µæ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        response = self.client.embeddings.create(
            model=self.model,
            input=[text],
            dimensions=self.dimensions,
            encoding_format="float"
        )
        return response.data[0].embedding
    
    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        response = self.client.embeddings.create(
            model=self.model,
            input=texts,
            dimensions=self.dimensions,
            encoding_format="float"
        )
        return [data.embedding for data in response.data]
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    def find_similar_texts(self, query: str, texts: List[str], top_k: int = 3) -> List[Tuple[str, float]]:
        """æ‰¾åˆ°ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬"""
        query_embedding = self.get_embedding(query)
        text_embeddings = self.get_embeddings_batch(texts)
        
        similarities = []
        for text, embedding in zip(texts, text_embeddings):
            sim = self.cosine_similarity(query_embedding, embedding)
            similarities.append((text, sim))
        
        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]

def demo_semantic_search():
    """æ¼”ç¤ºè¯­ä¹‰æœç´¢åŠŸèƒ½"""
    print("ğŸ¯ è¯­ä¹‰æœç´¢æ¼”ç¤º")
    print("=" * 50)
    
    # ç¤ºä¾‹æ–‡æ¡£åº“
    documents = [
        "è‹¹æœæ‰‹æœºæœ€æ–°æ¬¾iPhone 15å‘å¸ƒäº†ï¼Œé…å¤‡A17èŠ¯ç‰‡",
        "åä¸ºMate 60ç³»åˆ—æ­è½½éº’éºŸ9000Så¤„ç†å™¨ï¼Œæ”¯æŒ5Gç½‘ç»œ",
        "å°ç±³14ç³»åˆ—é¦–å‘éªé¾™8 Gen3ï¼Œæ€§èƒ½å¤§å¹…æå‡",
        "ç‰¹æ–¯æ‹‰Model Yé™ä»·2ä¸‡ï¼Œç”µåŠ¨è½¦å¸‚åœºç«äº‰æ¿€çƒˆ",
        "æ¯”äºšè¿ªæµ·è±¹DM-iæ··åŠ¨ç‰ˆæœ¬å³å°†ä¸Šå¸‚ï¼Œç»­èˆªè¶…1300å…¬é‡Œ",
        "æ˜Ÿå·´å…‹æ¨å‡ºç§‹å­£é™å®šé¥®å“ï¼Œå—ç“œæ‹¿é“å›å½’",
        "èŒ…å°é…’ä»·æ ¼ä¸Šæ¶¨ï¼Œç™½é…’å¸‚åœºæŒç»­å‡æ¸©",
        "ChatGPTæ¨å‡ºè¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼ŒAIåŠ©æ‰‹æ›´åŠ æ™ºèƒ½"
    ]
    
    tutorial = EmbeddingTutorial()
    
    # æœç´¢æŸ¥è¯¢
    queries = ["æ‰‹æœºæ–°å“", "ç”µåŠ¨è½¦", "äººå·¥æ™ºèƒ½", "å’–å•¡é¥®å“"]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
        results = tutorial.find_similar_texts(query, documents, top_k=2)
        for text, score in results:
            print(f"  ğŸ“„ {text} (ç›¸ä¼¼åº¦: {score:.3f})")

def demo_text_classification():
    """æ¼”ç¤ºæ–‡æœ¬åˆ†ç±»"""
    print("\nğŸ·ï¸ æ–‡æœ¬åˆ†ç±»æ¼”ç¤º")
    print("=" * 50)
    
    # é¢„å®šä¹‰ç±»åˆ«
    categories = {
        "ç§‘æŠ€": ["äººå·¥æ™ºèƒ½çªç ´", "æ–°æ¬¾èŠ¯ç‰‡å‘å¸ƒ", "æ“ä½œç³»ç»Ÿå‡çº§"],
        "è´¢ç»": ["è‚¡å¸‚å¤§æ¶¨", "å¤®è¡Œé™æ¯", "ä¼ä¸šè´¢æŠ¥"],
        "å¨±ä¹": ["ç”µå½±ä¸Šæ˜ ", "æ˜æ˜Ÿå…«å¦", "éŸ³ä¹ä¸“è¾‘"],
        "ä½“è‚²": ["è¶³çƒæ¯”èµ›", "ç¯®çƒè”èµ›", "å¥¥è¿ä¼š"]
    }
    
    tutorial = EmbeddingTutorial()
    
    # è·å–æ¯ä¸ªç±»åˆ«çš„ä¸­å¿ƒå‘é‡
    category_centers = {}
    for category, examples in categories.items():
        embeddings = tutorial.get_embeddings_batch(examples)
        center = np.mean(embeddings, axis=0)
        category_centers[category] = center
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "è°·æ­Œå‘å¸ƒæœ€æ–°AIæ¨¡å‹Gemini",
        "ç¾è”å‚¨åŠ æ¯å½±å“å…¨çƒè‚¡å¸‚",
        "NBAæ€»å†³èµ›å³å°†æ‰“å“",
        "æ–°ç”µå½±ç¥¨æˆ¿ç ´10äº¿"
    ]
    
    for text in test_texts:
        text_embedding = np.array(tutorial.get_embedding(text))
        
        best_category = None
        best_score = -1
        
        for category, center in category_centers.items():
            similarity = tutorial.cosine_similarity(text_embedding.tolist(), center.tolist())
            if similarity > best_score:
                best_score = similarity
                best_category = category
        
        print(f"ğŸ“„ '{text}' -> {best_category} (ç½®ä¿¡åº¦: {best_score:.3f})")

def demo_recommendation_system():
    """æ¼”ç¤ºæ¨èç³»ç»Ÿ"""
    print("\nğŸ¯ åŸºäºå†…å®¹çš„æ¨èæ¼”ç¤º")
    print("=" * 50)
    
    # ç”¨æˆ·é˜…è¯»å†å²
    user_history = [
        "æ·±åº¦å­¦ä¹ é©å‘½ï¼šç¥ç»ç½‘ç»œçš„å‘å±•å†ç¨‹",
        "æœºå™¨å­¦ä¹ å…¥é—¨ï¼šä»é›¶å¼€å§‹ç†è§£ç®—æ³•",
        "äººå·¥æ™ºèƒ½çš„æœªæ¥è¶‹åŠ¿åˆ†æ",
        "æ•°æ®ç§‘å­¦å®¶å¿…å¤‡æŠ€èƒ½æŒ‡å—"
    ]
    
    # å¾…æ¨èæ–‡ç« 
    articles = [
        "ç¥ç»ç½‘ç»œåœ¨å›¾åƒè¯†åˆ«ä¸­çš„åº”ç”¨æ¡ˆä¾‹",
        "ä¼ ç»Ÿç»Ÿè®¡å­¦ä¸æœºå™¨å­¦ä¹ çš„åŒºåˆ«",
        "åŒºå—é“¾æŠ€æœ¯å¦‚ä½•æ”¹å˜é‡‘èè¡Œä¸š",
        "Pythonæ•°æ®åˆ†æå®æˆ˜æ•™ç¨‹",
        "äº‘è®¡ç®—æœåŠ¡æ¯”è¾ƒï¼šAWS vs Azure vs GCP",
        "æ·±åº¦å­¦ä¹ æ¡†æ¶æ¯”è¾ƒï¼šTensorFlow vs PyTorch",
        "ç½‘ç»œå®‰å…¨å¨èƒä¸é˜²æŠ¤æªæ–½",
        "äººå·¥æ™ºèƒ½ä¼¦ç†é—®é¢˜æ¢è®¨"
    ]
    
    tutorial = EmbeddingTutorial()
    
    # è·å–ç”¨æˆ·å…´è¶£å‘é‡
    history_embeddings = tutorial.get_embeddings_batch(user_history)
    user_interest = np.mean(history_embeddings, axis=0)
    
    # è®¡ç®—æ¨èåˆ†æ•°
    article_embeddings = tutorial.get_embeddings_batch(articles)
    recommendations = []
    
    for article, embedding in zip(articles, article_embeddings):
        score = tutorial.cosine_similarity(user_interest.tolist(), embedding)
        recommendations.append((article, score))
    
    # æ’åºå¹¶æ˜¾ç¤ºå‰5ä¸ªæ¨è
    recommendations.sort(key=lambda x: x[1], reverse=True)
    
    print("ä¸ºæ‚¨æ¨èçš„æ–‡ç« ï¼š")
    for i, (article, score) in enumerate(recommendations[:5], 1):
        print(f"{i}. {article} (ç›¸å…³åº¦: {score:.3f})")

def demo_sentiment_analysis():
    """æ¼”ç¤ºæƒ…æ„Ÿåˆ†æ"""
    print("\nğŸ˜Š æƒ…æ„Ÿåˆ†ææ¼”ç¤º")
    print("=" * 50)
    
    # æƒ…æ„ŸåŸºå‡†æ–‡æœ¬
    positive_examples = [
        "å¤ªæ£’äº†ï¼è¿™ä¸ªäº§å“éå¸¸å¥½ç”¨",
        "æœåŠ¡å¾ˆè´´å¿ƒï¼Œä½“éªŒæä½³",
        "éå¸¸æ»¡æ„ï¼Œå¼ºçƒˆæ¨è"
    ]
    
    negative_examples = [
        "å¾ˆå·®åŠ²ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼",
        "æœåŠ¡æ€åº¦æ¶åŠ£ï¼Œè®©äººå¤±æœ›",
        "äº§å“è´¨é‡æœ‰é—®é¢˜ï¼Œä¸æ¨èè´­ä¹°"
    ]
    
    tutorial = EmbeddingTutorial()
    
    # è·å–æƒ…æ„ŸåŸºå‡†å‘é‡
    pos_embeddings = tutorial.get_embeddings_batch(positive_examples)
    neg_embeddings = tutorial.get_embeddings_batch(negative_examples)
    
    pos_center = np.mean(pos_embeddings, axis=0)
    neg_center = np.mean(neg_embeddings, axis=0)
    
    # æµ‹è¯•è¯„è®º
    reviews = [
        "è¿™ä¸ªäº§å“çœŸçš„å¾ˆå¥½ç”¨ï¼Œç‰©è¶…æ‰€å€¼ï¼",
        "ä¸€èˆ¬èˆ¬ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„",
        "è´¨é‡å¤ªå·®äº†ï¼Œåæ‚”è´­ä¹°",
        "å®¢æœå¾ˆè€å¿ƒï¼Œé—®é¢˜è§£å†³äº†",
        "ä»·æ ¼æœ‰ç‚¹è´µï¼Œä½†æ•ˆæœè¿˜å¯ä»¥"
    ]
    
    for review in reviews:
        embedding = np.array(tutorial.get_embedding(review))
        
        pos_sim = tutorial.cosine_similarity(embedding.tolist(), pos_center.tolist())
        neg_sim = tutorial.cosine_similarity(embedding.tolist(), neg_center.tolist())
        
        if pos_sim > neg_sim:
            sentiment = "æ­£é¢"
            confidence = pos_sim
        else:
            sentiment = "è´Ÿé¢"
            confidence = neg_sim
        
        print(f"ğŸ“„ '{review}' -> {sentiment}æƒ…æ„Ÿ (ç½®ä¿¡åº¦: {confidence:.3f})")

def demo_clustering():
    """æ¼”ç¤ºæ–‡æœ¬èšç±»"""
    print("\nğŸ¯ æ–‡æœ¬èšç±»æ¼”ç¤º")
    print("=" * 50)
    
    from sklearn.cluster import KMeans
    
    # æ··åˆæ–‡æœ¬
    texts = [
        "iPhone 15å‘å¸ƒï¼Œæ€§èƒ½å¤§å¹…æå‡",
        "åä¸ºMate 60æ”¯æŒå«æ˜Ÿé€šä¿¡åŠŸèƒ½",
        "ç‰¹æ–¯æ‹‰Model 3ä»·æ ¼é™è‡³20ä¸‡ä»¥å†…",
        "æ¯”äºšè¿ªæ–°èƒ½æºè½¦é”€é‡åˆ›æ–°é«˜",
        "å‘¨æ°ä¼¦æ–°ä¸“è¾‘å³å°†å‘è¡Œ",
        "Taylor Swiftä¸–ç•Œå·¡æ¼”å¼€å§‹",
        "è‚¡å¸‚å¤§æ¶¨ï¼ŒæŠ•èµ„è€…ä¿¡å¿ƒå¢å¼º",
        "å¤®è¡Œå®£å¸ƒé™æ¯æ”¿ç­–"
    ]
    
    tutorial = EmbeddingTutorial()
    embeddings = tutorial.get_embeddings_batch(texts)
    
    # ä½¿ç”¨K-meansèšç±»
    kmeans = KMeans(n_clusters=3, random_state=42)
    clusters = kmeans.fit_predict(embeddings)
    
    # æ˜¾ç¤ºèšç±»ç»“æœ
    cluster_groups = {0: [], 1: [], 2: []}
    for text, cluster in zip(texts, clusters):
        cluster_groups[cluster].append(text)
    
    for cluster_id, group_texts in cluster_groups.items():
        print(f"\nèšç±» {cluster_id + 1}:")
        for text in group_texts:
            print(f"  ğŸ“„ {text}")

def save_and_load_embeddings():
    """æ¼”ç¤ºåµŒå…¥å‘é‡çš„ä¿å­˜å’ŒåŠ è½½"""
    print("\nğŸ’¾ åµŒå…¥å‘é‡ä¿å­˜ä¸åŠ è½½")
    print("=" * 50)
    
    tutorial = EmbeddingTutorial()
    
    # åˆ›å»ºçŸ¥è¯†åº“
    knowledge_base = {
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºé€šè¿‡æ•°æ®å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ ": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚é—®é¢˜",
        "è‡ªç„¶è¯­è¨€å¤„ç†": "NLPè®©è®¡ç®—æœºç†è§£å’Œå¤„ç†äººç±»è¯­è¨€",
        "è®¡ç®—æœºè§†è§‰": "è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œåˆ†æå›¾åƒå’Œè§†é¢‘å†…å®¹"
    }
    
    # ç”Ÿæˆå¹¶ä¿å­˜åµŒå…¥
    embeddings = {}
    for title, content in knowledge_base.items():
        embedding = tutorial.get_embedding(content)
        embeddings[title] = {
            "content": content,
            "embedding": embedding
        }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    filename = f"knowledge_embeddings_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
    with open(filename, 'wb') as f:
        pickle.dump(embeddings, f)
    
    print(f"âœ… çŸ¥è¯†åº“åµŒå…¥å·²ä¿å­˜åˆ°: {filename}")
    
    # ä»æ–‡ä»¶åŠ è½½
    with open(filename, 'rb') as f:
        loaded_embeddings = pickle.load(f)
    
    print(f"ğŸ“Š å·²åŠ è½½ {len(loaded_embeddings)} ä¸ªçŸ¥è¯†æ¡ç›®")
    
    # ä½¿ç”¨åŠ è½½çš„åµŒå…¥è¿›è¡Œæœç´¢
    query = "ä»€ä¹ˆæ˜¯AI"
    results = []
    
    query_embedding = tutorial.get_embedding(query)
    for title, data in loaded_embeddings.items():
        similarity = tutorial.cosine_similarity(query_embedding, data["embedding"])
        results.append((title, data["content"], similarity))
    
    results.sort(key=lambda x: x[2], reverse=True)
    
    print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
    for title, content, score in results[:2]:
        print(f"ğŸ“„ {title} (ç›¸ä¼¼åº¦: {score:.3f})")
        print(f"   {content}")

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("ğŸš€ æ–‡æœ¬åµŒå…¥(Embedding)å…¨é¢å­¦ä¹ æ•™ç¨‹")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
        return
    
    try:
        # è¿è¡Œå„ç§æ¼”ç¤º
        demo_semantic_search()
        demo_text_classification()
        demo_recommendation_system()
        demo_sentiment_analysis()
        demo_clustering()
        save_and_load_embeddings()
        
        print("\nğŸ‰ æ•™ç¨‹å®Œæˆï¼æ–‡æœ¬åµŒå…¥çš„åº”ç”¨åœºæ™¯åŒ…æ‹¬ï¼š")
        print("   â€¢ è¯­ä¹‰æœç´¢å’Œé—®ç­”ç³»ç»Ÿ")
        print("   â€¢ æ–‡æœ¬åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æ")
        print("   â€¢ ä¸ªæ€§åŒ–æ¨è")
        print("   â€¢ å†…å®¹èšç±»å’Œå»é‡")
        print("   â€¢ å¼‚å¸¸æ£€æµ‹å’Œåƒåœ¾å†…å®¹è¿‡æ»¤")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()