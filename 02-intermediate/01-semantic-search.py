#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­çº§è¯¾ç¨‹ç¬¬1è¯¾ï¼šè¯­ä¹‰æœç´¢ç³»ç»Ÿ
============================

æœ¬è¯¾ç¨‹å°†æ•™ä½ æ„å»ºä¸€ä¸ªå®Œæ•´çš„è¯­ä¹‰æœç´¢ç³»ç»Ÿã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£è¯­ä¹‰æœç´¢vså…³é”®è¯æœç´¢çš„åŒºåˆ«
2. æ„å»ºæ–‡æ¡£ç´¢å¼•ç³»ç»Ÿ
3. å®ç°æ™ºèƒ½æœç´¢ç®—æ³•
4. ä¼˜åŒ–æœç´¢ç»“æœæ’åº
5. æ·»åŠ æœç´¢å»ºè®®åŠŸèƒ½

"""

import os
import sys
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from openai import OpenAI
import pickle
from datetime import datetime

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    text: str
    score: float
    metadata: Dict
    index: int

class SemanticSearchEngine:
    """è¯­ä¹‰æœç´¢å¼•æ“ç±»"""
    
    def __init__(self, api_key: str = None, index_file: str = "semantic_index.pkl"):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        try:
            self.client = OpenAI(
                api_key=api_key or os.getenv("DASHSCOPE_API_KEY"),
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            self.model = "text-embedding-v4"
            self.dimensions = 1024
            self.index_file = index_file
            self.documents = []
            self.embeddings = []
            self.metadata = []
            print("âœ… è¯­ä¹‰æœç´¢å¼•æ“åˆå§‹åŒ–æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            sys.exit(1)
    
    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬åµŒå…¥"""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=[text],
                dimensions=self.dimensions,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ è·å–åµŒå…¥å¤±è´¥: {e}")
            return []
    
    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡è·å–æ–‡æœ¬åµŒå…¥"""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=texts,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            print(f"âŒ æ‰¹é‡è·å–å¤±è´¥: {e}")
            return []
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    def build_index(self, documents: List[Dict]) -> bool:
        """æ„å»ºæœç´¢ç´¢å¼•"""
        print("ğŸ¯ æ„å»ºæœç´¢ç´¢å¼•...")
        print("=" * 50)
        
        if not documents:
            print("âŒ æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
            return False
        
        try:
            # æå–æ–‡æœ¬å’Œå…ƒæ•°æ®
            texts = [doc['text'] for doc in documents]
            self.metadata = [doc.get('metadata', {}) for doc in documents]
            
            # è·å–åµŒå…¥
            print(f"ğŸ“Š æ­£åœ¨å¤„ç† {len(texts)} ä¸ªæ–‡æ¡£...")
            self.embeddings = self.get_embeddings_batch(texts)
            self.documents = texts
            
            if not self.embeddings:
                print("âŒ è·å–åµŒå…¥å¤±è´¥")
                return False
            
            # ä¿å­˜ç´¢å¼•
            self.save_index()
            
            print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
            print(f"   æ–‡æ¡£æ•°é‡: {len(self.documents)}")
            print(f"   å‘é‡ç»´åº¦: {len(self.embeddings[0])}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ„å»ºç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def search(self, query: str, top_k: int = 5, threshold: float = 0.3) -> List[SearchResult]:
        """æ‰§è¡Œè¯­ä¹‰æœç´¢"""
        if not self.embeddings:
            print("âŒ æœç´¢ç´¢å¼•ä¸ºç©º")
            return []
        
        try:
            # è·å–æŸ¥è¯¢åµŒå…¥
            query_embedding = self.get_embedding(query)
            if not query_embedding:
                return []
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for i, (embedding, doc) in enumerate(zip(self.embeddings, self.documents)):
                score = self.cosine_similarity(query_embedding, embedding)
                if score >= threshold:
                    similarities.append((i, score))
            
            # æ’åºå¹¶è¿”å›ç»“æœ
            similarities.sort(key=lambda x: x[1], reverse=True)
            top_results = similarities[:top_k]
            
            results = []
            for idx, score in top_results:
                result = SearchResult(
                    text=self.documents[idx],
                    score=score,
                    metadata=self.metadata[idx],
                    index=idx
                )
                results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        try:
            index_data = {
                'documents': self.documents,
                'embeddings': self.embeddings,
                'metadata': self.metadata,
                'created_at': datetime.now().isoformat()
            }
            
            with open(self.index_file, 'wb') as f:
                pickle.dump(index_data, f)
            
            print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ° {self.index_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def load_index(self) -> bool:
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        try:
            if not os.path.exists(self.index_file):
                print("âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            with open(self.index_file, 'rb') as f:
                index_data = pickle.load(f)
            
            self.documents = index_data['documents']
            self.embeddings = index_data['embeddings']
            self.metadata = index_data['metadata']
            
            print(f"âœ… ç´¢å¼•åŠ è½½æˆåŠŸï¼")
            print(f"   æ–‡æ¡£æ•°é‡: {len(self.documents)}")
            print(f"   åˆ›å»ºæ—¶é—´: {index_data['created_at']}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def add_documents(self, new_documents: List[Dict]) -> bool:
        """æ·»åŠ æ–°æ–‡æ¡£åˆ°ç´¢å¼•"""
        try:
            if not new_documents:
                return True
            
            # è·å–æ–°æ–‡æ¡£çš„åµŒå…¥
            texts = [doc['text'] for doc in new_documents]
            new_embeddings = self.get_embeddings_batch(texts)
            new_metadata = [doc.get('metadata', {}) for doc in new_documents]
            
            if not new_embeddings:
                return False
            
            # æ·»åŠ åˆ°ç°æœ‰ç´¢å¼•
            self.documents.extend(texts)
            self.embeddings.extend(new_embeddings)
            self.metadata.extend(new_metadata)
            
            # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
            self.save_index()
            
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(new_documents)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def get_search_suggestions(self, query: str, max_suggestions: int = 5) -> List[str]:
        """è·å–æœç´¢å»ºè®®"""
        if not self.documents:
            return []
        
        # ç®€å•çš„æœç´¢å»ºè®®ï¼šåŸºäºæ–‡æ¡£æ–‡æœ¬çš„å‰ç¼€åŒ¹é…
        suggestions = []
        query_lower = query.lower()
        
        for doc in self.documents:
            if query_lower in doc.lower():
                # æå–åŒ¹é…çš„çŸ­è¯­
                words = doc.split()
                for i in range(len(words)):
                    phrase = ' '.join(words[i:i+3])  # 3-gram
                    if query_lower in phrase.lower() and phrase not in suggestions:
                        suggestions.append(phrase)
                        if len(suggestions) >= max_suggestions:
                            break
                if len(suggestions) >= max_suggestions:
                    break
        
        return suggestions[:max_suggestions]

def demo_product_search():
    """æ¼”ç¤ºäº§å“æœç´¢"""
    print("ğŸ¯ äº§å“æœç´¢æ¼”ç¤º")
    print("=" * 50)
    
    # ç¤ºä¾‹äº§å“æ•°æ®
    products = [
        {
            "text": "iPhone 15 Pro Max 256GB åŸè‰²é’›é‡‘å± 5Gæ‰‹æœº",
            "metadata": {
                "category": "æ‰‹æœº",
                "brand": "Apple",
                "price": 9999,
                "rating": 4.8
            }
        },
        {
            "text": "åä¸ºMate 60 Pro 12GB+512GB é›…é»‘ å«æ˜Ÿé€šä¿¡",
            "metadata": {
                "category": "æ‰‹æœº",
                "brand": "Huawei",
                "price": 6999,
                "rating": 4.7
            }
        },
        {
            "text": "å°ç±³14 Ultra 16GB+1TB é’›é‡‘å±ç‰ˆ å¾•å¡å½±åƒ",
            "metadata": {
                "category": "æ‰‹æœº",
                "brand": "Xiaomi",
                "price": 5999,
                "rating": 4.6
            }
        },
        {
            "text": "MacBook Pro M3 14è‹±å¯¸ 18GB+512GB æ·±ç©ºé»‘",
            "metadata": {
                "category": "ç¬”è®°æœ¬",
                "brand": "Apple",
                "price": 14999,
                "rating": 4.9
            }
        },
        {
            "text": "æˆ´æ£®V12æ— çº¿å¸å°˜å™¨ æ‰‹æŒé™¤è¨ æ¿€å…‰æ¢æµ‹",
            "metadata": {
                "category": "å®¶ç”µ",
                "brand": "Dyson",
                "price": 3999,
                "rating": 4.5
            }
        },
        {
            "text": "ç´¢å°¼WH-1000XM5 é™å™ªè€³æœº æ— çº¿è“ç‰™",
            "metadata": {
                "category": "è€³æœº",
                "brand": "Sony",
                "price": 2499,
                "rating": 4.7
            }
        }
    ]
    
    # åˆ›å»ºæœç´¢å¼•æ“
    engine = SemanticSearchEngine()
    
    # æ„å»ºç´¢å¼•
    if engine.build_index(products):
        # æ‰§è¡Œæœç´¢
        queries = [
            "æœ€å¥½çš„æ‹ç…§æ‰‹æœº",
            "è‹¹æœçš„äº§å“",
            "æ— çº¿è€³æœº",
            "åŠå…¬ç”¨çš„ç¬”è®°æœ¬ç”µè„‘",
            "æ€§ä»·æ¯”é«˜çš„æ‰‹æœº"
        ]
        
        for query in queries:
            print(f"\nğŸ” æœç´¢: '{query}'")
            results = engine.search(query, top_k=3)
            
            if results:
                for i, result in enumerate(results, 1):
                    print(f"   {i}. {result.text}")
                    print(f"      ç›¸ä¼¼åº¦: {result.score:.3f}")
                    print(f"      å“ç‰Œ: {result.metadata['brand']}")
                    print(f"      ä»·æ ¼: Â¥{result.metadata['price']}")
                    print(f"      è¯„åˆ†: {result.metadata['rating']}")
            else:
                print("   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")

def demo_document_search():
    """æ¼”ç¤ºæ–‡æ¡£æœç´¢"""
    print("\nğŸ¯ æ–‡æ¡£æœç´¢æ¼”ç¤º")
    print("=" * 50)
    
    # ç¤ºä¾‹æ–‡æ¡£æ•°æ®
    documents = [
        {
            "text": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ",
            "metadata": {
                "category": "æŠ€æœ¯",
                "tags": ["AI", "æœºå™¨å­¦ä¹ "],
                "author": "å¼ æ•™æˆ",
                "date": "2024-01-15"
            }
        },
        {
            "text": "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚é—®é¢˜ï¼Œéœ€è¦å¤§é‡è®¡ç®—èµ„æº",
            "metadata": {
                "category": "æŠ€æœ¯",
                "tags": ["AI", "æ·±åº¦å­¦ä¹ "],
                "author": "æåšå£«",
                "date": "2024-01-20"
            }
        },
        {
            "text": "è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œå¤„ç†äººç±»è¯­è¨€",
            "metadata": {
                "category": "æŠ€æœ¯",
                "tags": ["AI", "NLP"],
                "author": "ç‹ç ”ç©¶å‘˜",
                "date": "2024-01-25"
            }
        },
        {
            "text": "Pythonæ˜¯æœ€æµè¡Œçš„æ•°æ®ç§‘å­¦ç¼–ç¨‹è¯­è¨€ï¼Œæœ‰ä¸°å¯Œçš„æœºå™¨å­¦ä¹ åº“",
            "metadata": {
                "category": "ç¼–ç¨‹",
                "tags": ["Python", "æ•°æ®ç§‘å­¦"],
                "author": "é™ˆå·¥ç¨‹å¸ˆ",
                "date": "2024-02-01"
            }
        },
        {
            "text": "è®¡ç®—æœºè§†è§‰å¯ä»¥è¯†åˆ«å’Œåˆ†æå›¾åƒå†…å®¹ï¼Œåº”ç”¨äºäººè„¸è¯†åˆ«å’Œè‡ªåŠ¨é©¾é©¶",
            "metadata": {
                "category": "æŠ€æœ¯",
                "tags": ["AI", "è®¡ç®—æœºè§†è§‰"],
                "author": "èµµä¸“å®¶",
                "date": "2024-02-05"
            }
        }
    ]
    
    # åˆ›å»ºæœç´¢å¼•æ“
    engine = SemanticSearchEngine(index_file="document_index.pkl")
    
    # æ„å»ºç´¢å¼•
    if engine.build_index(documents):
        # æ‰§è¡Œæ–‡æ¡£æœç´¢
        queries = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ä¹ éœ€è¦å“ªäº›èµ„æº",
            "Pythonåœ¨æ•°æ®ç§‘å­¦ä¸­çš„åº”ç”¨",
            "è®¡ç®—æœºè§†è§‰çš„å®é™…åº”ç”¨",
            "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯"
        ]
        
        for query in queries:
            print(f"\nğŸ” æ–‡æ¡£æœç´¢: '{query}'")
            results = engine.search(query, top_k=2)
            
            if results:
                for i, result in enumerate(results, 1):
                    print(f"   {i}. {result.text}")
                    print(f"      ç›¸ä¼¼åº¦: {result.score:.3f}")
                    print(f"      ä½œè€…: {result.metadata['author']}")
                    print(f"      æ—¥æœŸ: {result.metadata['date']}")
            else:
                print("   æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

def demo_search_suggestions():
    """æ¼”ç¤ºæœç´¢å»ºè®®"""
    print("\nğŸ¯ æœç´¢å»ºè®®æ¼”ç¤º")
    print("=" * 50)
    
    # ä½¿ç”¨äº§å“æœç´¢çš„ç´¢å¼•
    products = [
        {"text": "iPhone 15 Pro Max æ™ºèƒ½æ‰‹æœº", "metadata": {}},
        {"text": "åä¸ºMate 60 Pro å«æ˜Ÿé€šä¿¡æ‰‹æœº", "metadata": {}},
        {"text": "å°ç±³14 Ultra å¾•å¡å½±åƒæ‰‹æœº", "metadata": {}},
        {"text": "MacBook Pro M3 ç¬”è®°æœ¬ç”µè„‘", "metadata": {}},
        {"text": "æˆ´æ£®V12 æ— çº¿å¸å°˜å™¨", "metadata": {}}
    ]
    
    engine = SemanticSearchEngine()
    engine.build_index(products)
    
    # æµ‹è¯•æœç´¢å»ºè®®
    test_queries = ["iPhone", "åä¸º", "å°ç±³", "ç¬”è®°æœ¬", "å¸å°˜"]
    
    for query in test_queries:
        suggestions = engine.get_search_suggestions(query)
        print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
        print(f"   æœç´¢å»ºè®®: {suggestions}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸­çº§è¯¾ç¨‹ç¬¬1è¯¾ï¼šè¯­ä¹‰æœç´¢ç³»ç»Ÿ")
    print("=" * 60)
    print("æœ¬è¯¾ç¨‹å°†æ•™ä½ æ„å»ºä¸€ä¸ªå®Œæ•´çš„è¯­ä¹‰æœç´¢ç³»ç»Ÿã€‚\n")
    
    try:
        # æ£€æŸ¥APIå¯†é’¥
        if not os.getenv("DASHSCOPE_API_KEY"):
            print("ğŸ”‘ APIå¯†é’¥æ£€æŸ¥")
            print("-" * 30)
            print("âš ï¸ æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
            print("\nè§£å†³æ–¹æ³•ï¼š")
            print("1. ä¸´æ—¶è®¾ç½®: set DASHSCOPE_API_KEY=ä½ çš„å¯†é’¥ (Windows)")
            print("2. ä¸´æ—¶è®¾ç½®: export DASHSCOPE_API_KEY=ä½ çš„å¯†é’¥ (Linux/Mac)")
            print("\nğŸ“ è·å–APIå¯†é’¥ï¼š")
            print("   è®¿é—® https://dashscope.console.aliyun.com ç”³è¯·")
            return
        else:
            print("âœ… æ£€æµ‹åˆ°APIå¯†é’¥")
        
        input("\nğŸ›’ æŒ‰å›è½¦é”®å¼€å§‹äº§å“æœç´¢æ¼”ç¤º...")
        print("\n" + "="*60)
        demo_product_search()
        
        input("\nğŸ“„ æŒ‰å›è½¦é”®å¼€å§‹æ–‡æ¡£æœç´¢æ¼”ç¤º...")
        print("\n" + "="*60)
        demo_document_search()
        
        input("\nğŸ’¡ æŒ‰å›è½¦é”®æŸ¥çœ‹æœç´¢å»ºè®®åŠŸèƒ½...")
        print("\n" + "="*60)
        demo_search_suggestions()
        
        print("\n" + "="*60)
        print("ğŸ‰ è¯­ä¹‰æœç´¢è¯¾ç¨‹å®Œæˆï¼")
        print("ğŸ¯ ä½ å·²ç»å­¦ä¼šäº†ï¼š")
        print("âœ… æ„å»ºæœç´¢ç´¢å¼•")
        print("âœ… å®ç°è¯­ä¹‰æœç´¢")
        print("âœ… æœç´¢ç»“æœæ’åº")
        print("âœ… æ·»åŠ æœç´¢å»ºè®®")
        print("âœ… ç´¢å¼•æŒä¹…åŒ–")
        print("\nğŸ“‚ ç´¢å¼•æ–‡ä»¶å·²ä¿å­˜ä¸º .pkl æ–‡ä»¶")
        print("\nğŸš€ å‡†å¤‡è¿›å…¥ä¸‹ä¸€è¯¾ç¨‹...")
        print("\nä¸­çº§æ¨¡å—ï¼š02-text-classification.py - æ–‡æœ¬åˆ†ç±»")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è¯¾ç¨‹å·²ä¸­æ–­ï¼Œæ¬¢è¿ä¸‹æ¬¡ç»§ç»­å­¦ä¹ ï¼")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œé”™è¯¯: {e}")
        print("ğŸ”„ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")
    finally:
        input("\nğŸ“š æŒ‰å›è½¦é”®é€€å‡ºè¯¾ç¨‹...")

if __name__ == "__main__":
    main()