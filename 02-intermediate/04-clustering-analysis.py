#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­çº§è¯¾ç¨‹ç¬¬4è¯¾ï¼šæ–‡æœ¬èšç±»åˆ†æ
=======================

åŸºäºæ–‡æœ¬åµŒå…¥çš„èšç±»åˆ†æç³»ç»Ÿå®ç°ã€‚
é€šè¿‡å‘é‡åŒ–æŠ€æœ¯å®ç°æ–‡æœ¬æ•°æ®çš„æ— ç›‘ç£èšç±»å’Œä¸»é¢˜å‘ç°ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£èšç±»åˆ†æçš„å·¥ä½œåŸç†
2. æŒæ¡K-meansã€å±‚æ¬¡èšç±»ã€DBSCANç®—æ³•
3. å®ç°æ–‡æœ¬ä¸»é¢˜å‘ç°
4. èšç±»ç»“æœè¯„ä¼°å’Œå¯è§†åŒ–
5. å¤„ç†å¤§è§„æ¨¡æ–‡æœ¬èšç±»
"""

import os
import sys
import numpy as np
from typing import List, Dict, Tuple
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score, adjusted_rand_score
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from collections import Counter

# æ·»åŠ utilsç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.embedding_client import EmbeddingClient

class TextClusteringSystem:
    """æ–‡æœ¬èšç±»åˆ†æç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–èšç±»ç³»ç»Ÿ"""
        self.client = EmbeddingClient()
        self.embeddings = []
        self.texts = []
        self.labels = []
        
    def load_sample_data(self) -> List[str]:
        """åŠ è½½ç¤ºä¾‹èšç±»æ•°æ®"""
        sample_texts = [
            # ç§‘æŠ€ç±»
            "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ”¹å˜åŒ»ç–—è¯Šæ–­æ–¹å¼ï¼Œæé«˜ç–¾ç—…æ£€æµ‹å‡†ç¡®ç‡",
            "æœºå™¨å­¦ä¹ ç®—æ³•åœ¨é‡‘èé£æ§ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œæ•ˆæœæ˜¾è‘—",
            "æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—é‡å¤§çªç ´ï¼Œå‡†ç¡®ç‡è¶…è¿‡äººç±»",
            "é‡å­è®¡ç®—æœºç ”ç©¶å–å¾—æ–°è¿›å±•ï¼Œæœ‰æœ›è§£å†³ä¼ ç»Ÿè®¡ç®—æœºæ— æ³•å¤„ç†çš„é—®é¢˜",
            "5Gç½‘ç»œæŠ€æœ¯æ¨åŠ¨ç‰©è”ç½‘åº”ç”¨å¿«é€Ÿå‘å±•ï¼Œè¿æ¥è®¾å¤‡æ•°é‡æ¿€å¢",
            
            # ä½“è‚²ç±»
            "å›½è¶³åœ¨ä¸–ç•Œæ¯é¢„é€‰èµ›ä¸­è¡¨ç°å‡ºè‰²ï¼Œçƒè¿·çƒ­æƒ…é«˜æ¶¨æœŸå¾…æ™‹çº§",
            "NBAæ€»å†³èµ›å³å°†æ‰“å“ï¼Œæ¹–äººé˜Ÿå’Œå‡¯å°”ç‰¹äººé˜Ÿäº‰å¤ºæ€»å† å†›",
            "ä¸­å›½å¥³æ’åœ¨ä¸–ç•Œé”¦æ ‡èµ›ä¸­è·å¾—é‡‘ç‰Œï¼Œå±•ç°å¼ºå¤§å®åŠ›èµ¢å¾—å°Šé‡",
            "è¶³çƒä¸–ç•Œæ¯å³å°†å¼€å¹•ï¼Œå„æ”¯çƒé˜Ÿç§¯æå¤‡æˆ˜äº‰å–å¥½æˆç»©",
            "å¥¥è¿ä¼šç­¹å¤‡å·¥ä½œè¿›å±•é¡ºåˆ©ï¼Œåœºé¦†å»ºè®¾å®ŒæˆæœŸå¾…è¿åŠ¨å‘˜ç²¾å½©è¡¨ç°",
            
            # è´¢ç»ç±»
            "å¤®è¡Œå®£å¸ƒé™æ¯æ”¿ç­–ï¼Œåˆºæ¿€ç»æµå¢é•¿åº”å¯¹å¸‚åœºå˜åŒ–",
            "è‚¡ç¥¨å¸‚åœºä»Šæ—¥å¤§æ¶¨ï¼Œç§‘æŠ€è‚¡é¢†æ¶¨å¤§ç›˜æŠ•èµ„è€…ä¿¡å¿ƒå¢å¼º",
            "æˆ¿åœ°äº§å¸‚åœºè°ƒæ§æ”¿ç­–æ•ˆæœæ˜¾è‘—ï¼Œæˆ¿ä»·è¶‹äºç¨³å®šå¸‚åœºé¢„æœŸæ”¹å–„",
            "å›½é™…è´¸æ˜“åˆä½œåŠ å¼ºï¼ŒåŒè¾¹è´¸æ˜“é¢åˆ›æ–°é«˜ä¿ƒè¿›ç»æµå‘å±•",
            "æ•°å­—è´§å¸è¯•ç‚¹é¡¹ç›®è¿›å±•é¡ºåˆ©ï¼Œä¸ºé‡‘èç§‘æŠ€åˆ›æ–°æä¾›æ–°æœºé‡",
            
            # å¨±ä¹ç±»
            "ç”µå½±ã€Šæµæµªåœ°çƒ3ã€‹ç¥¨æˆ¿çªç ´10äº¿ï¼Œåˆ›å½±å²çºªå½•è§‚ä¼—å¥½è¯„å¦‚æ½®",
            "æŸçŸ¥åæ­Œæ‰‹å‘å¸ƒæ–°ä¸“è¾‘ï¼ŒéŸ³ä¹é£æ ¼åˆ›æ–°å¤§å—å¥½è¯„é”€é‡é¢†å…ˆ",
            "ç”µè§†å‰§ã€Šä¸‰ä½“ã€‹è·å¾—è§‚ä¼—ä¸€è‡´å¥½è¯„ï¼Œç§‘å¹»é¢˜æå—æ¬¢è¿è®¨è®ºçƒ­çƒˆ",
            "ç»¼è‰ºèŠ‚ç›®åˆ›æ–°å½¢å¼å¸å¼•å¤§é‡å¹´è½»è§‚ä¼—ï¼Œæ”¶è§†ç‡å’Œå£ç¢‘åŒä¸°æ”¶",
            "æŸæ˜æ˜Ÿæ…ˆå–„æ´»åŠ¨è·å¾—å¹¿æ³›å…³æ³¨ï¼Œæ­£èƒ½é‡ä¼ æ’­ç¤¾ä¼šå½±å“åŠ›å¤§"
        ]
        return sample_texts
    
    def prepare_embeddings(self, texts: List[str]) -> np.ndarray:
        """å‡†å¤‡æ–‡æœ¬åµŒå…¥"""
        print("ğŸ¯ å‡†å¤‡æ–‡æœ¬åµŒå…¥...")
        
        embeddings = []
        for text in texts:
            embedding = self.client.get_embedding(text)
            embeddings.append(embedding)
        
        embeddings_array = np.array(embeddings)
        self.embeddings = embeddings_array
        self.texts = texts
        
        print(f"âœ… å·²å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬ï¼ŒåµŒå…¥ç»´åº¦: {embeddings_array.shape}")
        return embeddings_array
    
    def kmeans_clustering(self, embeddings: np.ndarray, n_clusters: int = 4) -> Tuple[np.ndarray, Dict]:
        """K-meansèšç±»"""
        print(f"\nğŸ¯ K-meansèšç±» (k={n_clusters})")
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings)
        
        # è®¡ç®—è½®å»“ç³»æ•°
        silhouette_avg = silhouette_score(embeddings, labels)
        
        # è·å–èšç±»ä¸­å¿ƒ
        cluster_centers = kmeans.cluster_centers_
        
        results = {
            'algorithm': 'K-means',
            'labels': labels,
            'silhouette_score': silhouette_avg,
            'cluster_centers': cluster_centers,
            'inertia': kmeans.inertia_
        }
        
        return labels, results
    
    def hierarchical_clustering(self, embeddings: np.ndarray, n_clusters: int = 4) -> Tuple[np.ndarray, Dict]:
        """å±‚æ¬¡èšç±»"""
        print(f"\nğŸ¯ å±‚æ¬¡èšç±» (k={n_clusters})")
        
        hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
        labels = hierarchical.fit_predict(embeddings)
        
        # è®¡ç®—è½®å»“ç³»æ•°
        silhouette_avg = silhouette_score(embeddings, labels)
        
        results = {
            'algorithm': 'Hierarchical',
            'labels': labels,
            'silhouette_score': silhouette_avg,
            'linkage': 'ward'
        }
        
        return labels, results
    
    def dbscan_clustering(self, embeddings: np.ndarray, eps: float = 0.5, min_samples: int = 3) -> Tuple[np.ndarray, Dict]:
        """DBSCANèšç±»"""
        print(f"\nğŸ¯ DBSCANèšç±» (eps={eps}, min_samples={min_samples})")
        
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(embeddings)
        
        # è®¡ç®—æœ‰æ•ˆèšç±»çš„è½®å»“ç³»æ•°ï¼ˆæ’é™¤å™ªå£°ç‚¹ï¼‰
        mask = labels != -1
        if np.sum(mask) > 1:
            silhouette_avg = silhouette_score(embeddings[mask], labels[mask])
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        else:
            silhouette_avg = -1
            n_clusters = 0
        
        results = {
            'algorithm': 'DBSCAN',
            'labels': labels,
            'silhouette_score': silhouette_avg,
            'n_clusters': n_clusters,
            'n_noise': np.sum(labels == -1)
        }
        
        return labels, results
    
    def analyze_clusters(self, texts: List[str], labels: np.ndarray, algorithm_name: str) -> Dict:
        """åˆ†æèšç±»ç»“æœ"""
        print(f"\nğŸ“Š {algorithm_name} èšç±»ç»“æœåˆ†æ:")
        
        cluster_info = {}
        unique_labels = set(labels)
        
        for label in unique_labels:
            if label == -1:  # å™ªå£°ç‚¹
                continue
                
            cluster_indices = np.where(labels == label)[0]
            cluster_texts = [texts[i] for i in cluster_indices]
            
            # è®¡ç®—èšç±»å¤§å°
            cluster_size = len(cluster_texts)
            
            # æå–å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
            keywords = self.extract_keywords(cluster_texts)
            
            cluster_info[f"èšç±»_{label}"] = {
                'size': cluster_size,
                'texts': cluster_texts[:3],  # æ˜¾ç¤ºå‰3ä¸ªæ–‡æœ¬
                'keywords': keywords[:5],   # æ˜¾ç¤ºå‰5ä¸ªå…³é”®è¯
                'percentage': cluster_size / len(texts) * 100
            }
        
        return cluster_info
    
    def extract_keywords(self, texts: List[str]) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        combined_text = ' '.join(texts)
        
        # ç®€å•çš„å…³é”®è¯æå–
        words = combined_text.replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ã€', ' ').split()
        word_counts = Counter(words)
        
        # è¿‡æ»¤æ‰å¸¸è§è¯ï¼Œè¿”å›é«˜é¢‘è¯
        common_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'ä¸º', 'å¯¹', 'ä¸­', 'ä¸Š', 'ä¸‹', 'è¿™', 'é‚£'}
        keywords = [word for word, count in word_counts.most_common() 
                   if word not in common_words and len(word) > 1][:10]
        
        return keywords
    
    def visualize_clusters(self, embeddings: np.ndarray, labels: np.ndarray, texts: List[str], 
                          algorithm_name: str):
        """å¯è§†åŒ–èšç±»ç»“æœ"""
        # è®¾ç½®matplotlibä»¥æ”¯æŒä¸­æ–‡æ˜¾ç¤º
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # é™ç»´åˆ°2D
        pca = PCA(n_components=2)
        embeddings_2d = pca.fit_transform(embeddings)
        
        # åˆ›å»ºå›¾å½¢
        plt.figure(figsize=(12, 8))
        
        # è·å–å”¯ä¸€æ ‡ç­¾
        unique_labels = set(labels)
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
        
        for i, label in enumerate(unique_labels):
            mask = labels == label
            if label == -1:  # å™ªå£°ç‚¹
                plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], 
                          color='gray', marker='x', s=50, label='å™ªå£°ç‚¹')
            else:
                plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], 
                          color=colors[i % len(colors)], alpha=0.7, label=f'èšç±»{label}')
        
        # æ·»åŠ æ–‡æœ¬æ ‡ç­¾ï¼ˆæ˜¾ç¤ºéƒ¨åˆ†æ–‡æœ¬ï¼‰
        for i, (x, y) in enumerate(embeddings_2d):
            if labels[i] != -1:  # ä¸ä¸ºå™ªå£°ç‚¹
                plt.annotate(f"{i}", (x, y), fontsize=8, alpha=0.7)
        
        plt.title(f"{algorithm_name}èšç±»ç»“æœå¯è§†åŒ–", fontsize=14)
        plt.xlabel("ä¸»æˆåˆ†1", fontsize=12)
        plt.ylabel("ä¸»æˆåˆ†2", fontsize=12)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜å›¾ç‰‡
        filename = f"02-intermediate/clustering_{algorithm_name.lower()}_visualization.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜: {filename}")
    
    def compare_algorithms(self, embeddings: np.ndarray) -> Dict:
        """æ¯”è¾ƒä¸åŒèšç±»ç®—æ³•"""
        print("\nğŸ¯ èšç±»ç®—æ³•æ¯”è¾ƒ")
        print("=" * 50)
        
        algorithms = {
            'K-means': self.kmeans_clustering,
            'Hierarchical': self.hierarchical_clustering,
            'DBSCAN': lambda emb: self.dbscan_clustering(emb, eps=0.6, min_samples=3)
        }
        
        comparison_results = {}
        
        for name, algorithm in algorithms.items():
            try:
                labels, results = algorithm(embeddings)
                comparison_results[name] = results
                
                # åˆ†æèšç±»ç»“æœ
                cluster_info = self.analyze_clusters(self.texts, labels, name)
                
                print(f"\n{name}:")
                print(f"   è½®å»“ç³»æ•°: {results.get('silhouette_score', 'N/A'):.3f}")
                if 'n_clusters' in results:
                    print(f"   èšç±»æ•°é‡: {results['n_clusters']}")
                if 'n_noise' in results:
                    print(f"   å™ªå£°ç‚¹æ•°é‡: {results['n_noise']}")
                
                # å¯è§†åŒ–
                self.visualize_clusters(embeddings, labels, self.texts, name)
                
            except Exception as e:
                print(f"   âŒ {name} æ‰§è¡Œå¤±è´¥: {e}")
        
        return comparison_results
    
    def find_optimal_clusters(self, embeddings: np.ndarray, max_k: int = 8) -> Dict:
        """å¯»æ‰¾æœ€ä¼˜èšç±»æ•°"""
        print("\nğŸ¯ å¯»æ‰¾æœ€ä¼˜èšç±»æ•°")
        print("=" * 50)
        
        silhouette_scores = []
        inertias = []
        
        for k in range(2, max_k + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(embeddings)
            
            score = silhouette_score(embeddings, labels)
            silhouette_scores.append(score)
            inertias.append(kmeans.inertia_)
            
            print(f"   k={k}: è½®å»“ç³»æ•°={score:.3f}, æƒ¯æ€§={kmeans.inertia_:.1f}")
        
        # æ‰¾åˆ°æœ€ä¼˜kå€¼
        optimal_k = np.argmax(silhouette_scores) + 2
        
        return {
            'silhouette_scores': silhouette_scores,
            'inertias': inertias,
            'optimal_k': optimal_k,
            'max_silhouette_score': max(silhouette_scores)
        }
    
    def demo_clustering_system(self):
        """æ¼”ç¤ºèšç±»ç³»ç»Ÿ"""
        print("ğŸš€ æ–‡æœ¬èšç±»åˆ†ææ¼”ç¤º")
        print("=" * 60)
        
        # åŠ è½½ç¤ºä¾‹æ•°æ®
        texts = self.load_sample_data()
        print(f"ğŸ“Š å·²åŠ è½½ {len(texts)} ä¸ªç¤ºä¾‹æ–‡æœ¬")
        
        # å‡†å¤‡åµŒå…¥
        embeddings = self.prepare_embeddings(texts)
        
        # å¯»æ‰¾æœ€ä¼˜èšç±»æ•°
        optimal_info = self.find_optimal_clusters(embeddings)
        print(f"\nğŸ“ˆ æœ€ä¼˜èšç±»æ•°: {optimal_info['optimal_k']}")
        print(f"   æœ€é«˜è½®å»“ç³»æ•°: {optimal_info['max_silhouette_score']:.3f}")
        
        # ä½¿ç”¨æœ€ä¼˜èšç±»æ•°è¿›è¡ŒK-meansèšç±»
        print(f"\nğŸ¯ ä½¿ç”¨æœ€ä¼˜èšç±»æ•° k={optimal_info['optimal_k']} è¿›è¡Œèšç±»")
        labels, results = self.kmeans_clustering(embeddings, optimal_info['optimal_k'])
        
        # åˆ†æèšç±»ç»“æœ
        cluster_info = self.analyze_clusters(texts, labels, "æœ€ä¼˜K-means")
        
        print("\nğŸ“Š èšç±»ç»“æœ:")
        for cluster_name, info in cluster_info.items():
            print(f"\n{cluster_name}:")
            print(f"   å¤§å°: {info['size']} ({info['percentage']:.1f}%)")
            print(f"   å…³é”®è¯: {', '.join(info['keywords'])}")
            print(f"   ç¤ºä¾‹æ–‡æœ¬: {info['texts'][0][:50]}...")
        
        # æ¯”è¾ƒä¸åŒç®—æ³•
        print("\nğŸ¯ ç¬¬5éƒ¨åˆ†ï¼šç®—æ³•æ¯”è¾ƒ")
        comparison_results = self.compare_algorithms(embeddings)
        
        print("\nğŸ‰ èšç±»åˆ†ææ¼”ç¤ºå®Œæˆï¼")
        print("\næ ¸å¿ƒæŠ€æœ¯æ€»ç»“:")
        print("   â€¢ K-meansèšç±»")
        print("   â€¢ å±‚æ¬¡èšç±»")
        print("   â€¢ DBSCANèšç±»")
        print("   â€¢ è½®å»“ç³»æ•°è¯„ä¼°")
        print("   â€¢ èšç±»å¯è§†åŒ–")
        print("\nå®é™…åº”ç”¨åœºæ™¯:")
        print("   â€¢ æ–‡æ¡£ä¸»é¢˜å‘ç°")
        print("   â€¢ æ–°é—»èšç±»")
        print("   â€¢ å®¢æˆ·åé¦ˆåˆ†æ")
        print("   â€¢ ç¤¾äº¤åª’ä½“å†…å®¹åˆ†ç»„")
        print("   â€¢ æ¨èç³»ç»Ÿç”¨æˆ·åˆ†ç¾¤")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸­çº§è¯¾ç¨‹ç¬¬4è¯¾ï¼šæ–‡æœ¬èšç±»åˆ†æ")
    print("=" * 60)
    print("åŸºäºæ–‡æœ¬åµŒå…¥çš„èšç±»åˆ†æç³»ç»Ÿå®ç°ã€‚\n")
    
    try:
        # æ£€æŸ¥APIå¯†é’¥
        if not os.getenv("DASHSCOPE_API_KEY"):
            print("ğŸ”‘ APIå¯†é’¥æ£€æŸ¥")
            print("-" * 30)
            print("âš ï¸ æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
            print("\nè§£å†³æ–¹æ³•ï¼š")
            print("1. ä¸´æ—¶è®¾ç½®: set DASHSCOPE_API_KEY=ä½ çš„å¯†é’¥ (Windows)")
            print("2. ä¸´æ—¶è®¾ç½®: export DASHSCOPE_API_KEY=ä½ çš„å¯†é’¥ (Linux/Mac)")
            print("\nğŸ“ è·å–APIå¯†é’¥ï¼š")
            print("   è®¿é—® https://dashscope.console.aliyun.com ç”³è¯·")
            return
        else:
            print("âœ… æ£€æµ‹åˆ°APIå¯†é’¥")
        
        input("\nğŸ“Š æŒ‰å›è½¦é”®å¼€å§‹èšç±»åˆ†ææ¼”ç¤º...")
        print("\n" + "="*60)
        clustering_system = TextClusteringSystem()
        clustering_system.demo_clustering_system()
        
        print("\n" + "="*60)
        print("ğŸ‰ èšç±»åˆ†æè¯¾ç¨‹å®Œæˆï¼")
        print("ğŸ¯ ä½ å·²ç»æŒæ¡äº†ï¼š")
        print("âœ… K-meansèšç±»")
        print("âœ… å±‚æ¬¡èšç±»")
        print("âœ… DBSCANèšç±»")
        print("âœ… è½®å»“ç³»æ•°è¯„ä¼°")
        print("âœ… èšç±»å¯è§†åŒ–")
        print("\nğŸ“‚ å¯è§†åŒ–ç»“æœå·²ä¿å­˜ä¸º clustering_*.png")
        print("\nğŸš€ å®é™…åº”ç”¨åœºæ™¯:")
        print("   â€¢ æ–‡æ¡£ä¸»é¢˜å‘ç°")
        print("   â€¢ æ–°é—»èšç±»")
        print("   â€¢ å®¢æˆ·åé¦ˆåˆ†æ")
        print("   â€¢ ç¤¾äº¤åª’ä½“å†…å®¹åˆ†ç»„")
        print("   â€¢ æ¨èç³»ç»Ÿç”¨æˆ·åˆ†ç¾¤")
        print("\nğŸ“ æ­å–œä½ å®Œæˆäº†ä¸­çº§è¯¾ç¨‹ï¼")
        print("\nğŸ¯ å‡†å¤‡è¿›å…¥é«˜çº§æ¨¡å—...")
        print("\né«˜çº§æ¨¡å—ï¼š03-advanced/01-knowledge-base.py - æ™ºèƒ½çŸ¥è¯†åº“")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è¯¾ç¨‹å·²ä¸­æ–­ï¼Œæ¬¢è¿ä¸‹æ¬¡ç»§ç»­å­¦ä¹ ï¼")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œé”™è¯¯: {e}")
        print("ğŸ”„ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")
    finally:
        input("\nğŸ“š æŒ‰å›è½¦é”®é€€å‡ºè¯¾ç¨‹...")

if __name__ == "__main__":
    main()