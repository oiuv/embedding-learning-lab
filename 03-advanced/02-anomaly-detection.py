#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§åŠŸèƒ½ç¬¬2è¯¾ï¼šå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ
=========================

åŸºäºæ–‡æœ¬åµŒå…¥çš„å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿï¼Œç”¨äºè¯†åˆ«åƒåœ¾å†…å®¹ã€æ¬ºè¯ˆä¿¡æ¯ç­‰ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£å¼‚å¸¸æ£€æµ‹åŸç†
2. å®ç°åŸºäºåµŒå…¥çš„å¼‚å¸¸è¯†åˆ«
3. åƒåœ¾å†…å®¹è¿‡æ»¤ç³»ç»Ÿ
4. æ¬ºè¯ˆæ£€æµ‹ç®—æ³•
5. å®æ—¶ç›‘æ§ç³»ç»Ÿ
"""

import os
import sys
import numpy as np
from typing import List, Dict, Tuple
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
import matplotlib.pyplot as plt
from collections import defaultdict
import json

# æ·»åŠ utilsç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.embedding_client import EmbeddingClient

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
from matplotlib import rcParams
rcParams['font.family'] = ['sans-serif']
rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False

class AnomalyDetectionSystem:
    """å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ"""
        self.client = EmbeddingClient()
        self.normal_profiles = {}
        self.thresholds = {}
        self.detection_models = {}
        
    def load_sample_data(self) -> Dict[str, List[str]]:
        """åŠ è½½ç¤ºä¾‹æ•°æ®ï¼ˆæ­£å¸¸å’Œå¼‚å¸¸æ–‡æœ¬ï¼‰"""
        sample_data = {
            "æ­£å¸¸è¯„è®º": [
                "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œç‰©æµé€Ÿåº¦å¿«ï¼ŒåŒ…è£…å®Œå¥½",
                "å®¢æœæ€åº¦å¾ˆå¥½ï¼Œè§£å†³é—®é¢˜åŠæ—¶ï¼Œå¾ˆæ»¡æ„è¿™æ¬¡è´­ç‰©ä½“éªŒ",
                "ä»·æ ¼åˆç†ï¼Œç‰©æœ‰æ‰€å€¼ï¼Œä¼šå†æ¬¡è´­ä¹°æ¨èç»™å¤§å®¶",
                "å•†å“æè¿°å‡†ç¡®ï¼Œå’Œå›¾ç‰‡ä¸€è‡´ï¼Œæ²¡æœ‰è‰²å·®",
                "å‘è´§è¿…é€Ÿï¼Œç‰©æµè·Ÿè¸ªä¿¡æ¯å‡†ç¡®ï¼Œæ”¶è´§åŠæ—¶"
            ],
            "åƒåœ¾å¹¿å‘Š": [
                "ğŸ”¥ğŸ”¥ğŸ”¥é™æ—¶æŠ¢è´­ï¼ç‚¹å‡»é“¾æ¥è·å–ä¼˜æƒ ğŸ”¥ğŸ”¥ğŸ”¥",
                "åŠ æˆ‘å¾®ä¿¡ï¼šXXXXXï¼Œè·å–æ›´å¤šä¼˜æƒ ä¿¡æ¯",
                "ç‰¹ä»·å•†å“ï¼Œæ•°é‡æœ‰é™ï¼Œé€ŸæŠ¢ï¼è”ç³»QQï¼š123456",
                "ğŸ”¥è¶…å€¼ä¼˜æƒ ğŸ”¥ä¸è¦é”™è¿‡æœºä¼šğŸ”¥ç«‹å³è´­ä¹°ğŸ”¥",
                "ä¸“ä¸šåˆ·å•å›¢é˜Ÿï¼Œå®‰å…¨å¯é ï¼Œè¯·è”ç³»å®¢æœ"
            ],
            "è™šå‡è¯„è®º": [
                "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„äº§å“ï¼Œæˆ‘ä»æ¥æ²¡æœ‰ç”¨è¿‡è¿™ä¹ˆå¥½çš„ä¸œè¥¿",
                "ç»å¯¹å®Œç¾ï¼Œæ²¡æœ‰ä»»ä½•ç¼ºç‚¹ï¼Œ100%æ¨èè´­ä¹°",
                "å¤ªæ£’äº†å¤ªæ£’äº†å¤ªæ£’äº†é‡è¦çš„äº‹æƒ…è¯´ä¸‰é",
                "è¿™ä¸ªäº§å“æ”¹å˜äº†æˆ‘çš„ç”Ÿæ´»ï¼Œå¼ºçƒˆæ¨èç»™å¤§å®¶",
                "æˆ‘ä»æ¥æ²¡æœ‰è§è¿‡è¿™ä¹ˆå¥½çš„å•†å“ï¼Œå¿…é¡»ç»™äº”æ˜Ÿ"
            ],
            "æ¶æ„æ”»å‡»": [
                "åƒåœ¾äº§å“ï¼Œåƒä¸‡åˆ«ä¹°ï¼Œéª—å­å•†å®¶",
                "è´¨é‡å·®åˆ°æç‚¹ï¼Œå®Œå…¨æ˜¯è™šå‡å®£ä¼ ",
                "å®¢æœæ€åº¦æ¶åŠ£ï¼Œé—®é¢˜ä¸è§£å†³è¿˜éª‚äºº",
                "æ”¶åˆ°è´§å°±åäº†ï¼Œå•†å®¶ä¸å¤„ç†è¿˜æ¨å¸è´£ä»»",
                "æµªè´¹é’±ï¼Œåæ‚”è´­ä¹°ï¼Œå¤§å®¶åƒä¸‡åˆ«ä¸Šå½“"
            ],
            "æ­£å¸¸æŠ€æœ¯è®¨è®º": [
                "è¿™ä¸ªç®—æ³•çš„å®ç°æ€è·¯å¾ˆæ¸…æ™°ï¼Œä»£ç è´¨é‡ä¸é”™",
                "æ–‡æ¡£å†™å¾—å¾ˆè¯¦ç»†ï¼Œå¯¹ç†è§£é¡¹ç›®å¾ˆæœ‰å¸®åŠ©",
                "æ€§èƒ½æµ‹è¯•ç»“æœç¬¦åˆé¢„æœŸï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–",
                "APIè®¾è®¡åˆç†ï¼Œæ¥å£è°ƒç”¨ç®€å•æ–¹ä¾¿",
                "æµ‹è¯•è¦†ç›–ç‡å¾ˆé«˜ï¼Œä»£ç è´¨é‡æœ‰ä¿éšœ"
            ]
        }
        return sample_data
    
    def create_normal_profile(self, normal_texts: List[str], domain: str = "default") -> Dict:
        """åˆ›å»ºæ­£å¸¸æ–‡æœ¬ç‰¹å¾æ¡£æ¡ˆ"""
        print(f"ğŸ¯ åˆ›å»º{domain}é¢†åŸŸæ­£å¸¸æ–‡æœ¬æ¡£æ¡ˆ...")
        
        embeddings = []
        for text in normal_texts:
            embedding = self.client.get_embedding(text)
            embeddings.append(embedding)
        
        embeddings_array = np.array(embeddings)
        
        # è®¡ç®—æ­£å¸¸æ–‡æœ¬çš„ä¸­å¿ƒå‘é‡
        center_vector = np.mean(embeddings_array, axis=0)
        
        # è®¡ç®—æ¯ä¸ªæ–‡æœ¬ä¸ä¸­å¿ƒçš„è·ç¦»ä½œä¸ºé˜ˆå€¼åŸºç¡€
        distances = []
        for emb in embeddings_array:
            distance = np.linalg.norm(emb - center_vector)
            distances.append(distance)
        
        # è®¾ç½®å¼‚å¸¸é˜ˆå€¼ï¼ˆå‡å€¼ + 2æ ‡å‡†å·®ï¼‰
        mean_distance = np.mean(distances)
        std_distance = np.std(distances)
        threshold = mean_distance + 2 * std_distance
        
        self.normal_profiles[domain] = {
            'center_vector': center_vector,
            'mean_distance': mean_distance,
            'std_distance': std_distance,
            'threshold': threshold,
            'normal_count': len(normal_texts)
        }
        
        print(f"   âœ… é˜ˆå€¼è®¾å®š: {threshold:.3f}")
        return self.normal_profiles[domain]
    
    def distance_based_detection(self, texts: List[str], domain: str = "default") -> List[Dict]:
        """åŸºäºè·ç¦»çš„å¼‚å¸¸æ£€æµ‹"""
        if domain not in self.normal_profiles:
            raise ValueError(f"è¯·å…ˆä¸º{domain}é¢†åŸŸåˆ›å»ºæ­£å¸¸æ¡£æ¡ˆ")
        
        profile = self.normal_profiles[domain]
        center_vector = profile['center_vector']
        threshold = profile['threshold']
        
        results = []
        for text in texts:
            embedding = self.client.get_embedding(text)
            distance = np.linalg.norm(np.array(embedding) - center_vector)
            
            is_anomaly = distance > threshold
            confidence = min(distance / threshold, 1.0) if is_anomaly else 1.0 - distance / threshold
            
            results.append({
                'text': text,
                'distance': distance,
                'is_anomaly': is_anomaly,
                'confidence': confidence,
                'method': 'distance_based'
            })
        
        return results
    
    def isolation_forest_detection(self, texts: List[str]) -> List[Dict]:
        """Isolation Forestå¼‚å¸¸æ£€æµ‹"""
        print("ğŸ¯ Isolation Forestå¼‚å¸¸æ£€æµ‹...")
        
        embeddings = [self.client.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        # ä½¿ç”¨Isolation Forest
        isolation_forest = IsolationForest(contamination=0.1, random_state=42)
        predictions = isolation_forest.fit_predict(embeddings_array)
        scores = isolation_forest.decision_function(embeddings_array)
        
        results = []
        for i, (text, pred, score) in enumerate(zip(texts, predictions, scores)):
            is_anomaly = pred == -1  # -1è¡¨ç¤ºå¼‚å¸¸
            confidence = abs(score)  # åˆ†æ•°ç»å¯¹å€¼ä½œä¸ºç½®ä¿¡åº¦
            
            results.append({
                'text': text,
                'is_anomaly': is_anomaly,
                'confidence': confidence,
                'score': score,
                'method': 'isolation_forest'
            })
        
        return results
    
    def lof_detection(self, texts: List[str]) -> List[Dict]:
        """Local Outlier Factorå¼‚å¸¸æ£€æµ‹"""
        print("ğŸ¯ LOFå¼‚å¸¸æ£€æµ‹...")
        
        embeddings = [self.client.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        # ä½¿ç”¨LOF
        lof = LocalOutlierFactor(n_neighbors=5, contamination=0.1)
        predictions = lof.fit_predict(embeddings_array)
        scores = lof.negative_outlier_factor_
        
        results = []
        for i, (text, pred, score) in enumerate(zip(texts, predictions, scores)):
            is_anomaly = pred == -1
            confidence = abs(score)
            
            results.append({
                'text': text,
                'is_anomaly': is_anomaly,
                'confidence': confidence,
                'score': score,
                'method': 'lof'
            })
        
        return results
    
    def one_class_svm_detection(self, texts: List[str], normal_texts: List[str]) -> List[Dict]:
        """One-Class SVMå¼‚å¸¸æ£€æµ‹"""
        print("ğŸ¯ One-Class SVMå¼‚å¸¸æ£€æµ‹...")
        
        # ä½¿ç”¨æ­£å¸¸æ–‡æœ¬è®­ç»ƒæ¨¡å‹
        normal_embeddings = [self.client.get_embedding(text) for text in normal_texts]
        normal_array = np.array(normal_embeddings)
        
        # è®­ç»ƒOne-Class SVM
        svm = OneClassSVM(kernel='rbf', nu=0.1)
        svm.fit(normal_array)
        
        # æ£€æµ‹å¼‚å¸¸
        embeddings = [self.client.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        predictions = svm.predict(embeddings_array)
        scores = svm.decision_function(embeddings_array)
        
        results = []
        for i, (text, pred, score) in enumerate(zip(texts, predictions, scores)):
            is_anomaly = pred == -1
            confidence = abs(score)
            
            results.append({
                'text': text,
                'is_anomaly': is_anomaly,
                'confidence': confidence,
                'score': score,
                'method': 'one_class_svm'
            })
        
        return results
    
    def ensemble_detection(self, texts: List[str], normal_texts: List[str], domain: str = "default") -> List[Dict]:
        """é›†æˆå¼‚å¸¸æ£€æµ‹"""
        print("ğŸ¯ é›†æˆå¼‚å¸¸æ£€æµ‹...")
        
        # è·å–æ‰€æœ‰æ£€æµ‹æ–¹æ³•çš„ç»“æœ
        distance_results = self.distance_based_detection(texts, domain)
        isolation_results = self.isolation_forest_detection(texts)
        lof_results = self.lof_detection(texts)
        svm_results = self.one_class_svm_detection(texts, normal_texts)
        
        # é›†æˆç»“æœ
        ensemble_results = []
        for i, text in enumerate(texts):
            # æ”¶é›†æ‰€æœ‰æ–¹æ³•çš„æ£€æµ‹ç»“æœ
            votes = []
            confidences = []
            
            for method_results in [distance_results, isolation_results, lof_results, svm_results]:
                vote = 1 if method_results[i]['is_anomaly'] else 0
                confidence = method_results[i]['confidence']
                votes.append(vote)
                confidences.append(confidence)
            
            # æŠ•ç¥¨ç»“æœ
            ensemble_vote = sum(votes) / len(votes)
            ensemble_confidence = np.mean(confidences)
            
            # å¤šæ•°æŠ•ç¥¨å†³å®šæœ€ç»ˆç»“æœ
            is_anomaly = ensemble_vote > 0.5
            
            ensemble_results.append({
                'text': text,
                'is_anomaly': is_anomaly,
                'confidence': ensemble_confidence,
                'vote_score': ensemble_vote,
                'individual_results': {
                    'distance': distance_results[i],
                    'isolation_forest': isolation_results[i],
                    'lof': lof_results[i],
                    'svm': svm_results[i]
                },
                'method': 'ensemble'
            })
        
        return ensemble_results
    
    def visualize_anomaly_detection(self, texts: List[str], results: List[Dict], method_name: str):
        """å¯è§†åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœ"""
        embeddings = [self.client.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        # é™ç»´å¯è§†åŒ–
        from sklearn.decomposition import PCA
        pca = PCA(n_components=2)
        embeddings_2d = pca.fit_transform(embeddings_array)
        
        plt.figure(figsize=(12, 8))
        
        # åˆ†ç¦»æ­£å¸¸å’Œå¼‚å¸¸ç‚¹
        normal_mask = [not r['is_anomaly'] for r in results]
        anomaly_mask = [r['is_anomaly'] for r in results]
        
        plt.scatter(embeddings_2d[normal_mask, 0], embeddings_2d[normal_mask, 1], 
                   c='blue', alpha=0.6, label='æ­£å¸¸æ–‡æœ¬', s=100)
        plt.scatter(embeddings_2d[anomaly_mask, 0], embeddings_2d[anomaly_mask, 1], 
                   c='red', alpha=0.8, label='å¼‚å¸¸æ–‡æœ¬', s=100, marker='^')
        
        # æ·»åŠ ç½®ä¿¡åº¦æ ‡ç­¾
        for i, (x, y) in enumerate(embeddings_2d):
            if results[i]['is_anomaly']:
                plt.annotate(f"{results[i]['confidence']:.2f}", 
                           (x, y), fontsize=8, color='red')
        
        plt.title(f"{method_name}å¼‚å¸¸æ£€æµ‹ç»“æœå¯è§†åŒ–", fontsize=14)
        plt.xlabel("ä¸»æˆåˆ†1", fontsize=12)
        plt.ylabel("ä¸»æˆåˆ†2", fontsize=12)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        filename = f"03-advanced/anomaly_{method_name.lower().replace(' ', '_')}_visualization.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜: {filename}")
    
    def real_time_monitoring(self, text_stream: List[str], normal_profile: Dict) -> List[Dict]:
        """å®æ—¶å¼‚å¸¸ç›‘æ§"""
        print("ğŸ¯ å®æ—¶å¼‚å¸¸ç›‘æ§...")
        
        alerts = []
        for i, text in enumerate(text_stream):
            # å¿«é€Ÿæ£€æµ‹
            distance_results = self.distance_based_detection([text], "default")
            
            if distance_results[0]['is_anomaly']:
                alerts.append({
                    'timestamp': datetime.now(),
                    'text': text,
                    'anomaly_type': 'real_time_detection',
                    'confidence': distance_results[0]['confidence'],
                    'severity': 'high' if distance_results[0]['confidence'] > 0.8 else 'medium'
                })
        
        return alerts
    
    def generate_detection_report(self, texts: List[str], results: List[Dict]) -> Dict:
        """ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š"""
        total_texts = len(texts)
        anomaly_count = sum(1 for r in results if r['is_anomaly'])
        normal_count = total_texts - anomaly_count
        
        # æŒ‰ç½®ä¿¡åº¦åˆ†ç±»
        high_confidence = [r for r in results if r['is_anomaly'] and r['confidence'] > 0.8]
        medium_confidence = [r for r in results if r['is_anomaly'] and 0.5 <= r['confidence'] <= 0.8]
        low_confidence = [r for r in results if r['is_anomaly'] and r['confidence'] < 0.5]
        
        report = {
            'total_texts': total_texts,
            'anomaly_count': anomaly_count,
            'normal_count': normal_count,
            'anomaly_rate': anomaly_count / total_texts if total_texts > 0 else 0,
            'high_confidence_anomalies': len(high_confidence),
            'medium_confidence_anomalies': len(medium_confidence),
            'low_confidence_anomalies': len(low_confidence),
            'anomaly_texts': [r['text'] for r in results if r['is_anomaly']],
            'method': results[0]['method'] if results else 'unknown'
        }
        
        return report
    
    def demo_anomaly_detection(self):
        """æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ"""
        print("ğŸš€ é«˜çº§åŠŸèƒ½ç¬¬2è¯¾ï¼šå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ")
        print("=" * 60)
        
        # åŠ è½½ç¤ºä¾‹æ•°æ®
        sample_data = self.load_sample_data()
        
        print("ğŸ“Š æ•°æ®å‡†å¤‡...")
        normal_texts = sample_data["æ­£å¸¸è¯„è®º"] + sample_data["æ­£å¸¸æŠ€æœ¯è®¨è®º"]
        test_texts = (
            sample_data["æ­£å¸¸è¯„è®º"][:2] +
            sample_data["åƒåœ¾å¹¿å‘Š"] +
            sample_data["è™šå‡è¯„è®º"] +
            sample_data["æ¶æ„æ”»å‡»"]
        )
        
        # åˆ›å»ºæ­£å¸¸æ¡£æ¡ˆ
        profile = self.create_normal_profile(normal_texts, "è¯„è®ºç³»ç»Ÿ")
        
        print("\nğŸ” ç¬¬1éƒ¨åˆ†ï¼šåŸºäºè·ç¦»çš„å¼‚å¸¸æ£€æµ‹")
        print("=" * 50)
        
        distance_results = self.distance_based_detection(test_texts, "è¯„è®ºç³»ç»Ÿ")
        self._print_detection_results(distance_results, "è·ç¦»æ³•")
        
        print("\nğŸ” ç¬¬2éƒ¨åˆ†ï¼šIsolation Forestå¼‚å¸¸æ£€æµ‹")
        print("=" * 50)
        
        isolation_results = self.isolation_forest_detection(test_texts)
        self._print_detection_results(isolation_results, "Isolation Forest")
        
        print("\nğŸ” ç¬¬3éƒ¨åˆ†ï¼šLocal Outlier Factoræ£€æµ‹")
        print("=" * 50)
        
        lof_results = self.lof_detection(test_texts)
        self._print_detection_results(lof_results, "LOF")
        
        print("\nğŸ” ç¬¬4éƒ¨åˆ†ï¼šOne-Class SVMæ£€æµ‹")
        print("=" * 50)
        
        svm_results = self.one_class_svm_detection(test_texts, normal_texts)
        self._print_detection_results(svm_results, "One-Class SVM")
        
        print("\nğŸ” ç¬¬5éƒ¨åˆ†ï¼šé›†æˆå¼‚å¸¸æ£€æµ‹")
        print("=" * 50)
        
        ensemble_results = self.ensemble_detection(test_texts, normal_texts, "è¯„è®ºç³»ç»Ÿ")
        self._print_detection_results(ensemble_results, "é›†æˆæ–¹æ³•")
        
        # ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š
        report = self.generate_detection_report(test_texts, ensemble_results)
        print(f"\nğŸ“Š æ£€æµ‹ç»Ÿè®¡æŠ¥å‘Š:")
        print(f"   æ€»æ–‡æœ¬æ•°: {report['total_texts']}")
        print(f"   å¼‚å¸¸æ•°: {report['anomaly_count']}")
        print(f"   å¼‚å¸¸ç‡: {report['anomaly_rate']:.2%}")
        print(f"   é«˜ç½®ä¿¡åº¦å¼‚å¸¸: {report['high_confidence_anomalies']}")
        print(f"   ä¸­ç½®ä¿¡åº¦å¼‚å¸¸: {report['medium_confidence_anomalies']}")
        print(f"   ä½ç½®ä¿¡åº¦å¼‚å¸¸: {report['low_confidence_anomalies']}")
        
        # å¯è§†åŒ–ç»“æœ
        self.visualize_anomaly_detection(test_texts, ensemble_results, "Ensemble")
    
    def _print_detection_results(self, results: List[Dict], method_name: str):
        """æ‰“å°æ£€æµ‹ç»“æœ"""
        anomalies = [r for r in results if r['is_anomaly']]
        
        print(f"\n{method_name}æ£€æµ‹ç»“æœ:")
        print(f"   æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸æ–‡æœ¬")
        
        for anomaly in anomalies[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"   âš ï¸ {anomaly['text'][:50]}... (ç½®ä¿¡åº¦: {anomaly['confidence']:.3f})")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ")
    print("=" * 60)
    
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    try:
        detector = AnomalyDetectionSystem()
        detector.demo_anomaly_detection()
        
        print("\nğŸ‰ å¼‚å¸¸æ£€æµ‹æ¼”ç¤ºå®Œæˆï¼")
        print("\næ ¸å¿ƒæŠ€æœ¯æ€»ç»“ï¼š")
        print("   â€¢ å¤šç§å¼‚å¸¸æ£€æµ‹ç®—æ³•")
        print("   â€¢ é›†æˆæ£€æµ‹æ–¹æ³•")
        print("   â€¢ å®æ—¶ç›‘æ§ç³»ç»Ÿ")
        print("   â€¢ å¯è§†åŒ–åˆ†æ")
        print("\nå®é™…åº”ç”¨åœºæ™¯ï¼š")
        print("   â€¢ åƒåœ¾å†…å®¹è¿‡æ»¤")
        print("   â€¢ æ¬ºè¯ˆæ£€æµ‹")
        print("   â€¢ å†…å®¹è´¨é‡ç›‘æ§")
        print("   â€¢ å®æ—¶é£æ§ç³»ç»Ÿ")
        print("\nä¸‹ä¸€è¯¾ï¼š03-03-visualization.py - é«˜çº§å¯è§†åŒ–")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()