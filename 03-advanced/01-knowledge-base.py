#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§åŠŸèƒ½ç¬¬1è¯¾ï¼šæ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ
=========================

æ„å»ºä¼ä¸šçº§æ™ºèƒ½çŸ¥è¯†åº“ï¼Œå®ç°è¯­ä¹‰æŸ¥è¯¢ã€çŸ¥è¯†å›¾è°±ã€å¢é‡æ›´æ–°ç­‰åŠŸèƒ½ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. æ„å»ºçŸ¥è¯†å›¾è°±å’Œè¯­ä¹‰ç½‘ç»œ
2. å®ç°æ™ºèƒ½é—®ç­”å’ŒæŸ¥è¯¢ä¼˜åŒ–
3. è®¾è®¡å¢é‡æ›´æ–°æœºåˆ¶
4. å¤šæ¨¡æ€çŸ¥è¯†èåˆ
5. ä¼ä¸šçº§çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ
"""

import os
import sys
import json
import numpy as np
import sqlite3
from typing import List, Dict, Tuple, Optional, Any
import pickle
from datetime import datetime
import hashlib

# ä¿®å¤Python 3.12 datetimeè­¦å‘Š
sqlite3.register_adapter(datetime, lambda dt: dt.isoformat())
sqlite3.register_converter("timestamp", lambda x: datetime.fromisoformat(x.decode()))

# æ·»åŠ utilsç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.embedding_client import EmbeddingClient

class KnowledgeBaseSystem:
    """æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ"""
    
    def __init__(self, db_path: str = "knowledge_base.db"):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        self.client = EmbeddingClient()
        self.db_path = db_path
        self.init_database()
        self.embedding_cache = {}
        
    def init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çŸ¥è¯†æ¡ç›®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_entries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                category TEXT,
                tags TEXT,
                embedding TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                version INTEGER DEFAULT 1
            )
        ''')
        
        # çŸ¥è¯†å…³ç³»è¡¨ï¼ˆçŸ¥è¯†å›¾è°±ï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_relationships (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER,
                target_id INTEGER,
                relationship_type TEXT,
                confidence REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES knowledge_entries(id),
                FOREIGN KEY (target_id) REFERENCES knowledge_entries(id)
            )
        ''')
        
        # æŸ¥è¯¢å†å²è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS query_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query TEXT NOT NULL,
                response TEXT,
                relevance_score REAL,
                query_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def add_knowledge(self, title: str, content: str, category: str = "", tags: List[str] = None) -> int:
        """æ·»åŠ çŸ¥è¯†æ¡ç›®"""
        embedding = self.client.get_embedding(f"{title} {content}")
        embedding_str = json.dumps(embedding)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO knowledge_entries (title, content, category, tags, embedding)
            VALUES (?, ?, ?, ?, ?)
        ''', (title, content, category, json.dumps(tags or []), embedding_str))
        
        entry_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        print(f"âœ… å·²æ·»åŠ çŸ¥è¯†æ¡ç›®: {title} (ID: {entry_id})")
        return entry_id
    
    def build_knowledge_graph(self, entries: List[Dict]) -> Dict:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        print("ğŸ”¨ æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # æ·»åŠ æ‰€æœ‰çŸ¥è¯†æ¡ç›®
        entry_ids = []
        for entry in entries:
            entry_id = self.add_knowledge(
                entry['title'],
                entry['content'],
                entry.get('category', ''),
                entry.get('tags', [])
            )
            entry_ids.append(entry_id)
        
        # æ„å»ºå…³ç³»ï¼ˆåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        relationships = self._build_relationships(entry_ids)
        
        return {
            'total_entries': len(entry_ids),
            'relationships': len(relationships),
            'entry_ids': entry_ids
        }
    
    def _build_relationships(self, entry_ids: List[int]) -> List[Dict]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ„å»ºçŸ¥è¯†å…³ç³»"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–æ‰€æœ‰æ¡ç›®
        cursor.execute('SELECT id, title, content, embedding FROM knowledge_entries WHERE id IN ({})'.format(
            ','.join('?' * len(entry_ids))), entry_ids)
        entries = cursor.fetchall()
        
        relationships = []
        
        # è®¡ç®—æ¡ç›®é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        for i, (id1, title1, content1, emb1_str) in enumerate(entries):
            embedding1 = np.array(json.loads(emb1_str))
            
            for j, (id2, title2, content2, emb2_str) in enumerate(entries[i+1:], i+1):
                embedding2 = np.array(json.loads(emb2_str))
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(embedding1, embedding2) / (
                    np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
                )
                
                if similarity > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    relationship = {
                        'source_id': id1,
                        'target_id': id2,
                        'relationship_type': 'semantic_similarity',
                        'confidence': similarity
                    }
                    relationships.append(relationship)
                    
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    cursor.execute('''
                        INSERT INTO knowledge_relationships (source_id, target_id, relationship_type, confidence)
                        VALUES (?, ?, ?, ?)
                    ''', (id1, id2, 'semantic_similarity', float(similarity)))
        
        conn.commit()
        conn.close()
        
        print(f"âœ… å·²æ„å»º {len(relationships)} ä¸ªçŸ¥è¯†å…³ç³»")
        return relationships
    
    def semantic_query(self, query: str, top_k: int = 5, category: str = None) -> List[Dict]:
        """è¯­ä¹‰æŸ¥è¯¢"""
        query_embedding = self.client.get_embedding(query)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        base_query = '''
            SELECT id, title, content, category, tags, created_at, embedding
            FROM knowledge_entries
        '''
        params = []
        
        if category:
            base_query += ' WHERE category = ?'
            params.append(category)
        
        cursor.execute(base_query, params)
        entries = cursor.fetchall()
        
        # è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
        results = []
        for entry in entries:
            id, title, content, cat, tags_str, created_at, emb_str = entry
            embedding = np.array(json.loads(emb_str))
            
            similarity = np.dot(query_embedding, embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(embedding)
            )
            
            results.append({
                'id': id,
                'title': title,
                'content': content[:200] + '...' if len(content) > 200 else content,
                'category': cat,
                'tags': json.loads(tags_str),
                'similarity': similarity,
                'created_at': created_at
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        top_results = results[:top_k]
        
        # è®°å½•æŸ¥è¯¢å†å²
        self._record_query(query, json.dumps(top_results), 
                          top_results[0]['similarity'] if top_results else 0)
        
        conn.close()
        return top_results
    
    def smart_qa(self, question: str) -> Dict:
        """æ™ºèƒ½é—®ç­”"""
        # è¯­ä¹‰æŸ¥è¯¢è·å–ç›¸å…³çŸ¥è¯†
        relevant_knowledge = self.semantic_query(question, top_k=3)
        
        if not relevant_knowledge:
            return {
                'answer': 'æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çŸ¥è¯†ç‚¹',
                'confidence': 0.0,
                'sources': []
            }
        
        # æ„å»ºç­”æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…ä¸­å¯ä½¿ç”¨æ›´å¤æ‚çš„LLMï¼‰
        context = '\n'.join([k['content'] for k in relevant_knowledge])
        
        # åŸºäºæœ€ç›¸å…³çŸ¥è¯†ç”Ÿæˆç­”æ¡ˆ
        best_match = relevant_knowledge[0]
        
        answer = f"åŸºäºçŸ¥è¯†åº“ï¼Œ{best_match['title']}çš„ç›¸å…³å†…å®¹æ˜¯ï¼š\n{best_match['content']}"
        
        return {
            'answer': answer,
            'confidence': best_match['similarity'],
            'sources': relevant_knowledge
        }
    
    def incremental_update(self, new_entries: List[Dict]) -> Dict:
        """å¢é‡æ›´æ–°"""
        print("ğŸ”„ æ‰§è¡Œå¢é‡æ›´æ–°...")
        
        updated_count = 0
        new_count = 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for entry in new_entries:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸºäºæ ‡é¢˜+å†…å®¹å“ˆå¸Œï¼‰
            content_hash = hashlib.md5(f"{entry['title']}{entry['content']}".encode()).hexdigest()
            
            cursor.execute('''
                SELECT id, content FROM knowledge_entries 
                WHERE title = ? AND content = ?
            ''', (entry['title'], entry['content']))
            
            existing = cursor.fetchone()
            
            if existing:
                # æ›´æ–°ç°æœ‰æ¡ç›®
                cursor.execute('''
                    UPDATE knowledge_entries 
                    SET content = ?, category = ?, tags = ?, updated_at = ?, version = version + 1
                    WHERE id = ?
                ''', (entry['content'], entry.get('category', ''), 
                      json.dumps(entry.get('tags', [])), datetime.now(), existing[0]))
                updated_count += 1
            else:
                # æ·»åŠ æ–°æ¡ç›®
                self.add_knowledge(entry['title'], entry['content'], 
                                 entry.get('category', ''), entry.get('tags', []))
                new_count += 1
        
        conn.commit()
        conn.close()
        
        # é‡æ–°æ„å»ºå—å½±å“çš„å…³ç³»
        if new_count > 0 or updated_count > 0:
            self._rebuild_relationships()
        
        return {
            'new_entries': new_count,
            'updated_entries': updated_count,
            'total_entries': new_count + updated_count
        }
    
    def _rebuild_relationships(self):
        """é‡å»ºçŸ¥è¯†å…³ç³»"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ¸…ç©ºç°æœ‰å…³ç³»
        cursor.execute('DELETE FROM knowledge_relationships')
        
        # è·å–æ‰€æœ‰æ¡ç›®ID
        cursor.execute('SELECT id FROM knowledge_entries')
        entry_ids = [row[0] for row in cursor.fetchall()]
        
        conn.commit()
        conn.close()
        
        # é‡æ–°æ„å»ºå…³ç³»
        self._build_relationships(entry_ids)
    
    def _record_query(self, query: str, response: str, relevance_score: float):
        """è®°å½•æŸ¥è¯¢å†å²"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO query_history (query, response, relevance_score)
            VALUES (?, ?, ?)
        ''', (query, response, relevance_score))
        
        conn.commit()
        conn.close()
    
    def get_analytics(self) -> Dict:
        """è·å–çŸ¥è¯†åº“åˆ†ææ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åŸºæœ¬ç»Ÿè®¡
        cursor.execute('SELECT COUNT(*) FROM knowledge_entries')
        total_entries = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM knowledge_relationships')
        total_relationships = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM query_history')
        total_queries = cursor.fetchone()[0]
        
        # ç±»åˆ«åˆ†å¸ƒ
        cursor.execute('SELECT category, COUNT(*) FROM knowledge_entries GROUP BY category')
        category_stats = dict(cursor.fetchall())
        
        # æŸ¥è¯¢ç»Ÿè®¡
        cursor.execute('SELECT AVG(relevance_score) FROM query_history')
        avg_relevance = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            'total_entries': total_entries,
            'total_relationships': total_relationships,
            'total_queries': total_queries,
            'category_stats': category_stats,
            'average_relevance_score': avg_relevance
        }
    
    def load_sample_knowledge(self) -> List[Dict]:
        """åŠ è½½ç¤ºä¾‹çŸ¥è¯†æ•°æ®"""
        sample_knowledge = [
            {
                "title": "æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ",
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚ä¸»è¦ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚",
                "category": "äººå·¥æ™ºèƒ½",
                "tags": ["æœºå™¨å­¦ä¹ ", "AIåŸºç¡€", "ç›‘ç£å­¦ä¹ ", "æ— ç›‘ç£å­¦ä¹ "]
            },
            {
                "title": "æ·±åº¦å­¦ä¹ åŸç†",
                "content": "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚é—®é¢˜ï¼Œé€šè¿‡åå‘ä¼ æ’­ç®—æ³•ä¼˜åŒ–ç½‘ç»œæƒé‡ã€‚å¸¸è§æ¶æ„åŒ…æ‹¬CNNã€RNNå’ŒTransformerã€‚",
                "category": "äººå·¥æ™ºèƒ½",
                "tags": ["æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "CNN", "RNN", "Transformer"]
            },
            {
                "title": "Pythonæ•°æ®ç§‘å­¦å·¥å…·é“¾",
                "content": "Pythonåœ¨æ•°æ®ç§‘å­¦ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä¸»è¦å·¥å…·åŒ…æ‹¬NumPyã€Pandasã€Scikit-learnã€Matplotlibå’ŒTensorFlow/PyTorchã€‚",
                "category": "ç¼–ç¨‹è¯­è¨€",
                "tags": ["Python", "æ•°æ®ç§‘å­¦", "NumPy", "Pandas", "Scikit-learn"]
            },
            {
                "title": "è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨",
                "content": "NLPè®©è®¡ç®—æœºç†è§£å’Œå¤„ç†äººç±»è¯­è¨€ï¼Œåº”ç”¨åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘å’Œé—®ç­”ç³»ç»Ÿã€‚",
                "category": "äººå·¥æ™ºèƒ½",
                "tags": ["NLP", "æ–‡æœ¬åˆ†ç±»", "æƒ…æ„Ÿåˆ†æ", "æœºå™¨ç¿»è¯‘", "é—®ç­”ç³»ç»Ÿ"]
            },
            {
                "title": "äº‘è®¡ç®—æœåŠ¡ç±»å‹",
                "content": "äº‘è®¡ç®—æä¾›ä¸‰ç§ä¸»è¦æœåŠ¡ç±»å‹ï¼šIaaSï¼ˆåŸºç¡€è®¾æ–½å³æœåŠ¡ï¼‰ã€PaaSï¼ˆå¹³å°å³æœåŠ¡ï¼‰å’ŒSaaSï¼ˆè½¯ä»¶å³æœåŠ¡ï¼‰ã€‚",
                "category": "æŠ€æœ¯æ¶æ„",
                "tags": ["äº‘è®¡ç®—", "IaaS", "PaaS", "SaaS", "æ¶æ„"]
            },
            {
                "title": "åŒºå—é“¾æŠ€æœ¯åŸç†",
                "content": "åŒºå—é“¾æ˜¯åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œé€šè¿‡å¯†ç å­¦ä¿è¯æ•°æ®ä¸å¯ç¯¡æ”¹ï¼Œä¸»è¦åº”ç”¨äºåŠ å¯†è´§å¸ã€ä¾›åº”é“¾ç®¡ç†å’Œæ™ºèƒ½åˆçº¦ã€‚",
                "category": "æŠ€æœ¯æ¶æ„",
                "tags": ["åŒºå—é“¾", "åˆ†å¸ƒå¼è´¦æœ¬", "åŠ å¯†è´§å¸", "æ™ºèƒ½åˆçº¦"]
            }
        ]
        return sample_knowledge
    
    def demo_knowledge_base(self):
        """æ¼”ç¤ºçŸ¥è¯†åº“ç³»ç»Ÿ"""
        print("ğŸš€ é«˜çº§åŠŸèƒ½ç¬¬1è¯¾ï¼šæ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ")
        print("=" * 60)
        
        # åŠ è½½ç¤ºä¾‹çŸ¥è¯†
        sample_knowledge = self.load_sample_knowledge()
        
        print("ğŸ“š æ„å»ºçŸ¥è¯†åº“...")
        stats = self.build_knowledge_graph(sample_knowledge)
        print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼š{stats['total_entries']} æ¡ç›®ï¼Œ{stats['relationships']} å…³ç³»")
        
        # æ¼”ç¤ºæŸ¥è¯¢åŠŸèƒ½
        print("\nğŸ” ç¬¬1éƒ¨åˆ†ï¼šè¯­ä¹‰æŸ¥è¯¢æ¼”ç¤º")
        print("=" * 50)
        
        test_queries = [
            "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
            "Pythonæ•°æ®åˆ†æå·¥å…·",
            "åŒºå—é“¾å¦‚ä½•å·¥ä½œ",
            "æ·±åº¦å­¦ä¹ æ¶æ„"
        ]
        
        for query in test_queries:
            print(f"\nâ“ æŸ¥è¯¢: {query}")
            results = self.semantic_query(query, top_k=2)
            
            for result in results:
                print(f"   ğŸ“– {result['title']} (ç›¸ä¼¼åº¦: {result['similarity']:.3f})")
                print(f"      {result['content'][:100]}...")
        
        # æ¼”ç¤ºæ™ºèƒ½é—®ç­”
        print("\nğŸ¤– ç¬¬2éƒ¨åˆ†ï¼šæ™ºèƒ½é—®ç­”æ¼”ç¤º")
        print("=" * 50)
        
        questions = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "Pythonåœ¨æ•°æ®ç§‘å­¦ä¸­æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ",
            "åŒºå—é“¾æœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ"
        ]
        
        for question in questions:
            print(f"\nâ“ é—®é¢˜: {question}")
            response = self.smart_qa(question)
            print(f"   ğŸ¤– {response['answer'][:200]}...")
            print(f"   ğŸ“Š ç½®ä¿¡åº¦: {response['confidence']:.3f}")
        
        # æ¼”ç¤ºå¢é‡æ›´æ–°
        print("\nğŸ”„ ç¬¬3éƒ¨åˆ†ï¼šå¢é‡æ›´æ–°æ¼”ç¤º")
        print("=" * 50)
        
        new_entries = [
            {
                "title": "å¼ºåŒ–å­¦ä¹ ç®€ä»‹",
                "content": "å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±å’Œæƒ©ç½šæœºåˆ¶è®­ç»ƒæ™ºèƒ½ä½“åšå‡ºæœ€ä¼˜å†³ç­–ï¼Œåº”ç”¨åŒ…æ‹¬æ¸¸æˆAIå’Œæœºå™¨äººæ§åˆ¶ã€‚",
                "category": "äººå·¥æ™ºèƒ½",
                "tags": ["å¼ºåŒ–å­¦ä¹ ", "æ¸¸æˆAI", "æœºå™¨äººæ§åˆ¶"]
            },
            {
                "title": "è¾¹ç¼˜è®¡ç®—æ¦‚å¿µ",
                "content": "è¾¹ç¼˜è®¡ç®—å°†è®¡ç®—èƒ½åŠ›éƒ¨ç½²åˆ°æ•°æ®æºé™„è¿‘ï¼Œå‡å°‘å»¶è¿Ÿï¼Œé€‚ç”¨äºIoTå’Œå®æ—¶åº”ç”¨ã€‚",
                "category": "æŠ€æœ¯æ¶æ„",
                "tags": ["è¾¹ç¼˜è®¡ç®—", "IoT", "å®æ—¶å¤„ç†"]
            }
        ]
        
        update_stats = self.incremental_update(new_entries)
        print(f"âœ… å¢é‡æ›´æ–°å®Œæˆï¼šæ–°å¢ {update_stats['new_entries']} æ¡ï¼Œæ›´æ–° {update_stats['updated_entries']} æ¡")
        
        # å±•ç¤ºåˆ†ææ•°æ®
        print("\nğŸ“Š ç¬¬4éƒ¨åˆ†ï¼šçŸ¥è¯†åº“åˆ†æ")
        print("=" * 50)
        
        analytics = self.get_analytics()
        print(f"ğŸ“ˆ æ€»æ¡ç›®æ•°: {analytics['total_entries']}")
        print(f"ğŸ”— å…³ç³»æ•°: {analytics['total_relationships']}")
        print(f"â“ æŸ¥è¯¢æ•°: {analytics['total_queries']}")
        print(f"ğŸ¯ å¹³å‡ç›¸å…³åº¦: {analytics['average_relevance_score']:.3f}")
        
        if analytics['category_stats']:
            print("\nğŸ“‚ ç±»åˆ«åˆ†å¸ƒ:")
            for category, count in analytics['category_stats'].items():
                print(f"   {category}: {count} æ¡")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ")
    print("=" * 60)
    
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    try:
        kb_system = KnowledgeBaseSystem()
        kb_system.demo_knowledge_base()
        
        print("\nğŸ‰ çŸ¥è¯†åº“ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
        print("\næ ¸å¿ƒæŠ€æœ¯æ€»ç»“ï¼š")
        print("   â€¢ è¯­ä¹‰æŸ¥è¯¢å’Œæ™ºèƒ½é—®ç­”")
        print("   â€¢ çŸ¥è¯†å›¾è°±æ„å»º")
        print("   â€¢ å¢é‡æ›´æ–°æœºåˆ¶")
        print("   â€¢ å¤šæ¨¡æ€çŸ¥è¯†èåˆ")
        print("\nå®é™…åº”ç”¨åœºæ™¯ï¼š")
        print("   â€¢ ä¼ä¸šçŸ¥è¯†åº“")
        print("   â€¢ æŠ€æœ¯æ–‡æ¡£é—®ç­”")
        print("   â€¢ äº§å“æ‰‹å†Œæ™ºèƒ½æŸ¥è¯¢")
        print("   â€¢ æ•™è‚²çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ")
        print("\nä¸‹ä¸€è¯¾ï¼š03-02-anomaly-detection.py - å¼‚å¸¸æ£€æµ‹")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()