#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§åŠŸèƒ½ç¬¬3è¯¾ï¼šé«˜çº§å¯è§†åŒ–ç³»ç»Ÿ
=========================

åŸºäºæ–‡æœ¬åµŒå…¥çš„é«˜çº§æ•°æ®å¯è§†åŒ–ç³»ç»Ÿï¼Œå®ç°3Dç©ºé—´å±•ç¤ºã€äº¤äº’å¼å›¾è¡¨ã€æ—¶é—´åºåˆ—åˆ†æç­‰ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. æ„å»º3Dç©ºé—´å¯è§†åŒ–
2. å®ç°äº¤äº’å¼å¯è§†åŒ–ç•Œé¢
3. æ—¶é—´åºåˆ—åµŒå…¥åˆ†æ
4. èšç±»ç»“æœåŠ¨æ€å±•ç¤º
5. è¯­ä¹‰ç½‘ç»œå›¾å¯è§†åŒ–
"""

import os
import sys
import numpy as np
from typing import List, Dict, Tuple, Any
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
from datetime import datetime, timedelta
import json
import pickle
from sklearn.decomposition import PCA

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
from matplotlib import rcParams
rcParams['font.family'] = ['sans-serif']
rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False
from sklearn.manifold import TSNE
import networkx as nx
from collections import defaultdict
import seaborn as sns

# æ·»åŠ utilsç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.embedding_client import EmbeddingClient

class AdvancedVisualizationSystem:
    """é«˜çº§å¯è§†åŒ–ç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¯è§†åŒ–ç³»ç»Ÿ"""
        self.client = EmbeddingClient()
        self.color_palettes = {
            'tech': '#1f77b4',
            'sports': '#ff7f0e', 
            'finance': '#2ca02c',
            'entertainment': '#d62728',
            'default': '#9467bd'
        }
        
    def load_sample_data(self) -> Dict[str, List[str]]:
        """åŠ è½½ç¤ºä¾‹æ•°æ®"""
        # åŒ…å«æ—¶é—´æˆ³çš„æ•°æ®
        base_date = datetime.now()
        sample_data = {
            'ç§‘æŠ€æ–°é—»': [
                {'text': 'äººå·¥æ™ºèƒ½æŠ€æœ¯çªç ´ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å‚æ•°é‡çªç ´ä¸‡äº¿çº§åˆ«', 'date': base_date - timedelta(days=1)},
                {'text': 'é‡å­è®¡ç®—æœºå®ç°é‡å­éœ¸æƒï¼Œè®¡ç®—é€Ÿåº¦æå‡ä¸‡å€', 'date': base_date - timedelta(days=3)},
                {'text': '5Gç½‘ç»œå…¨çƒéƒ¨ç½²å®Œæˆï¼Œç‰©è”ç½‘è®¾å¤‡è¿æ¥æ•°è¶…è¿‡100äº¿', 'date': base_date - timedelta(days=5)},
                {'text': 'è‡ªåŠ¨é©¾é©¶æŠ€æœ¯é‡å¤§çªç ´ï¼ŒL4çº§åˆ«è½¦è¾†å¼€å§‹å•†ä¸šåŒ–è¿è¥', 'date': base_date - timedelta(days=7)},
                {'text': 'åŒºå—é“¾æŠ€æœ¯åœ¨é‡‘èé¢†åŸŸåº”ç”¨æ‰©å¤§ï¼Œæ•°å­—è´§å¸äº¤æ˜“é‡åˆ›æ–°é«˜', 'date': base_date - timedelta(days=9)}
            ],
            'ä½“è‚²æ–°é—»': [
                {'text': 'å›½è¶³ä¸–ç•Œæ¯é¢„é€‰èµ›å–å¾—èƒœåˆ©ï¼Œçƒè¿·çƒ­æƒ…é«˜æ¶¨', 'date': base_date - timedelta(days=2)},
                {'text': 'NBAæ€»å†³èµ›æ¹–äººvså‡¯å°”ç‰¹äººæŠ¢ä¸ƒå¤§æˆ˜å³å°†å¼€å§‹', 'date': base_date - timedelta(days=4)},
                {'text': 'ä¸­å›½å¥³æ’ä¸–ç•Œé”¦æ ‡èµ›å¤ºå† ï¼Œå±•ç°å¼ºå¤§å®åŠ›', 'date': base_date - timedelta(days=6)},
                {'text': 'å¥¥è¿ä¼šç­¹å¤‡å·¥ä½œå®Œæˆï¼Œå„å›½è¿åŠ¨å‘˜é™†ç»­æŠµè¾¾', 'date': base_date - timedelta(days=8)},
                {'text': 'ä¸–ç•Œæ¯è¶³çƒèµ›åˆ†ç»„æŠ½ç­¾ç»“æœå…¬å¸ƒï¼Œå¼ºé˜Ÿäº‘é›†', 'date': base_date - timedelta(days=10)}
            ],
            'è´¢ç»æ–°é—»': [
                {'text': 'å¤®è¡Œå®£å¸ƒé™æ¯æ”¿ç­–ï¼Œåˆºæ¿€ç»æµå¢é•¿', 'date': base_date - timedelta(days=1)},
                {'text': 'è‚¡å¸‚ä»Šæ—¥å¤§æ¶¨ï¼Œç§‘æŠ€è‚¡é¢†æ¶¨å¤§ç›˜', 'date': base_date - timedelta(days=2)},
                {'text': 'æˆ¿åœ°äº§å¸‚åœºè°ƒæ§æ”¿ç­–æ•ˆæœæ˜¾è‘—ï¼Œæˆ¿ä»·ç¨³å®š', 'date': base_date - timedelta(days=4)},
                {'text': 'å›½é™…è´¸æ˜“é¢åˆ›æ–°é«˜ï¼ŒåŒè¾¹åˆä½œåŠ å¼º', 'date': base_date - timedelta(days=6)},
                {'text': 'æ•°å­—è´§å¸è¯•ç‚¹æ‰©å¤§ï¼Œé‡‘èç§‘æŠ€å‘å±•è¿…é€Ÿ', 'date': base_date - timedelta(days=8)}
            ]
        }
        return sample_data
    
    def create_3d_visualization(self, texts: List[str], labels: List[str] = None) -> Dict:
        """åˆ›å»º3Dç©ºé—´å¯è§†åŒ–"""
        print("ğŸ¯ åˆ›å»º3Dç©ºé—´å¯è§†åŒ–...")
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = [self.client.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        # ä½¿ç”¨PCAé™ç»´åˆ°3D
        pca_3d = PCA(n_components=3)
        embeddings_3d = pca_3d.fit_transform(embeddings_array)
        
        # åˆ›å»º3Då›¾å½¢
        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111, projection='3d')
        
        # æ ¹æ®æ ‡ç­¾ç€è‰²
        if labels:
            unique_labels = list(set(labels))
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
            
            for i, label in enumerate(unique_labels):
                mask = np.array(labels) == label
                ax.scatter(embeddings_3d[mask, 0], embeddings_3d[mask, 1], embeddings_3d[mask, 2],
                          c=[colors[i]], label=label, alpha=0.7, s=50)
        else:
            ax.scatter(embeddings_3d[:, 0], embeddings_3d[:, 1], embeddings_3d[:, 2],
                      c='blue', alpha=0.6, s=50)
        
        ax.set_xlabel('ä¸»æˆåˆ†1', fontsize=12)
        ax.set_ylabel('ä¸»æˆåˆ†2', fontsize=12)
        ax.set_zlabel('ä¸»æˆåˆ†3', fontsize=12)
        ax.set_title('æ–‡æœ¬åµŒå…¥3Dç©ºé—´å¯è§†åŒ–', fontsize=14)
        ax.legend()
        
        filename = "03-advanced/3d_visualization.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        return {
            'embeddings_3d': embeddings_3d,
            'explained_variance_ratio': pca_3d.explained_variance_ratio_,
            'filename': filename
        }
    
    def create_interactive_plotly(self, texts: List[str], categories: List[str] = None) -> str:
        """åˆ›å»ºäº¤äº’å¼Plotlyå¯è§†åŒ–"""
        print("ğŸ¯ åˆ›å»ºäº¤äº’å¼å¯è§†åŒ–...")
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = [self.client.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        # ä½¿ç”¨t-SNEé™ç»´åˆ°2D
        tsne = TSNE(n_components=2, random_state=42, perplexity=5)
        embeddings_2d = tsne.fit_transform(embeddings_array)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame({
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1],
            'text': texts,
            'category': categories or ['default'] * len(texts),
            'length': [len(text) for text in texts]
        })
        
        # åˆ›å»ºäº¤äº’å¼å›¾å½¢
        fig = px.scatter(df, x='x', y='y', 
                        color='category',
                        text='text',
                        size='length',
                        hover_data={'text': True, 'category': True},
                        title='æ–‡æœ¬åµŒå…¥äº¤äº’å¼å¯è§†åŒ–')
        
        # æ›´æ–°å¸ƒå±€
        fig.update_traces(textposition='top center', textfont_size=8)
        fig.update_layout(
            height=600,
            showlegend=True,
            xaxis_title="t-SNEç»´åº¦1",
            yaxis_title="t-SNEç»´åº¦2"
        )
        
        filename = "03-advanced/interactive_visualization.html"
        fig.write_html(filename)
        # åœ¨å‘½ä»¤è¡Œç¯å¢ƒä¸æ˜¾ç¤ºï¼Œåªä¿å­˜æ–‡ä»¶
        # fig.show()
        
        return filename
    
    def create_time_series_analysis(self, timed_texts: List[Dict]) -> Dict:
        """åˆ›å»ºæ—¶é—´åºåˆ—åµŒå…¥åˆ†æ"""
        print("ğŸ¯ åˆ›å»ºæ—¶é—´åºåˆ—åˆ†æ...")
        
        # å¤„ç†æ—¶é—´åºåˆ—æ•°æ®
        categories = list(set([item['category'] for item in timed_texts]))
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºæ—¶é—´åºåˆ—åˆ†æ
        results = {}
        
        for category in categories:
            category_data = [item for item in timed_texts if item['category'] == category]
            
            # æŒ‰æ—¶é—´æ’åº
            category_data.sort(key=lambda x: x['date'])
            
            # è·å–åµŒå…¥å‘é‡
            texts = [item['text'] for item in category_data]
            embeddings = [self.client.get_embedding(text) for text in texts]
            embeddings_array = np.array(embeddings)
            
            # ä½¿ç”¨PCAé™ç»´åˆ°1Dï¼ˆæ—¶é—´è½´ï¼‰
            pca_1d = PCA(n_components=1)
            time_embeddings = pca_1d.fit_transform(embeddings_array)
            
            # åˆ›å»ºæ—¶é—´åºåˆ—å›¾
            dates = [item['date'] for item in category_data]
            
            plt.figure(figsize=(12, 6))
            plt.plot(dates, time_embeddings.flatten(), marker='o', linewidth=2, markersize=6)
            plt.title(f'{category} - è¯­ä¹‰æ¼”è¿›æ—¶é—´åºåˆ—', fontsize=14)
            plt.xlabel('æ—¶é—´', fontsize=12)
            plt.ylabel('è¯­ä¹‰ä½ç½®ï¼ˆä¸»æˆåˆ†1ï¼‰', fontsize=12)
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
            for i, (date, text, embedding) in enumerate(zip(dates, texts, time_embeddings)):
                if i % 2 == 0:  # æ¯éš”ä¸€ä¸ªæ˜¾ç¤º
                    plt.annotate(text[:20] + '...', 
                               (date, embedding[0]), 
                               xytext=(10, 10), 
                               textcoords='offset points',
                               fontsize=8, alpha=0.7)
            
            filename = f"03-advanced/time_series_{category}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.show()
            
            results[category] = {
                'dates': dates,
                'embeddings': time_embeddings,
                'texts': texts,
                'filename': filename
            }
        
        return results
    
    def create_semantic_network(self, texts: List[str], similarity_threshold: float = 0.7) -> Dict:
        """åˆ›å»ºè¯­ä¹‰ç½‘ç»œå›¾"""
        print("ğŸ¯ åˆ›å»ºè¯­ä¹‰ç½‘ç»œå›¾...")
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = [self.client.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        n_texts = len(texts)
        similarity_matrix = np.zeros((n_texts, n_texts))
        
        for i in range(n_texts):
            for j in range(i+1, n_texts):
                similarity = np.dot(embeddings_array[i], embeddings_array[j]) / (
                    np.linalg.norm(embeddings_array[i]) * np.linalg.norm(embeddings_array[j])
                )
                similarity_matrix[i][j] = similarity
                similarity_matrix[j][i] = similarity
        
        # åˆ›å»ºç½‘ç»œå›¾
        G = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for i, text in enumerate(texts):
            G.add_node(i, text=text[:50] + '...' if len(text) > 50 else text)
        
        # æ·»åŠ è¾¹ï¼ˆåŸºäºç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
        for i in range(n_texts):
            for j in range(i+1, n_texts):
                if similarity_matrix[i][j] > similarity_threshold:
                    G.add_edge(i, j, weight=similarity_matrix[i][j])
        
        # ç»˜åˆ¶ç½‘ç»œå›¾
        plt.figure(figsize=(12, 10))
        pos = nx.spring_layout(G, k=3, iterations=50)
        
        # èŠ‚ç‚¹é¢œè‰²åŸºäºèšç±»ï¼ˆä½¿ç”¨K-meansï¼‰
        from sklearn.cluster import KMeans
        n_clusters = min(4, len(texts))
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        node_labels = kmeans.fit_predict(embeddings_array)
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))
        for i in range(n_clusters):
            cluster_nodes = [n for n in G.nodes() if node_labels[n] == i]
            nx.draw_networkx_nodes(G, pos, nodelist=cluster_nodes, 
                                 node_color=[colors[i]], node_size=1000, alpha=0.8)
        
        # ç»˜åˆ¶è¾¹
        edges = G.edges()
        weights = [G[u][v]['weight'] for u, v in edges]
        nx.draw_networkx_edges(G, pos, edges, width=[w*3 for w in weights], 
                             alpha=0.5, edge_color='gray')
        
        # ç»˜åˆ¶æ ‡ç­¾
        labels = nx.get_node_attributes(G, 'text')
        nx.draw_networkx_labels(G, pos, labels, font_size=8, alpha=0.7)
        
        plt.title('æ–‡æœ¬è¯­ä¹‰ç½‘ç»œå›¾', fontsize=14)
        plt.axis('off')
        
        filename = "03-advanced/semantic_network.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        return {
            'graph': G,
            'similarity_matrix': similarity_matrix,
            'node_labels': node_labels,
            'filename': filename
        }
    
    def create_clustering_animation(self, texts: List[str], categories: List[str]) -> str:
        """åˆ›å»ºèšç±»åŠ¨æ€å±•ç¤º"""
        print("ğŸ¯ åˆ›å»ºèšç±»åŠ¨æ€å±•ç¤º...")
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = [self.client.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        # ä½¿ç”¨t-SNEé™ç»´
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(texts)//2))
        embeddings_2d = tsne.fit_transform(embeddings_array)
        
        # åˆ›å»ºåŠ¨ç”»æ•°æ®
        frames = []
        
        # K-meansèšç±»è¿‡ç¨‹ï¼ˆå¤šä¸ªkå€¼ï¼‰
        from sklearn.cluster import KMeans
        for k in range(2, min(6, len(texts))):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(embeddings_array)
            
            # åˆ›å»ºå¸§æ•°æ®
            frame_data = {
                'k': k,
                'embeddings': embeddings_2d,
                'labels': labels,
                'texts': texts,
                'categories': categories
            }
            frames.append(frame_data)
        
        # åˆ›å»ºäº¤äº’å¼åŠ¨ç”»
        fig = make_subplots(
            rows=1, cols=1,
            subplot_titles=['èšç±»è¿‡ç¨‹åŠ¨æ€å±•ç¤º']
        )
        
        # ä¸ºæ¯ä¸ªkå€¼åˆ›å»ºå­å›¾
        for i, frame in enumerate(frames):
            df = pd.DataFrame({
                'x': frame['embeddings'][:, 0],
                'y': frame['embeddings'][:, 1],
                'label': frame['labels'],
                'text': frame['texts'],
                'category': frame['categories']
            })
            
            # æ·»åŠ æ•£ç‚¹å›¾
            fig.add_trace(
                go.Scatter(
                    x=df['x'], y=df['y'],
                    mode='markers+text',
                    text=df['text'].apply(lambda x: x[:20] + '...'),
                    textposition='top center',
                    textfont_size=8,
                    marker=dict(
                        color=df['label'],
                        colorscale='Viridis',
                        size=10,
                        showscale=True
                    ),
                    name=f'k={frame["k"]}'
                ),
                row=1, col=1
            )
        
        fig.update_layout(
            title='èšç±»è¿‡ç¨‹åŠ¨æ€å±•ç¤º',
            height=600,
            showlegend=True,
            updatemenus=[{
                'type': 'buttons',
                'buttons': [
                    {
                        'label': 'æ’­æ”¾',
                        'method': 'animate',
                        'args': [None, {'frame': {'duration': 1000, 'redraw': True}}]
                    }
                ]
            }]
        )
        
        filename = "03-advanced/clustering_animation.html"
        fig.write_html(filename)
        # åœ¨å‘½ä»¤è¡Œç¯å¢ƒä¸æ˜¾ç¤ºï¼Œåªä¿å­˜æ–‡ä»¶
        # fig.show()
        
        return filename
    
    def create_heatmap_visualization(self, texts: List[str], categories: List[str]) -> str:
        """åˆ›å»ºç›¸ä¼¼åº¦çƒ­åŠ›å›¾"""
        print("ğŸ¯ åˆ›å»ºç›¸ä¼¼åº¦çƒ­åŠ›å›¾...")
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = [self.client.get_embedding(text) for text in texts]
        embeddings_array = np.array(embeddings)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        n_texts = len(texts)
        similarity_matrix = np.zeros((n_texts, n_texts))
        
        for i in range(n_texts):
            for j in range(n_texts):
                similarity = np.dot(embeddings_array[i], embeddings_array[j]) / (
                    np.linalg.norm(embeddings_array[i]) * np.linalg.norm(embeddings_array[j])
                )
                similarity_matrix[i][j] = similarity
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(12, 10))
        
        # ä½¿ç”¨seabornåˆ›å»ºæ›´ç¾è§‚çš„çƒ­åŠ›å›¾
        sns.heatmap(similarity_matrix, 
                   xticklabels=[text[:20] + '...' for text in texts],
                   yticklabels=[text[:20] + '...' for text in texts],
                   cmap='RdYlBu_r',
                   center=0,
                   square=True,
                   linewidths=0.5)
        
        plt.title('æ–‡æœ¬ç›¸ä¼¼åº¦çƒ­åŠ›å›¾', fontsize=14)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        filename = "03-advanced/similarity_heatmap.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        return filename
    
    def demo_advanced_visualization(self):
        """æ¼”ç¤ºé«˜çº§å¯è§†åŒ–ç³»ç»Ÿ"""
        print("ğŸš€ é«˜çº§åŠŸèƒ½ç¬¬3è¯¾ï¼šé«˜çº§å¯è§†åŒ–ç³»ç»Ÿ")
        print("=" * 60)
        
        # åŠ è½½ç¤ºä¾‹æ•°æ®
        sample_data = self.load_sample_data()
        
        # å‡†å¤‡æ•°æ®
        all_texts = []
        all_categories = []
        
        for category, items in sample_data.items():
            for item in items:
                all_texts.append(item['text'])
                all_categories.append(category)
        
        print(f"ğŸ“Š å·²åŠ è½½ {len(all_texts)} ä¸ªæ–‡æœ¬æ•°æ®")
        
        # æ¼”ç¤º1ï¼š3Då¯è§†åŒ–
        print("\nğŸ¯ ç¬¬1éƒ¨åˆ†ï¼š3Dç©ºé—´å¯è§†åŒ–")
        print("=" * 50)
        result_3d = self.create_3d_visualization(all_texts, all_categories)
        print(f"âœ… 3Då¯è§†åŒ–å®Œæˆï¼Œè§£é‡Šæ–¹å·®æ¯”ä¾‹: {sum(result_3d['explained_variance_ratio']):.3f}")
        
        # æ¼”ç¤º2ï¼šäº¤äº’å¼å¯è§†åŒ–
        print("\nğŸ¯ ç¬¬2éƒ¨åˆ†ï¼šäº¤äº’å¼å¯è§†åŒ–")
        print("=" * 50)
        interactive_file = self.create_interactive_plotly(all_texts, all_categories)
        print(f"âœ… äº¤äº’å¼å¯è§†åŒ–å·²ä¿å­˜: {interactive_file}")
        
        # æ¼”ç¤º3ï¼šæ—¶é—´åºåˆ—åˆ†æ
        print("\nğŸ¯ ç¬¬3éƒ¨åˆ†ï¼šæ—¶é—´åºåˆ—åˆ†æ")
        print("=" * 50)
        timed_texts = []
        for category, items in sample_data.items():
            for item in items:
                item['category'] = category
                timed_texts.append(item)
        
        time_results = self.create_time_series_analysis(timed_texts)
        print(f"âœ… æ—¶é—´åºåˆ—åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(time_results)} ä¸ªç±»åˆ«")
        
        # æ¼”ç¤º4ï¼šè¯­ä¹‰ç½‘ç»œå›¾
        print("\nğŸ¯ ç¬¬4éƒ¨åˆ†ï¼šè¯­ä¹‰ç½‘ç»œå›¾")
        print("=" * 50)
        network_result = self.create_semantic_network(all_texts[:8], similarity_threshold=0.6)
        print(f"âœ… è¯­ä¹‰ç½‘ç»œå›¾åˆ›å»ºå®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(network_result['graph'].nodes())}, è¾¹æ•°: {len(network_result['graph'].edges())}")
        
        # æ¼”ç¤º5ï¼šç›¸ä¼¼åº¦çƒ­åŠ›å›¾
        print("\nğŸ¯ ç¬¬5éƒ¨åˆ†ï¼šç›¸ä¼¼åº¦çƒ­åŠ›å›¾")
        print("=" * 50)
        heatmap_file = self.create_heatmap_visualization(all_texts[:6], all_categories[:6])
        print(f"âœ… ç›¸ä¼¼åº¦çƒ­åŠ›å›¾å·²ä¿å­˜: {heatmap_file}")
        
        # æ¼”ç¤º6ï¼šèšç±»åŠ¨ç”»
        print("\nğŸ¯ ç¬¬6éƒ¨åˆ†ï¼šèšç±»åŠ¨æ€å±•ç¤º")
        print("=" * 50)
        animation_file = self.create_clustering_animation(all_texts[:8], all_categories[:8])
        print(f"âœ… èšç±»åŠ¨æ€å±•ç¤ºå·²ä¿å­˜: {animation_file}")
        
        print("\nğŸ‰ é«˜çº§å¯è§†åŒ–æ¼”ç¤ºå®Œæˆï¼")
        print("\næ ¸å¿ƒæŠ€æœ¯æ€»ç»“ï¼š")
        print("   â€¢ 3Dç©ºé—´å¯è§†åŒ–")
        print("   â€¢ äº¤äº’å¼å¯è§†åŒ–")
        print("   â€¢ æ—¶é—´åºåˆ—åˆ†æ")
        print("   â€¢ è¯­ä¹‰ç½‘ç»œå›¾")
        print("   â€¢ ç›¸ä¼¼åº¦çƒ­åŠ›å›¾")
        print("   â€¢ èšç±»åŠ¨æ€å±•ç¤º")
        print("\nå®é™…åº”ç”¨åœºæ™¯ï¼š")
        print("   â€¢ æ–‡æœ¬ç©ºé—´å¯è§†åŒ–å·¥å…·")
        print("   â€¢ è¯­ä¹‰å…³ç³»å›¾")
        print("   â€¢ ç”¨æˆ·è¡Œä¸ºå¯è§†åŒ–")
        print("   â€¢ åŠ¨æ€å†…å®¹åˆ†æ")
        print("   â€¢ äº¤äº’å¼çŸ¥è¯†æ¢ç´¢")
        print("\nä¸‹ä¸€è¯¾ï¼š04-performance-optimization.py - æ€§èƒ½ä¼˜åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜çº§å¯è§†åŒ–ç³»ç»Ÿ")
    print("=" * 60)
    
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    try:
        viz_system = AdvancedVisualizationSystem()
        viz_system.demo_advanced_visualization()
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()