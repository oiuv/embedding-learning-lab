#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§åŠŸèƒ½ç¬¬4è¯¾ï¼šæ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ
=========================

æ–‡æœ¬åµŒå…¥ç³»ç»Ÿçš„æ€§èƒ½ä¼˜åŒ–ï¼ŒåŒ…æ‹¬å‘é‡ç´¢å¼•ä¼˜åŒ–ã€è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ã€ç¼“å­˜ç­–ç•¥ã€åˆ†å¸ƒå¼éƒ¨ç½²ç­‰ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. å‘é‡ç´¢å¼•ä¼˜åŒ–æŠ€æœ¯ï¼ˆFAISSã€Pineconeï¼‰
2. è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ç®—æ³•
3. ç¼“å­˜ç­–ç•¥å’Œå†…å­˜ä¼˜åŒ–
4. æ‰¹é‡å¤„ç†ä¼˜åŒ–
5. åˆ†å¸ƒå¼éƒ¨ç½²æ–¹æ¡ˆ
"""

import os
import sys
import numpy as np
from typing import List, Dict, Tuple, Optional
import time
import pickle
import json
from datetime import datetime, timedelta
import hashlib
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import redis
from functools import lru_cache
import psutil
import gc

# æ·»åŠ utilsç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.embedding_client import EmbeddingClient

# å°è¯•å¯¼å…¥FAISSï¼ˆå¯é€‰ï¼‰
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("âš ï¸ FAISSæœªå®‰è£…ï¼Œå°†ä½¿ç”¨numpyå®ç°")

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨"""
        self.client = EmbeddingClient()
        self.cache = {}
        self.index_cache = {}
        self.stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'total_queries': 0,
            'total_time': 0
        }
        
        # åˆå§‹åŒ–Redisç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            self.redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
            self.redis_client.ping()
            self.use_redis = True
            print("âœ… Redisç¼“å­˜å·²è¿æ¥")
        except:
            self.use_redis = False
            print("âš ï¸ Redisæœªè¿æ¥ï¼Œä½¿ç”¨å†…å­˜ç¼“å­˜")
        
        # FAISSç´¢å¼•
        self.faiss_index = None
        self.faiss_embeddings = None
        self.faiss_texts = []
    
    def load_benchmark_data(self, n_samples: int = 1000) -> List[str]:
        """åŠ è½½åŸºå‡†æµ‹è¯•æ•°æ®"""
        base_texts = [
            "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•ä¸æ–­ä¼˜åŒ–",
            "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«å’Œè¯­éŸ³å¤„ç†é¢†åŸŸå–å¾—é‡å¤§çªç ´",
            "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€",
            "äº‘è®¡ç®—æŠ€æœ¯æä¾›äº†å¼¹æ€§çš„è®¡ç®—èµ„æºå’Œå­˜å‚¨èƒ½åŠ›",
            "åŒºå—é“¾æŠ€æœ¯é€šè¿‡åˆ†å¸ƒå¼è´¦æœ¬ä¿è¯æ•°æ®çš„å®‰å…¨æ€§å’Œé€æ˜æ€§",
            "ç‰©è”ç½‘è®¾å¤‡è¿æ¥æ•°é‡æŒç»­å¢é•¿ï¼Œæ™ºèƒ½å®¶å±…åº”ç”¨æ™®åŠ",
            "å¤§æ•°æ®åˆ†æå¸®åŠ©ä¼ä¸šä»æµ·é‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯",
            "æ¨èç³»ç»Ÿæ ¹æ®ç”¨æˆ·è¡Œä¸ºæä¾›ä¸ªæ€§åŒ–å†…å®¹æ¨è",
            "æœç´¢å¼•æ“ä¼˜åŒ–æŠ€æœ¯æå‡ç½‘ç«™åœ¨æœç´¢ç»“æœä¸­çš„æ’å",
            "ç½‘ç»œå®‰å…¨å¨èƒæ—¥ç›Šä¸¥é‡ï¼Œé˜²æŠ¤æªæ–½éœ€è¦ä¸æ–­åŠ å¼º"
        ]
        
        # ç”Ÿæˆæ›´å¤šå˜ä½“
        texts = []
        for i in range(n_samples):
            base = base_texts[i % len(base_texts)]
            # æ·»åŠ ä¸€äº›å˜åŒ–
            variation = f"{base} - ç‰ˆæœ¬{i+1}"
            texts.append(variation)
        
        return texts
    
    def benchmark_embedding_generation(self, texts: List[str]) -> Dict:
        """åŸºå‡†æµ‹è¯•åµŒå…¥ç”Ÿæˆæ€§èƒ½"""
        print("ğŸ¯ åŸºå‡†æµ‹è¯•åµŒå…¥ç”Ÿæˆæ€§èƒ½...")
        
        results = {
            'total_texts': len(texts),
            'batch_sizes': [1, 5, 10, 20, 50],
            'results': {}
        }
        
        for batch_size in results['batch_sizes']:
            print(f"\nğŸ“Š æµ‹è¯•æ‰¹å¤§å°: {batch_size}")
            
            # æµ‹è¯•æ‰¹å¤„ç†æ€§èƒ½
            start_time = time.time()
            embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                batch_embeddings = [self.client.get_embedding(text) for text in batch]
                embeddings.extend(batch_embeddings)
            
            elapsed_time = time.time() - start_time
            
            results['results'][batch_size] = {
                'total_time': elapsed_time,
                'time_per_text': elapsed_time / len(texts),
                'texts_per_second': len(texts) / elapsed_time,
                'memory_usage': psutil.Process().memory_info().rss / 1024 / 1024  # MB
            }
            
            print(f"   æ€»æ—¶é—´: {elapsed_time:.2f}s")
            print(f"   æ¯æ–‡æœ¬æ—¶é—´: {results['results'][batch_size]['time_per_text']:.3f}s")
            print(f"   å¤„ç†é€Ÿåº¦: {results['results'][batch_size]['texts_per_second']:.1f} texts/s")
            print(f"   å†…å­˜ä½¿ç”¨: {results['results'][batch_size]['memory_usage']:.1f}MB")
        
        return results
    
    def build_faiss_index(self, texts: List[str]) -> Dict:
        """æ„å»ºFAISSå‘é‡ç´¢å¼•"""
        if not FAISS_AVAILABLE:
            print("âš ï¸ FAISSä¸å¯ç”¨ï¼Œè·³è¿‡ç´¢å¼•æ„å»º")
            return {'status': 'FAISSä¸å¯ç”¨'}
        
        print("ğŸ¯ æ„å»ºFAISSå‘é‡ç´¢å¼•...")
        
        start_time = time.time()
        
        # è·å–æ‰€æœ‰æ–‡æœ¬çš„åµŒå…¥
        embeddings = []
        for text in texts:
            embedding = self.client.get_embedding(text)
            embeddings.append(embedding)
        
        embeddings_array = np.array(embeddings).astype('float32')
        
        # æ„å»ºFAISSç´¢å¼•
        dimension = embeddings_array.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç›¸ä¼¼åº¦
        self.faiss_index.add(embeddings_array)
        self.faiss_embeddings = embeddings_array
        self.faiss_texts = texts
        
        build_time = time.time() - start_time
        
        return {
            'status': 'æˆåŠŸ',
            'total_vectors': len(texts),
            'dimension': dimension,
            'build_time': build_time,
            'memory_usage': embeddings_array.nbytes / 1024 / 1024  # MB
        }
    
    def faiss_similarity_search(self, query: str, k: int = 5) -> List[Dict]:
        """ä½¿ç”¨FAISSè¿›è¡Œç›¸ä¼¼åº¦æœç´¢"""
        if not FAISS_AVAILABLE or self.faiss_index is None:
            print("âš ï¸ FAISSç´¢å¼•ä¸å¯ç”¨ï¼Œä½¿ç”¨numpyå®ç°")
            return self.numpy_similarity_search(query, k)
        
        start_time = time.time()
        
        # è·å–æŸ¥è¯¢åµŒå…¥
        query_embedding = np.array(self.client.get_embedding(query)).astype('float32').reshape(1, -1)
        
        # æœç´¢ç›¸ä¼¼æ–‡æœ¬
        scores, indices = self.faiss_index.search(query_embedding, k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.faiss_texts):
                results.append({
                    'text': self.faiss_texts[idx],
                    'score': float(score),
                    'index': int(idx)
                })
        
        search_time = time.time() - start_time
        
        return {
            'results': results,
            'search_time': search_time,
            'results_per_second': 1 / search_time
        }
    
    def numpy_similarity_search(self, query: str, k: int = 5) -> List[Dict]:
        """ä½¿ç”¨numpyè¿›è¡Œç›¸ä¼¼åº¦æœç´¢"""
        if not hasattr(self, 'numpy_embeddings'):
            print("è¯·å…ˆæ„å»ºnumpyç´¢å¼•")
            return []
        
        start_time = time.time()
        
        # è·å–æŸ¥è¯¢åµŒå…¥
        query_embedding = np.array(self.client.get_embedding(query))
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i, embedding in enumerate(self.numpy_embeddings):
            similarity = np.dot(query_embedding, embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(embedding)
            )
            similarities.append((similarity, i))
        
        # æ’åºå¹¶è¿”å›å‰kä¸ª
        similarities.sort(key=lambda x: x[0], reverse=True)
        top_k = similarities[:k]
        
        results = []
        for score, idx in top_k:
            results.append({
                'text': self.numpy_texts[idx],
                'score': float(score),
                'index': int(idx)
            })
        
        search_time = time.time() - start_time
        
        return {
            'results': results,
            'search_time': search_time,
            'results_per_second': 1 / search_time
        }
    
    def build_numpy_index(self, texts: List[str]) -> Dict:
        """æ„å»ºnumpyç´¢å¼•"""
        print("ğŸ¯ æ„å»ºnumpyç´¢å¼•...")
        
        start_time = time.time()
        
        embeddings = []
        for text in texts:
            embedding = self.client.get_embedding(text)
            embeddings.append(embedding)
        
        self.numpy_embeddings = np.array(embeddings)
        self.numpy_texts = texts
        
        build_time = time.time() - start_time
        
        return {
            'total_vectors': len(texts),
            'dimension': self.numpy_embeddings.shape[1],
            'build_time': build_time,
            'memory_usage': self.numpy_embeddings.nbytes / 1024 / 1024  # MB
        }
    
    def implement_caching_strategy(self, texts: List[str]) -> Dict:
        """å®ç°ç¼“å­˜ç­–ç•¥"""
        print("ğŸ¯ å®ç°ç¼“å­˜ç­–ç•¥...")
        
        # å¤šå±‚ç¼“å­˜ç­–ç•¥
        cache_stats = {
            'memory_cache': {'hits': 0, 'misses': 0},
            'redis_cache': {'hits': 0, 'misses': 0},
            'api_calls': 0
        }
        
        def get_embedding_with_cache(text: str) -> np.ndarray:
            """å¸¦ç¼“å­˜çš„åµŒå…¥è·å–"""
            cache_key = hashlib.md5(text.encode()).hexdigest()
            
            # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
            if cache_key in self.cache:
                cache_stats['memory_cache']['hits'] += 1
                return self.cache[cache_key]
            
            cache_stats['memory_cache']['misses'] += 1
            
            # 2. æ£€æŸ¥Redisç¼“å­˜
            if self.use_redis:
                try:
                    cached_embedding = self.redis_client.get(f"embedding:{cache_key}")
                    if cached_embedding:
                        embedding = json.loads(cached_embedding)
                        self.cache[cache_key] = embedding
                        cache_stats['redis_cache']['hits'] += 1
                        return embedding
                    cache_stats['redis_cache']['misses'] += 1
                except:
                    pass
            
            # 3. è°ƒç”¨APIè·å–
            embedding = self.client.get_embedding(text)
            cache_stats['api_calls'] += 1
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.cache[cache_key] = embedding
            if self.use_redis:
                try:
                    self.redis_client.setex(
                        f"embedding:{cache_key}", 
                        timedelta(hours=24), 
                        json.dumps(embedding)
                    )
                except:
                    pass
            
            return embedding
        
        # æµ‹è¯•ç¼“å­˜æ€§èƒ½
        start_time = time.time()
        
        for text in texts:
            get_embedding_with_cache(text)
        
        total_time = time.time() - start_time
        
        return {
            'cache_stats': cache_stats,
            'total_time': total_time,
            'cache_hit_rate': (cache_stats['memory_cache']['hits'] + cache_stats['redis_cache']['hits']) / len(texts),
            'memory_cache_size': len(self.cache)
        }
    
    def parallel_processing_demo(self, texts: List[str], n_workers: int = 4) -> Dict:
        """å¹¶è¡Œå¤„ç†æ¼”ç¤º"""
        print(f"ğŸ¯ å¹¶è¡Œå¤„ç†æ¼”ç¤ºï¼ˆ{n_workers}ä¸ªå·¥ä½œè¿›ç¨‹ï¼‰...")
        
        def process_batch(batch_texts: List[str]) -> List[np.ndarray]:
            """å¤„ç†ä¸€æ‰¹æ–‡æœ¬"""
            embeddings = []
            for text in batch_texts:
                embedding = self.client.get_embedding(text)
                embeddings.append(embedding)
            return embeddings
        
        # åˆ†å‰²æ•°æ®
        batch_size = max(1, len(texts) // n_workers)
        batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]
        
        # ä¸²è¡Œå¤„ç†
        start_time = time.time()
        serial_embeddings = []
        for text in texts:
            serial_embeddings.append(self.client.get_embedding(text))
        serial_time = time.time() - start_time
        
        # å¹¶è¡Œå¤„ç†
        start_time = time.time()
        parallel_embeddings = []
        
        with ThreadPoolExecutor(max_workers=n_workers) as executor:
            future_to_batch = {executor.submit(process_batch, batch): batch for batch in batches}
            
            for future in as_completed(future_to_batch):
                batch_embeddings = future.result()
                parallel_embeddings.extend(batch_embeddings)
        
        parallel_time = time.time() - start_time
        
        return {
            'serial_time': serial_time,
            'parallel_time': parallel_time,
            'speedup': serial_time / parallel_time,
            'efficiency': (serial_time / parallel_time) / n_workers,
            'total_texts': len(texts),
            'batches': len(batches)
        }
    
    def memory_optimization_demo(self, texts: List[str]) -> Dict:
        """å†…å­˜ä¼˜åŒ–æ¼”ç¤º"""
        print("ğŸ¯ å†…å­˜ä¼˜åŒ–æ¼”ç¤º...")
        
        # åŸå§‹æ–¹æ³•
        start_time = time.time()
        original_embeddings = []
        for text in texts:
            embedding = self.client.get_embedding(text)
            original_embeddings.append(np.array(embedding, dtype=np.float32))
        
        original_memory = sum([emb.nbytes for emb in original_embeddings]) / 1024 / 1024
        original_time = time.time() - start_time
        
        # ä¼˜åŒ–æ–¹æ³•ï¼šä½¿ç”¨numpyæ•°ç»„
        start_time = time.time()
        optimized_embeddings = []
        for text in texts:
            embedding = self.client.get_embedding(text)
            optimized_embeddings.append(embedding)
        
        # è½¬æ¢ä¸ºå•ä¸ªnumpyæ•°ç»„
        optimized_array = np.array(optimized_embeddings, dtype=np.float32)
        optimized_memory = optimized_array.nbytes / 1024 / 1024
        optimized_time = time.time() - start_time
        
        # å†…å­˜å‹ç¼©
        compressed = np.round(optimized_array, decimals=4)
        compressed_memory = compressed.nbytes / 1024 / 1024
        
        return {
            'original_memory_mb': original_memory,
            'original_time': original_time,
            'optimized_memory_mb': optimized_memory,
            'optimized_time': optimized_time,
            'memory_reduction': (original_memory - optimized_memory) / original_memory,
            'compressed_memory_mb': compressed_memory,
            'compression_ratio': (original_memory - compressed_memory) / original_memory
        }
    
    def comprehensive_performance_test(self, n_samples: int = 100) -> Dict:
        """ç»¼åˆæ€§èƒ½æµ‹è¯•"""
        print("ğŸ¯ ç»¼åˆæ€§èƒ½æµ‹è¯•...")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        texts = self.load_benchmark_data(n_samples)
        
        # 1. åŸºå‡†æµ‹è¯•
        benchmark_results = self.benchmark_embedding_generation(texts[:20])
        
        # 2. ç´¢å¼•æ„å»º
        numpy_results = self.build_numpy_index(texts)
        
        # 3. ç›¸ä¼¼åº¦æœç´¢æµ‹è¯•
        search_queries = ["äººå·¥æ™ºèƒ½æŠ€æœ¯åº”ç”¨", "ä½“è‚²èµ›äº‹æŠ¥é“", "è´¢ç»å¸‚åœºåˆ†æ"]
        search_results = []
        
        for query in search_queries:
            result = self.numpy_similarity_search(query, k=5)
            search_results.append({
                'query': query,
                **result
            })
        
        # 4. ç¼“å­˜æµ‹è¯•
        cache_results = self.implement_caching_strategy(texts[:10])
        
        # 5. å¹¶è¡Œå¤„ç†æµ‹è¯•
        parallel_results = self.parallel_processing_demo(texts[:20], n_workers=2)
        
        # 6. å†…å­˜ä¼˜åŒ–æµ‹è¯•
        memory_results = self.memory_optimization_demo(texts[:20])
        
        # æ±‡æ€»ç»“æœ
        summary = {
            'test_config': {
                'total_texts': n_samples,
                'test_queries': len(search_queries)
            },
            'benchmark': benchmark_results,
            'index_build': numpy_results,
            'search_performance': search_results,
            'cache_performance': cache_results,
            'parallel_performance': parallel_results,
            'memory_optimization': memory_results
        }
        
        return summary
    
    def generate_optimization_report(self, results: Dict) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report = f"""
# æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š

## æµ‹è¯•é…ç½®
- æµ‹è¯•æ ·æœ¬æ•°: {results['test_config']['total_texts']}
- æµ‹è¯•æŸ¥è¯¢æ•°: {results['test_config']['test_queries']}

## åµŒå…¥ç”Ÿæˆæ€§èƒ½
"""
        
        for batch_size, data in results['benchmark']['results'].items():
            report += f"""
### æ‰¹å¤§å° {batch_size}
- å¤„ç†é€Ÿåº¦: {data['texts_per_second']:.1f} texts/s
- æ¯æ–‡æœ¬æ—¶é—´: {data['time_per_text']:.3f}s
- å†…å­˜ä½¿ç”¨: {data['memory_usage']:.1f}MB
"""
        
        report += f"""
## ç´¢å¼•æ„å»º
- æ„å»ºæ—¶é—´: {results['index_build']['build_time']:.2f}s
- å†…å­˜ä½¿ç”¨: {results['index_build']['memory_usage']:.1f}MB
- å‘é‡ç»´åº¦: {results['index_build']['dimension']}

## æœç´¢æ€§èƒ½
"""
        
        for search_result in results['search_performance']:
            report += f"""
### æŸ¥è¯¢: {search_result['query']}
- æœç´¢æ—¶é—´: {search_result['search_time']:.3f}s
- å¤„ç†é€Ÿåº¦: {search_result['results_per_second']:.1f} queries/s
"""
        
        report += f"""
## ç¼“å­˜æ€§èƒ½
- ç¼“å­˜å‘½ä¸­ç‡: {results['cache_performance']['cache_hit_rate']:.2%}
- å†…å­˜ç¼“å­˜å¤§å°: {results['cache_performance']['memory_cache_size']}
- Rediså¯ç”¨: {self.use_redis}

## å¹¶è¡Œå¤„ç†
- ä¸²è¡Œæ—¶é—´: {results['parallel_performance']['serial_time']:.2f}s
- å¹¶è¡Œæ—¶é—´: {results['parallel_performance']['parallel_time']:.2f}s
- åŠ é€Ÿæ¯”: {results['parallel_performance']['speedup']:.2f}x
- æ•ˆç‡: {results['parallel_performance']['efficiency']:.2%}

## å†…å­˜ä¼˜åŒ–
- å†…å­˜å‡å°‘: {results['memory_optimization']['memory_reduction']:.2%}
- å‹ç¼©æ¯”: {results['memory_optimization']['compression_ratio']:.2%}
"""
        
        return report
    
    def demo_performance_optimization(self):
        """æ¼”ç¤ºæ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ"""
        print("ğŸš€ é«˜çº§åŠŸèƒ½ç¬¬4è¯¾ï¼šæ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ")
        print("=" * 60)
        
        # è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•
        results = self.comprehensive_performance_test(n_samples=50)
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
        report = self.generate_optimization_report(results)
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        with open("03-advanced/performance_report.md", "w", encoding="utf-8") as f:
            f.write(report)
        
        print("\nğŸ‰ æ€§èƒ½ä¼˜åŒ–æ¼”ç¤ºå®Œæˆï¼")
        print("\næ ¸å¿ƒæŠ€æœ¯æ€»ç»“ï¼š")
        print("   â€¢ å‘é‡ç´¢å¼•ä¼˜åŒ–")
        print("   â€¢ è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢")
        print("   â€¢ ç¼“å­˜ç­–ç•¥")
        print("   â€¢ å¹¶è¡Œå¤„ç†")
        print("   â€¢ å†…å­˜ä¼˜åŒ–")
        print("\nå®é™…åº”ç”¨åœºæ™¯ï¼š")
        print("   â€¢ é«˜æ€§èƒ½æœç´¢å¼•æ“")
        print("   â€¢ å®æ—¶æ¨èç³»ç»Ÿ")
        print("   â€¢ å¤§è§„æ¨¡æ–‡æœ¬å¤„ç†")
        print("   â€¢ åˆ†å¸ƒå¼éƒ¨ç½²")
        print("\nä¸‹ä¸€é˜¶æ®µï¼šå®æˆ˜é¡¹ç›®å¼€å‘")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ")
    print("=" * 60)
    
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    try:
        optimizer = PerformanceOptimizer()
        optimizer.demo_performance_optimization()
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()