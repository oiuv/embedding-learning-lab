#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­çº§æŒ‘æˆ˜1ï¼šå¤šè¯­è¨€æ”¯æŒç³»ç»Ÿ
======================

å®ç°æ”¯æŒå¤šè¯­è¨€çš„æ–‡æœ¬åµŒå…¥ç³»ç»Ÿï¼Œå¤„ç†ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ç­‰ä¸åŒè¯­è¨€ã€‚

æŒ‘æˆ˜ç›®æ ‡ï¼š
1. å¤šè¯­è¨€æ–‡æœ¬é¢„å¤„ç†
2. è·¨è¯­è¨€è¯­ä¹‰æœç´¢
3. è¯­è¨€æ£€æµ‹åŠŸèƒ½
4. å¤šè¯­è¨€å¯è§†åŒ–
5. è¯­è¨€ç‰¹å®šä¼˜åŒ–
"""

import os
import sys
import json
import numpy as np
from typing import List, Dict, Tuple
import re
import langdetect
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
from utils.embedding_client import EmbeddingClient

class MultilingualSupportSystem:
    """å¤šè¯­è¨€æ”¯æŒç³»ç»Ÿ"""
    
    def __init__(self):
        self.client = EmbeddingClient()
        self.language_patterns = {
            'chinese': re.compile(r'[\u4e00-\u9fff]+'),
            'japanese': re.compile(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]+'),
            'korean': re.compile(r'[\uac00-\ud7af]+'),
            'english': re.compile(r'[a-zA-Z]+'),
            'numbers': re.compile(r'\d+')
        }
    
    def load_multilingual_data(self) -> List[Dict[str, str]]:
        """åŠ è½½å¤šè¯­è¨€æ•°æ®"""
        multilingual_texts = [
            {
                'text': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯',
                'language': 'zh',
                'translation': 'Machine learning is an important branch of artificial intelligence'
            },
            {
                'text': 'Deep learning is revolutionizing AI technology',
                'language': 'en',
                'translation': 'æ·±åº¦å­¦ä¹ æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜AIæŠ€æœ¯'
            },
            {
                'text': 'æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®é‡è¦ãªåˆ†é‡ã§ã™',
                'language': 'ja',
                'translation': 'Machine learning is an important field of artificial intelligence'
            },
            {
                'text': 'Pythonç¼–ç¨‹è¯­è¨€åœ¨æ•°æ®ç§‘å­¦ä¸­å¹¿æ³›åº”ç”¨',
                'language': 'zh',
                'translation': 'Python programming language is widely used in data science'
            },
            {
                'text': 'Data science combines statistics and programming',
                'language': 'en',
                'translation': 'æ•°æ®ç§‘å­¦ç»“åˆäº†ç»Ÿè®¡å­¦å’Œç¼–ç¨‹'
            },
            {
                'text': 'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¯çµ±è¨ˆå­¦ã¨ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’çµ„ã¿åˆã‚ã›ã‚‹',
                'language': 'ja',
                'translation': 'Data science combines statistics and programming'
            }
        ]
        return multilingual_texts
    
    def detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€
        
        æŒ‘æˆ˜ï¼šå®ç°å‡†ç¡®çš„è¯­è¨€æ£€æµ‹
        """
        try:
            # ä½¿ç”¨langdetectåº“
            detected = langdetect.detect(text)
            return detected
        except:
            # å¤‡ç”¨æ£€æµ‹æ–¹æ³•
            if self.language_patterns['chinese'].search(text):
                return 'zh'
            elif self.language_patterns['japanese'].search(text):
                return 'ja'
            elif self.language_patterns['korean'].search(text):
                return 'ko'
            elif self.language_patterns['english'].search(text):
                return 'en'
            else:
                return 'unknown'
    
    def preprocess_multilingual_text(self, text: str, language: str) -> str:
        """å¤šè¯­è¨€æ–‡æœ¬é¢„å¤„ç†
        
        æŒ‘æˆ˜ï¼šé’ˆå¯¹ä¸åŒè¯­è¨€è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†
        """
        # åŸºç¡€æ¸…ç†
        text = text.strip()
        
        if language == 'zh':
            # ä¸­æ–‡å¤„ç†
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦
            text = re.sub(r'[^\u4e00-\u9fff\w]', '', text)
        elif language == 'ja':
            # æ—¥æ–‡å¤„ç†
            # ä¿ç•™æ—¥æ–‡æ±‰å­—ã€å¹³å‡åã€ç‰‡å‡å
            text = re.sub(r'[^\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf\w]', '', text)
        elif language == 'en':
            # è‹±æ–‡å¤„ç†
            text = text.lower()
            text = re.sub(r'[^a-zA-Z\s]', '', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        return text
    
    def cross_language_search(self, query: str, target_language: str, texts: List[Dict]) -> List[Dict]:
        """è·¨è¯­è¨€è¯­ä¹‰æœç´¢
        
        æŒ‘æˆ˜ï¼šå®ç°è·¨è¯­è¨€çš„è¯­ä¹‰åŒ¹é…
        """
        # é¢„å¤„ç†æŸ¥è¯¢
        query_language = self.detect_language(query)
        query_processed = self.preprocess_multilingual_text(query, query_language)
        
        # è·å–æŸ¥è¯¢åµŒå…¥
        query_embedding = self.client.get_embedding(query_processed)
        
        # æœç´¢ç›®æ ‡è¯­è¨€æ–‡æœ¬
        results = []
        
        for text_data in texts:
            if text_data['language'] == target_language:
                # è·å–ç›®æ ‡æ–‡æœ¬åµŒå…¥
                target_text = self.preprocess_multilingual_text(
                    text_data['text'], 
                    text_data['language']
                )
                target_embedding = self.client.get_embedding(target_text)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = np.dot(query_embedding, target_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(target_embedding)
                )
                
                results.append({
                    'text': text_data['text'],
                    'language': text_data['language'],
                    'translation': text_data['translation'],
                    'similarity': similarity,
                    'query_language': query_language
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results
    
    def analyze_language_distribution(self, texts: List[str]) -> Dict[str, int]:
        """åˆ†æè¯­è¨€åˆ†å¸ƒ"""
        language_counts = Counter()
        
        for text in texts:
            lang = self.detect_language(text)
            language_counts[lang] += 1
        
        return dict(language_counts)
    
    def create_multilingual_embeddings(self, texts: List[Dict]) -> Dict[str, List[float]]:
        """åˆ›å»ºå¤šè¯­è¨€åµŒå…¥"""
        embeddings = {}
        
        for text_data in texts:
            language = text_data['language']
            text = text_data['text']
            
            # é¢„å¤„ç†
            processed_text = self.preprocess_multilingual_text(text, language)
            
            # è·å–åµŒå…¥
            embedding = self.client.get_embedding(processed_text)
            
            embeddings[text_data['text']] = {
                'embedding': embedding,
                'language': language,
                'processed_text': processed_text
            }
        
        return embeddings
    
    def multilingual_clustering(self, texts: List[Dict], n_clusters: int = 3) -> Dict:
        """å¤šè¯­è¨€èšç±»åˆ†æ"""
        # è·å–æ‰€æœ‰æ–‡æœ¬çš„åµŒå…¥
        embeddings = []
        text_info = []
        
        for text_data in texts:
            processed_text = self.preprocess_multilingual_text(
                text_data['text'], 
                text_data['language']
            )
            embedding = self.client.get_embedding(processed_text)
            
            embeddings.append(embedding)
            text_info.append(text_data)
        
        embeddings_array = np.array(embeddings)
        
        # ä½¿ç”¨K-meansèšç±»
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings_array)
        
        # æ„å»ºèšç±»ç»“æœ
        clusters = [[] for _ in range(n_clusters)]
        
        for i, (text_data, label) in enumerate(zip(text_info, labels)):
            clusters[label].append({
                'text': text_data['text'],
                'language': text_data['language'],
                'translation': text_data['translation'],
                'cluster': label
            })
        
        return {
            'clusters': clusters,
            'labels': labels,
            'embeddings': embeddings_array
        }
    
    def calculate_cross_language_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—è·¨è¯­è¨€ç›¸ä¼¼åº¦"""
        # æ£€æµ‹è¯­è¨€
        lang1 = self.detect_language(text1)
        lang2 = self.detect_language(text2)
        
        # é¢„å¤„ç†
        processed1 = self.preprocess_multilingual_text(text1, lang1)
        processed2 = self.preprocess_multilingual_text(text2, lang2)
        
        # è·å–åµŒå…¥
        embedding1 = self.client.get_embedding(processed1)
        embedding2 = self.client.get_embedding(processed2)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = np.dot(embedding1, embedding2) / (
            np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
        )
        
        return similarity
    
    def run_multilingual_challenge(self):
        """è¿è¡Œå¤šè¯­è¨€æŒ‘æˆ˜"""
        print("ğŸŒ ä¸­çº§æŒ‘æˆ˜1ï¼šå¤šè¯­è¨€æ”¯æŒç³»ç»Ÿ")
        print("=" * 60)
        
        # åŠ è½½å¤šè¯­è¨€æ•°æ®
        multilingual_data = self.load_multilingual_data()
        print(f"ğŸ“Š åŠ è½½ {len(multilingual_data)} ä¸ªå¤šè¯­è¨€æ–‡æœ¬")
        
        # è¯­è¨€æ£€æµ‹
        print("\nğŸ” è¯­è¨€æ£€æµ‹æ¼”ç¤º")
        print("-" * 30)
        
        for text_data in multilingual_data[:3]:
            detected = self.detect_language(text_data['text'])
            print(f"   æ–‡æœ¬: {text_data['text'][:30]}...")
            print(f"   å®é™…è¯­è¨€: {text_data['language']}")
            print(f"   æ£€æµ‹è¯­è¨€: {detected}")
            print(f"   æ£€æµ‹æ­£ç¡®: {'âœ…' if detected == text_data['language'] else 'âŒ'}")
            print()
        
        # è·¨è¯­è¨€æœç´¢
        print("ğŸ” è·¨è¯­è¨€è¯­ä¹‰æœç´¢æ¼”ç¤º")
        print("-" * 30)
        
        queries = [
            {'query': 'machine learning', 'target_lang': 'zh'},
            {'query': 'äººå·¥æ™ºèƒ½', 'target_lang': 'en'},
            {'query': 'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹', 'target_lang': 'zh'}
        ]
        
        for search_config in queries:
            results = self.cross_language_search(
                search_config['query'], 
                search_config['target_lang'], 
                multilingual_data
            )
            
            print(f"\n   æŸ¥è¯¢: {search_config['query']} ({self.detect_language(search_config['query'])}) â†’ {search_config['target_lang']}")
            for result in results[:2]:
                print(f"      {result['text']} (ç›¸ä¼¼åº¦: {result['similarity']:.3f})")
        
        # è¯­è¨€åˆ†å¸ƒåˆ†æ
        print("\nğŸ“Š è¯­è¨€åˆ†å¸ƒåˆ†æ")
        print("-" * 30)
        
        all_texts = [item['text'] for item in multilingual_data]
        distribution = self.analyze_language_distribution(all_texts)
        
        for lang, count in distribution.items():
            print(f"   {lang}: {count} ä¸ªæ–‡æœ¬")
        
        # å¤šè¯­è¨€èšç±»
        print("\nğŸ¯ å¤šè¯­è¨€èšç±»åˆ†æ")
        print("-" * 30)
        
        clustering_result = self.multilingual_clustering(multilingual_data)
        
        for i, cluster in enumerate(clustering_result['clusters']):
            print(f"\n   èšç±» {i+1}:")
            for item in cluster:
                print(f"      [{item['language']}] {item['text']}")
        
        # è·¨è¯­è¨€ç›¸ä¼¼åº¦æµ‹è¯•
        print("\nğŸ¯ è·¨è¯­è¨€ç›¸ä¼¼åº¦æµ‹è¯•")
        print("-" * 30)
        
        test_pairs = [
            ("æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯", "Machine learning is an important branch of AI"),
            ("æ·±åº¦å­¦ä¹ ", "Deep learning"),
            ("Pythonç¼–ç¨‹", "Python programming")
        ]
        
        for text1, text2 in test_pairs:
            similarity = self.calculate_cross_language_similarity(text1, text2)
            print(f"   ç›¸ä¼¼åº¦: {text1[:15]}... â†” {text2[:15]}... = {similarity:.3f}")
        
        print("\nâœ… å¤šè¯­è¨€æŒ‘æˆ˜å®Œæˆï¼")
        print("\nğŸ“ å­¦ä¹ è¦ç‚¹ï¼š")
        print("   â€¢ è¯­è¨€æ£€æµ‹ï¼šå‡†ç¡®è¯†åˆ«æ–‡æœ¬è¯­è¨€")
        print("   â€¢ è·¨è¯­è¨€åŒ¹é…ï¼šä¸åŒè¯­è¨€çš„è¯­ä¹‰å¯¹é½")
        print("   â€¢ å¤šè¯­è¨€èšç±»ï¼šè¯­è¨€æ— å…³çš„å†…å®¹åˆ†ç»„")
        print("   â€¢ é¢„å¤„ç†ç­–ç•¥ï¼šé’ˆå¯¹ä¸åŒè¯­è¨€çš„ä¼˜åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ ä¸­çº§æŒ‘æˆ˜ï¼šå¤šè¯­è¨€æ”¯æŒç³»ç»Ÿ")
    print("=" * 60)
    
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    try:
        challenge = MultilingualSupportSystem()
        challenge.run_multilingual_challenge()
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()